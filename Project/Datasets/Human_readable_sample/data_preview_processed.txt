=== DATASET PREVIEW ===
Source File: Project/Datasets/processed\train.jsonl.gz

--- SAMPLE #1 ---
DOCSTRING: Trains a k-nearest neighbors classifier for face recognition.
CODE:
def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):
    X = []
    y = []
    for class_dir in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_dir)):
            continue
        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):
            image = face_recognition.load_image_file(img_path)
            face_bounding_boxes = face_recognition.face_locations(image)
            if len(face_bounding_boxes) != 1:
                if verbose:
                    print("Image {} not suitable for training: {}".format(img_path, "Didn't find a face" if len(face_bounding_boxes) < 1 else "Found more than one face"))
            else:
                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])
                y.append(class_dir)
    if n_neighbors is None:
        n_neighbors = int(round(math.sqrt(len(X))))
        if verbose:
            print("Chose n_neighbors automatically:", n_neighbors)
    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')
    knn_clf.fit(X, y)
    if model_save_path is not None:
        with open(model_save_path, 'wb') as f:
            pickle.dump(knn_clf, f)
    return knn_clf
--------------------------------------------------

--- SAMPLE #2 ---
DOCSTRING: Recognizes faces in given image using a trained KNN classifier
CODE:
def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):
    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:
        raise Exception("Invalid image path: {}".format(X_img_path))
    if knn_clf is None and model_path is None:
        raise Exception("Must supply knn classifier either thourgh knn_clf or model_path")
    if knn_clf is None:
        with open(model_path, 'rb') as f:
            knn_clf = pickle.load(f)
    X_img = face_recognition.load_image_file(X_img_path)
    X_face_locations = face_recognition.face_locations(X_img)
    if len(X_face_locations) == 0:
        return []
    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)
    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)
    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]
    return [(pred, loc) if rec else ("unknown", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]
--------------------------------------------------

--- SAMPLE #3 ---
DOCSTRING: Shows the face recognition results visually.
CODE:
def show_prediction_labels_on_image(img_path, predictions):
    pil_image = Image.open(img_path).convert("RGB")
    draw = ImageDraw.Draw(pil_image)
    for name, (top, right, bottom, left) in predictions:
        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))
        name = name.encode("UTF-8")
        text_width, text_height = draw.textsize(name)
        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))
    del draw
    pil_image.show()
--------------------------------------------------

--- SAMPLE #4 ---
DOCSTRING: Returns an array of bounding boxes of human faces in a image
CODE:
def face_locations(img, number_of_times_to_upsample=1, model="hog"):
    if model == "cnn":
        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, "cnn")]
    else:
        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]
--------------------------------------------------

--- SAMPLE #5 ---
DOCSTRING: Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image
CODE:
def face_landmarks(face_image, face_locations=None, model="large"):
    landmarks = _raw_face_landmarks(face_image, face_locations, model)
    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]
    if model == 'large':
        return [{
            "chin": points[0:17],
            "left_eyebrow": points[17:22],
            "right_eyebrow": points[22:27],
            "nose_bridge": points[27:31],
            "nose_tip": points[31:36],
            "left_eye": points[36:42],
            "right_eye": points[42:48],
            "top_lip": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],
            "bottom_lip": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]
        } for points in landmarks_as_tuples]
    elif model == 'small':
        return [{
            "nose_tip": [points[4]],
            "left_eye": points[2:4],
            "right_eye": points[0:2],
        } for points in landmarks_as_tuples]
    else:
        raise ValueError("Invalid landmarks model type. Supported models are ['small', 'large'].")
--------------------------------------------------

--- SAMPLE #6 ---
DOCSTRING: Return the Catalyst datatype from the size of integers.
CODE:
def _int_size_to_type(size):
    if size <= 8:
        return ByteType
    if size <= 16:
        return ShortType
    if size <= 32:
        return IntegerType
    if size <= 64:
        return LongType
--------------------------------------------------

--- SAMPLE #7 ---
DOCSTRING: Infer the DataType from obj
CODE:
def _infer_type(obj):
    if obj is None:
        return NullType()
    if hasattr(obj, '__UDT__'):
        return obj.__UDT__
    dataType = _type_mappings.get(type(obj))
    if dataType is DecimalType:
        return DecimalType(38, 18)
    elif dataType is not None:
        return dataType()
    if isinstance(obj, dict):
        for key, value in obj.items():
            if key is not None and value is not None:
                return MapType(_infer_type(key), _infer_type(value), True)
        return MapType(NullType(), NullType(), True)
    elif isinstance(obj, list):
        for v in obj:
            if v is not None:
                return ArrayType(_infer_type(obj[0]), True)
        return ArrayType(NullType(), True)
    elif isinstance(obj, array):
        if obj.typecode in _array_type_mappings:
            return ArrayType(_array_type_mappings[obj.typecode](), False)
        else:
            raise TypeError("not supported type: array(%s)" % obj.typecode)
    else:
        try:
            return _infer_schema(obj)
        except TypeError:
            raise TypeError("not supported type: %s" % type(obj))
--------------------------------------------------

--- SAMPLE #8 ---
DOCSTRING: Infer the schema from dict/namedtuple/object
CODE:
def _infer_schema(row, names=None):
    if isinstance(row, dict):
        items = sorted(row.items())
    elif isinstance(row, (tuple, list)):
        if hasattr(row, "__fields__"):  
            items = zip(row.__fields__, tuple(row))
        elif hasattr(row, "_fields"):  
            items = zip(row._fields, tuple(row))
        else:
            if names is None:
                names = ['_%d' % i for i in range(1, len(row) + 1)]
            elif len(names) < len(row):
                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))
            items = zip(names, row)
    elif hasattr(row, "__dict__"):  
        items = sorted(row.__dict__.items())
    else:
        raise TypeError("Can not infer schema for type: %s" % type(row))
    fields = [StructField(k, _infer_type(v), True) for k, v in items]
    return StructType(fields)
--------------------------------------------------

--- SAMPLE #9 ---
DOCSTRING: Return whether there is NullType in `dt` or not
CODE:
def _has_nulltype(dt):
    if isinstance(dt, StructType):
        return any(_has_nulltype(f.dataType) for f in dt.fields)
    elif isinstance(dt, ArrayType):
        return _has_nulltype((dt.elementType))
    elif isinstance(dt, MapType):
        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)
    else:
        return isinstance(dt, NullType)
--------------------------------------------------

--- SAMPLE #10 ---
DOCSTRING: Create a converter to drop the names of fields in obj
CODE:
def _create_converter(dataType):
    if not _need_converter(dataType):
        return lambda x: x
    if isinstance(dataType, ArrayType):
        conv = _create_converter(dataType.elementType)
        return lambda row: [conv(v) for v in row]
    elif isinstance(dataType, MapType):
        kconv = _create_converter(dataType.keyType)
        vconv = _create_converter(dataType.valueType)
        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())
    elif isinstance(dataType, NullType):
        return lambda x: None
    elif not isinstance(dataType, StructType):
        return lambda x: x
    names = [f.name for f in dataType.fields]
    converters = [_create_converter(f.dataType) for f in dataType.fields]
    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)
    def convert_struct(obj):
        if obj is None:
            return
        if isinstance(obj, (tuple, list)):
            if convert_fields:
                return tuple(conv(v) for v, conv in zip(obj, converters))
            else:
                return tuple(obj)
        if isinstance(obj, dict):
            d = obj
        elif hasattr(obj, "__dict__"):  
            d = obj.__dict__
        else:
            raise TypeError("Unexpected obj type: %s" % type(obj))
        if convert_fields:
            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])
        else:
            return tuple([d.get(name) for name in names])
    return convert_struct
--------------------------------------------------

--- SAMPLE #11 ---
DOCSTRING: Convert Spark data type to pyarrow type
CODE:
def to_arrow_type(dt):
    import pyarrow as pa
    if type(dt) == BooleanType:
        arrow_type = pa.bool_()
    elif type(dt) == ByteType:
        arrow_type = pa.int8()
    elif type(dt) == ShortType:
        arrow_type = pa.int16()
    elif type(dt) == IntegerType:
        arrow_type = pa.int32()
    elif type(dt) == LongType:
        arrow_type = pa.int64()
    elif type(dt) == FloatType:
        arrow_type = pa.float32()
    elif type(dt) == DoubleType:
        arrow_type = pa.float64()
    elif type(dt) == DecimalType:
        arrow_type = pa.decimal128(dt.precision, dt.scale)
    elif type(dt) == StringType:
        arrow_type = pa.string()
    elif type(dt) == BinaryType:
        arrow_type = pa.binary()
    elif type(dt) == DateType:
        arrow_type = pa.date32()
    elif type(dt) == TimestampType:
        arrow_type = pa.timestamp('us', tz='UTC')
    elif type(dt) == ArrayType:
        if type(dt.elementType) in [StructType, TimestampType]:
            raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
        arrow_type = pa.list_(to_arrow_type(dt.elementType))
    elif type(dt) == StructType:
        if any(type(field.dataType) == StructType for field in dt):
            raise TypeError("Nested StructType not supported in conversion to Arrow")
        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
                  for field in dt]
        arrow_type = pa.struct(fields)
    else:
        raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
    return arrow_type
--------------------------------------------------

--- SAMPLE #12 ---
DOCSTRING: Convert pyarrow type to Spark data type.
CODE:
def from_arrow_type(at):
    import pyarrow.types as types
    if types.is_boolean(at):
        spark_type = BooleanType()
    elif types.is_int8(at):
        spark_type = ByteType()
    elif types.is_int16(at):
        spark_type = ShortType()
    elif types.is_int32(at):
        spark_type = IntegerType()
    elif types.is_int64(at):
        spark_type = LongType()
    elif types.is_float32(at):
        spark_type = FloatType()
    elif types.is_float64(at):
        spark_type = DoubleType()
    elif types.is_decimal(at):
        spark_type = DecimalType(precision=at.precision, scale=at.scale)
    elif types.is_string(at):
        spark_type = StringType()
    elif types.is_binary(at):
        spark_type = BinaryType()
    elif types.is_date32(at):
        spark_type = DateType()
    elif types.is_timestamp(at):
        spark_type = TimestampType()
    elif types.is_list(at):
        if types.is_timestamp(at.value_type):
            raise TypeError("Unsupported type in conversion from Arrow: " + str(at))
        spark_type = ArrayType(from_arrow_type(at.value_type))
    elif types.is_struct(at):
        if any(types.is_struct(field.type) for field in at):
            raise TypeError("Nested StructType not supported in conversion from Arrow: " + str(at))
        return StructType(
            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
             for field in at])
    else:
        raise TypeError("Unsupported type in conversion from Arrow: " + str(at))
    return spark_type
--------------------------------------------------

--- SAMPLE #13 ---
DOCSTRING: Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.
CODE:
def _check_series_localize_timestamps(s, timezone):
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()
    from pandas.api.types import is_datetime64tz_dtype
    tz = timezone or _get_local_timezone()
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(tz).dt.tz_localize(None)
    else:
        return s
--------------------------------------------------

--- SAMPLE #14 ---
DOCSTRING: Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for Spark internal storage
CODE:
def _check_series_convert_timestamps_internal(s, timezone):
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()
    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype
    if is_datetime64_dtype(s.dtype):
        tz = timezone or _get_local_timezone()
        return s.dt.tz_localize(tz, ambiguous=False).dt.tz_convert('UTC')
    elif is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert('UTC')
    else:
        return s
--------------------------------------------------

--- SAMPLE #15 ---
DOCSTRING: Convert timestamp to timezone-naive in the specified timezone or local timezone
CODE:
def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()
    import pandas as pd
    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype
    from_tz = from_timezone or _get_local_timezone()
    to_tz = to_timezone or _get_local_timezone()
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(to_tz).dt.tz_localize(None)
    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:
        return s.apply(
            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)
            if ts is not pd.NaT else pd.NaT)
    else:
        return s
--------------------------------------------------

--- SAMPLE #16 ---
DOCSTRING: Construct a StructType by adding new elements to it to define the schema. The method accepts either:
CODE:
def add(self, field, data_type=None, nullable=True, metadata=None):
        if isinstance(field, StructField):
            self.fields.append(field)
            self.names.append(field.name)
        else:
            if isinstance(field, str) and data_type is None:
                raise ValueError("Must specify DataType if passing name of struct_field to create.")
            if isinstance(data_type, str):
                data_type_f = _parse_datatype_json_value(data_type)
            else:
                data_type_f = data_type
            self.fields.append(StructField(field, data_type_f, nullable, metadata))
            self.names.append(field)
        self._needConversion = [f.needConversion() for f in self]
        self._needSerializeAnyField = any(self._needConversion)
        return self
--------------------------------------------------

--- SAMPLE #17 ---
DOCSTRING: Return as an dict
CODE:
def asDict(self, recursive=False):
        if not hasattr(self, "__fields__"):
            raise TypeError("Cannot convert a Row class into dict")
        if recursive:
            def conv(obj):
                if isinstance(obj, Row):
                    return obj.asDict(True)
                elif isinstance(obj, list):
                    return [conv(o) for o in obj]
                elif isinstance(obj, dict):
                    return dict((k, conv(v)) for k, v in obj.items())
                else:
                    return obj
            return dict(zip(self.__fields__, (conv(o) for o in self)))
        else:
            return dict(zip(self.__fields__, self))
--------------------------------------------------

--- SAMPLE #18 ---
DOCSTRING: Evaluates the model on a test dataset.
CODE:
def evaluate(self, dataset):
        if not isinstance(dataset, DataFrame):
            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))
        java_lr_summary = self._call_java("evaluate", dataset)
        return LinearRegressionSummary(java_lr_summary)
--------------------------------------------------

--- SAMPLE #19 ---
DOCSTRING: Evaluates the model on a test dataset.
CODE:
def evaluate(self, dataset):
        if not isinstance(dataset, DataFrame):
            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))
        java_glr_summary = self._call_java("evaluate", dataset)
        return GeneralizedLinearRegressionSummary(java_glr_summary)
--------------------------------------------------

--- SAMPLE #20 ---
DOCSTRING: Get all the directories
CODE:
def _get_local_dirs(sub):
    path = os.environ.get("SPARK_LOCAL_DIRS", "/tmp")
    dirs = path.split(",")
    if len(dirs) > 1:
        rnd = random.Random(os.getpid() + id(dirs))
        random.shuffle(dirs, rnd.random)
    return [os.path.join(d, "python", str(os.getpid()), sub) for d in dirs]
--------------------------------------------------

--- SAMPLE #21 ---
DOCSTRING: Combine the items by creator and combiner
CODE:
def mergeValues(self, iterator):
        creator, comb = self.agg.createCombiner, self.agg.mergeValue
        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch
        limit = self.memory_limit
        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else creator(v)
            c += 1
            if c >= batch:
                if get_used_memory() >= limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5
        if get_used_memory() >= limit:
            self._spill()
--------------------------------------------------

--- SAMPLE #22 ---
DOCSTRING: Merge (K,V) pair by mergeCombiner
CODE:
def mergeCombiners(self, iterator, limit=None):
        if limit is None:
            limit = self.memory_limit
        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size
        c, data, pdata, batch = 0, self.data, self.pdata, self.batch
        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else v
            if not limit:
                continue
            c += objsize(v)
            if c > batch:
                if get_used_memory() > limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5
        if limit and get_used_memory() >= limit:
            self._spill()
--------------------------------------------------

--- SAMPLE #23 ---
DOCSTRING: dump already partitioned data into disks.
CODE:
def _spill(self):
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)
        used_memory = get_used_memory()
        if not self.pdata:
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]
            for k, v in self.data.items():
                h = self._partition(k)
                self.serializer.dump_stream([(k, v)], streams[h])
            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()
            self.data.clear()
            self.pdata.extend([{} for i in range(self.partitions)])
        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, "wb") as f:
                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)
        self.spills += 1
        gc.collect()  
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #24 ---
DOCSTRING: Return all partitioned items as iterator
CODE:
def _external_items(self):
        assert not self.data
        if any(self.pdata):
            self._spill()
        self.pdata = []
        try:
            for i in range(self.partitions):
                for v in self._merged_items(i):
                    yield v
                self.data.clear()
                for j in range(self.spills):
                    path = self._get_spill_dir(j)
                    os.remove(os.path.join(path, str(i)))
        finally:
            self._cleanup()
--------------------------------------------------

--- SAMPLE #25 ---
DOCSTRING: merge the partitioned items and return the as iterator
CODE:
def _recursive_merged_items(self, index):
        subdirs = [os.path.join(d, "parts", str(index)) for d in self.localdirs]
        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,
                           self.scale * self.partitions, self.partitions, self.batch)
        m.pdata = [{} for _ in range(self.partitions)]
        limit = self._next_limit()
        for j in range(self.spills):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb') as f:
                m.mergeCombiners(self.serializer.load_stream(f), 0)
            if get_used_memory() > limit:
                m._spill()
                limit = self._next_limit()
        return m._external_items()
--------------------------------------------------

--- SAMPLE #26 ---
DOCSTRING: Sort the elements in iterator, do external sort when the memory goes above the limit.
CODE:
def sorted(self, iterator, key=None, reverse=False):
        global MemoryBytesSpilled, DiskBytesSpilled
        batch, limit = 100, self._next_limit()
        chunks, current_chunk = [], []
        iterator = iter(iterator)
        while True:
            chunk = list(itertools.islice(iterator, batch))
            current_chunk.extend(chunk)
            if len(chunk) < batch:
                break
            used_memory = get_used_memory()
            if used_memory > limit:
                current_chunk.sort(key=key, reverse=reverse)
                path = self._get_path(len(chunks))
                with open(path, 'wb') as f:
                    self.serializer.dump_stream(current_chunk, f)
                def load(f):
                    for v in self.serializer.load_stream(f):
                        yield v
                    f.close()
                chunks.append(load(open(path, 'rb')))
                current_chunk = []
                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
                DiskBytesSpilled += os.path.getsize(path)
                os.unlink(path)  
            elif not chunks:
                batch = min(int(batch * 1.5), 10000)
        current_chunk.sort(key=key, reverse=reverse)
        if not chunks:
            return current_chunk
        if current_chunk:
            chunks.append(iter(current_chunk))
        return heapq.merge(chunks, key=key, reverse=reverse)
--------------------------------------------------

--- SAMPLE #27 ---
DOCSTRING: dump the values into disk
CODE:
def _spill(self):
        global MemoryBytesSpilled, DiskBytesSpilled
        if self._file is None:
            self._open_file()
        used_memory = get_used_memory()
        pos = self._file.tell()
        self._ser.dump_stream(self.values, self._file)
        self.values = []
        gc.collect()
        DiskBytesSpilled += self._file.tell() - pos
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #28 ---
DOCSTRING: dump already partitioned data into disks.
CODE:
def _spill(self):
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)
        used_memory = get_used_memory()
        if not self.pdata:
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]
            self._sorted = len(self.data) < self.SORT_KEY_LIMIT
            if self._sorted:
                self.serializer = self.flattened_serializer()
                for k in sorted(self.data.keys()):
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, self.data[k])], streams[h])
            else:
                for k, v in self.data.items():
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, v)], streams[h])
            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()
            self.data.clear()
            self.pdata.extend([{} for i in range(self.partitions)])
        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, "wb") as f:
                    if self._sorted:
                        sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))
                        self.serializer.dump_stream(sorted_items, f)
                    else:
                        self.serializer.dump_stream(self.pdata[i].items(), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)
        self.spills += 1
        gc.collect()  
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #29 ---
DOCSTRING: load a partition from disk, then sort and group by key
CODE:
def _merge_sorted_items(self, index):
        def load_partition(j):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb', 65536) as f:
                for v in self.serializer.load_stream(f):
                    yield v
        disk_items = [load_partition(j) for j in range(self.spills)]
        if self._sorted:
            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))
        else:
            ser = self.flattened_serializer()
            sorter = ExternalSorter(self.memory_limit, ser)
            sorted_items = sorter.sorted(itertools.chain(*disk_items),
                                         key=operator.itemgetter(0))
        return ((k, vs) for k, vs in GroupByKey(sorted_items))
--------------------------------------------------

--- SAMPLE #30 ---
DOCSTRING: Called by a worker process after the fork().
CODE:
def worker(sock, authenticated):
    signal.signal(SIGHUP, SIG_DFL)
    signal.signal(SIGCHLD, SIG_DFL)
    signal.signal(SIGTERM, SIG_DFL)
    signal.signal(SIGINT, signal.default_int_handler)
    infile = os.fdopen(os.dup(sock.fileno()), "rb", 65536)
    outfile = os.fdopen(os.dup(sock.fileno()), "wb", 65536)
    if not authenticated:
        client_secret = UTF8Deserializer().loads(infile)
        if os.environ["PYTHON_WORKER_FACTORY_SECRET"] == client_secret:
            write_with_length("ok".encode("utf-8"), outfile)
            outfile.flush()
        else:
            write_with_length("err".encode("utf-8"), outfile)
            outfile.flush()
            sock.close()
            return 1
    exit_code = 0
    try:
        worker_main(infile, outfile)
    except SystemExit as exc:
        exit_code = compute_real_exit_code(exc.code)
    finally:
        try:
            outfile.flush()
        except Exception:
            pass
    return exit_code
--------------------------------------------------

--- SAMPLE #31 ---
DOCSTRING: This function returns consistent hash code for builtin types, especially for None and tuple with None.
CODE:
def portable_hash(x):
    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:
        raise Exception("Randomness of hash of string should be disabled via PYTHONHASHSEED")
    if x is None:
        return 0
    if isinstance(x, tuple):
        h = 0x345678
        for i in x:
            h ^= portable_hash(i)
            h *= 1000003
            h &= sys.maxsize
        h ^= len(x)
        if h == -1:
            h = -2
        return int(h)
    return hash(x)
--------------------------------------------------

--- SAMPLE #32 ---
DOCSTRING: Parse a memory string in the format supported by Java (e.g. 1g, 200m) and return the value in MiB
CODE:
def _parse_memory(s):
    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}
    if s[-1].lower() not in units:
        raise ValueError("invalid format: " + s)
    return int(float(s[:-1]) * units[s[-1].lower()])
--------------------------------------------------

--- SAMPLE #33 ---
DOCSTRING: Return a sampled subset of this RDD.
CODE:
def sample(self, withReplacement, fraction, seed=None):
        assert fraction >= 0.0, "Negative fraction value: %s" % fraction
        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)
--------------------------------------------------

--- SAMPLE #34 ---
DOCSTRING: Randomly splits this RDD with the provided weights.
CODE:
def randomSplit(self, weights, seed=None):
        s = float(sum(weights))
        cweights = [0.0]
        for w in weights:
            cweights.append(cweights[-1] + w / s)
        if seed is None:
            seed = random.randint(0, 2 ** 32 - 1)
        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)
                for lb, ub in zip(cweights, cweights[1:])]
--------------------------------------------------

--- SAMPLE #35 ---
DOCSTRING: Return a fixed-size sampled subset of this RDD.
CODE:
def takeSample(self, withReplacement, num, seed=None):
        numStDev = 10.0
        if num < 0:
            raise ValueError("Sample size cannot be negative.")
        elif num == 0:
            return []
        initialCount = self.count()
        if initialCount == 0:
            return []
        rand = random.Random(seed)
        if (not withReplacement) and num >= initialCount:
            samples = self.collect()
            rand.shuffle(samples)
            return samples
        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))
        if num > maxSampleSize:
            raise ValueError(
                "Sample size cannot be greater than %d." % maxSampleSize)
        fraction = RDD._computeFractionForSampleSize(
            num, initialCount, withReplacement)
        samples = self.sample(withReplacement, fraction, seed).collect()
        while len(samples) < num:
            seed = rand.randint(0, sys.maxsize)
            samples = self.sample(withReplacement, fraction, seed).collect()
        rand.shuffle(samples)
        return samples[0:num]
--------------------------------------------------

--- SAMPLE #36 ---
DOCSTRING: Returns a sampling rate that guarantees a sample of size >= sampleSizeLowerBound 99.99% of the time.
CODE:
def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):
        fraction = float(sampleSizeLowerBound) / total
        if withReplacement:
            numStDev = 5
            if (sampleSizeLowerBound < 12):
                numStDev = 9
            return fraction + numStDev * sqrt(fraction / total)
        else:
            delta = 0.00005
            gamma = - log(delta) / total
            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))
--------------------------------------------------

--- SAMPLE #37 ---
DOCSTRING: Return the union of this RDD and another one.
CODE:
def union(self, other):
        if self._jrdd_deserializer == other._jrdd_deserializer:
            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,
                      self._jrdd_deserializer)
        else:
            self_copy = self._reserialize()
            other_copy = other._reserialize()
            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,
                      self.ctx.serializer)
        if (self.partitioner == other.partitioner and
                self.getNumPartitions() == rdd.getNumPartitions()):
            rdd.partitioner = self.partitioner
        return rdd
--------------------------------------------------

--- SAMPLE #38 ---
DOCSTRING: Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys.
CODE:
def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,
                                           ascending=True, keyfunc=lambda x: x):
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        memory = _parse_memory(self.ctx._conf.get("spark.python.worker.memory", "512m"))
        serializer = self._jrdd_deserializer
        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))
        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)
--------------------------------------------------

--- SAMPLE #39 ---
DOCSTRING: Sorts this RDD, which is assumed to consist of (key, value) pairs.
CODE:
def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        memory = self._memory_limit()
        serializer = self._jrdd_deserializer
        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))
        if numPartitions == 1:
            if self.getNumPartitions() > 1:
                self = self.coalesce(1)
            return self.mapPartitions(sortPartition, True)
        rddSize = self.count()
        if not rddSize:
            return self  
        maxSampleSize = numPartitions * 20.0  
        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)
        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()
        samples = sorted(samples, key=keyfunc)
        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]
                  for i in range(0, numPartitions - 1)]
        def rangePartitioner(k):
            p = bisect.bisect_left(bounds, keyfunc(k))
            if ascending:
                return p
            else:
                return numPartitions - 1 - p
        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)
--------------------------------------------------

--- SAMPLE #40 ---
DOCSTRING: Return an RDD created by piping elements to a forked external process.
CODE:
def pipe(self, command, env=None, checkCode=False):
        if env is None:
            env = dict()
        def func(iterator):
            pipe = Popen(
                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)
            def pipe_objs(out):
                for obj in iterator:
                    s = unicode(obj).rstrip('\n') + '\n'
                    out.write(s.encode('utf-8'))
                out.close()
            Thread(target=pipe_objs, args=[pipe.stdin]).start()
            def check_return_code():
                pipe.wait()
                if checkCode and pipe.returncode:
                    raise Exception("Pipe function `%s' exited "
                                    "with error code %d" % (command, pipe.returncode))
                else:
                    for i in range(0):
                        yield i
            return (x.rstrip(b'\n').decode('utf-8') for x in
                    chain(iter(pipe.stdout.readline, b''), check_return_code()))
        return self.mapPartitions(func)
--------------------------------------------------

--- SAMPLE #41 ---
DOCSTRING: Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally.
CODE:
def reduce(self, f):
        f = fail_on_stopiteration(f)
        def func(iterator):
            iterator = iter(iterator)
            try:
                initial = next(iterator)
            except StopIteration:
                return
            yield reduce(f, iterator, initial)
        vals = self.mapPartitions(func).collect()
        if vals:
            return reduce(f, vals)
        raise ValueError("Can not reduce() empty RDD")
--------------------------------------------------

--- SAMPLE #42 ---
DOCSTRING: Reduces the elements of this RDD in a multi-level tree pattern.
CODE:
def treeReduce(self, f, depth=2):
        if depth < 1:
            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)
        zeroValue = None, True  
        def op(x, y):
            if x[1]:
                return y
            elif y[1]:
                return x
            else:
                return f(x[0], y[0]), False
        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)
        if reduced[1]:
            raise ValueError("Cannot reduce empty RDD.")
        return reduced[0]
--------------------------------------------------

--- SAMPLE #43 ---
DOCSTRING: Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral "zero value."
CODE:
def fold(self, zeroValue, op):
        op = fail_on_stopiteration(op)
        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = op(acc, obj)
            yield acc
        vals = self.mapPartitions(func).collect()
        return reduce(op, vals, zeroValue)
--------------------------------------------------

--- SAMPLE #44 ---
DOCSTRING: Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral "zero value."
CODE:
def aggregate(self, zeroValue, seqOp, combOp):
        seqOp = fail_on_stopiteration(seqOp)
        combOp = fail_on_stopiteration(combOp)
        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        vals = self.mapPartitions(func).collect()
        return reduce(combOp, vals, zeroValue)
--------------------------------------------------

--- SAMPLE #45 ---
DOCSTRING: Aggregates the elements of this RDD in a multi-level tree pattern.
CODE:
def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):
        if depth < 1:
            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)
        if self.getNumPartitions() == 0:
            return zeroValue
        def aggregatePartition(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        partiallyAggregated = self.mapPartitions(aggregatePartition)
        numPartitions = partiallyAggregated.getNumPartitions()
        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)
        while numPartitions > scale + numPartitions / scale:
            numPartitions /= scale
            curNumPartitions = int(numPartitions)
            def mapPartition(i, iterator):
                for obj in iterator:
                    yield (i % curNumPartitions, obj)
            partiallyAggregated = partiallyAggregated \
                .mapPartitionsWithIndex(mapPartition) \
                .reduceByKey(combOp, curNumPartitions) \
                .values()
        return partiallyAggregated.reduce(combOp)
--------------------------------------------------

--- SAMPLE #46 ---
DOCSTRING: Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.
CODE:
def countByValue(self):
        def countPartition(iterator):
            counts = defaultdict(int)
            for obj in iterator:
                counts[obj] += 1
            yield counts
        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] += v
            return m1
        return self.mapPartitions(countPartition).reduce(mergeMaps)
--------------------------------------------------

--- SAMPLE #47 ---
DOCSTRING: Get the top N elements from an RDD.
CODE:
def top(self, num, key=None):
        def topIterator(iterator):
            yield heapq.nlargest(num, iterator, key=key)
        def merge(a, b):
            return heapq.nlargest(num, a + b, key=key)
        return self.mapPartitions(topIterator).reduce(merge)
--------------------------------------------------

--- SAMPLE #48 ---
DOCSTRING: Take the first num elements of the RDD.
CODE:
def take(self, num):
        items = []
        totalParts = self.getNumPartitions()
        partsScanned = 0
        while len(items) < num and partsScanned < totalParts:
            numPartsToTry = 1
            if partsScanned > 0:
                if len(items) == 0:
                    numPartsToTry = partsScanned * 4
                else:
                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned
                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)
            left = num - len(items)
            def takeUpToNumLeft(iterator):
                iterator = iter(iterator)
                taken = 0
                while taken < left:
                    try:
                        yield next(iterator)
                    except StopIteration:
                        return
                    taken += 1
            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
            res = self.context.runJob(self, takeUpToNumLeft, p)
            items += res
            partsScanned += numPartsToTry
        return items[:num]
--------------------------------------------------

--- SAMPLE #49 ---
DOCSTRING: Save this RDD as a text file, using string representations of elements.
CODE:
def saveAsTextFile(self, path, compressionCodecClass=None):
        def func(split, iterator):
            for x in iterator:
                if not isinstance(x, (unicode, bytes)):
                    x = unicode(x)
                if isinstance(x, unicode):
                    x = x.encode("utf-8")
                yield x
        keyed = self.mapPartitionsWithIndex(func)
        keyed._bypass_serializer = True
        if compressionCodecClass:
            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)
        else:
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)
--------------------------------------------------

--- SAMPLE #50 ---
DOCSTRING: Merge the values for each key using an associative and commutative reduce function, but return the results immediately to the master as a dictionary.
CODE:
def reduceByKeyLocally(self, func):
        func = fail_on_stopiteration(func)
        def reducePartition(iterator):
            m = {}
            for k, v in iterator:
                m[k] = func(m[k], v) if k in m else v
            yield m
        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] = func(m1[k], v) if k in m1 else v
            return m1
        return self.mapPartitions(reducePartition).reduce(mergeMaps)
--------------------------------------------------

--- SAMPLE #51 ---
DOCSTRING: Return a copy of the RDD partitioned using the specified partitioner.
CODE:
def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        partitioner = Partitioner(numPartitions, partitionFunc)
        if self.partitioner == partitioner:
            return self
        outputSerializer = self.ctx._unbatched_serializer
        limit = (_parse_memory(self.ctx._conf.get(
            "spark.python.worker.memory", "512m")) / 2)
        def add_shuffle_key(split, iterator):
            buckets = defaultdict(list)
            c, batch = 0, min(10 * numPartitions, 1000)
            for k, v in iterator:
                buckets[partitionFunc(k) % numPartitions].append((k, v))
                c += 1
                if (c % 1000 == 0 and get_used_memory() > limit
                        or c > batch):
                    n, size = len(buckets), 0
                    for split in list(buckets.keys()):
                        yield pack_long(split)
                        d = outputSerializer.dumps(buckets[split])
                        del buckets[split]
                        yield d
                        size += len(d)
                    avg = int(size / n) >> 20
                    if avg < 1:
                        batch *= 1.5
                    elif avg > 10:
                        batch = max(int(batch / 1.5), 1)
                    c = 0
            for split, items in buckets.items():
                yield pack_long(split)
                yield outputSerializer.dumps(items)
        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)
        keyed._bypass_serializer = True
        with SCCallSiteSync(self.context) as css:
            pairRDD = self.ctx._jvm.PairwiseRDD(
                keyed._jrdd.rdd()).asJavaPairRDD()
            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,
                                                           id(partitionFunc))
        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))
        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))
        rdd.partitioner = partitioner
        return rdd
--------------------------------------------------

--- SAMPLE #52 ---
DOCSTRING: Generic function to combine the elements for each key using a custom set of aggregation functions.
CODE:
def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None, partitionFunc=portable_hash):
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        serializer = self.ctx.serializer
        memory = self._memory_limit()
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)
        def combineLocally(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()
        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)
        def _mergeCombiners(iterator):
            merger = ExternalMerger(agg, memory, serializer)
            merger.mergeCombiners(iterator)
            return merger.items()
        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)
--------------------------------------------------

--- SAMPLE #53 ---
DOCSTRING: Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.
CODE:
def groupByKey(self, numPartitions=None, partitionFunc=portable_hash):
        def createCombiner(x):
            return [x]
        def mergeValue(xs, x):
            xs.append(x)
            return xs
        def mergeCombiners(a, b):
            a.extend(b)
            return a
        memory = self._memory_limit()
        serializer = self._jrdd_deserializer
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)
        def combine(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()
        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)
        def groupByKey(it):
            merger = ExternalGroupBy(agg, memory, serializer)
            merger.mergeCombiners(it)
            return merger.items()
        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)
--------------------------------------------------

--- SAMPLE #54 ---
DOCSTRING: Return a subset of this RDD sampled by key (via stratified sampling). Create a sample of this RDD using variable sampling rates for different keys as specified by fractions, a key to sampling rate map.
CODE:
def sampleByKey(self, withReplacement, fractions, seed=None):
        for fraction in fractions.values():
            assert fraction >= 0.0, "Negative fraction value: %s" % fraction
        return self.mapPartitionsWithIndex(
            RDDStratifiedSampler(withReplacement, fractions, seed).func, True)
--------------------------------------------------

--- SAMPLE #55 ---
DOCSTRING: Return each (key, value) pair in C{self} that has no pair with matching key in C{other}.
CODE:
def subtractByKey(self, other, numPartitions=None):
        def filter_func(pair):
            key, (val1, val2) = pair
            return val1 and not val2
        return self.cogroup(other, numPartitions).filter(filter_func).flatMapValues(lambda x: x[0])
--------------------------------------------------

--- SAMPLE #56 ---
DOCSTRING: Return a new RDD that is reduced into `numPartitions` partitions.
CODE:
def coalesce(self, numPartitions, shuffle=False):
        if shuffle:
            batchSize = min(10, self.ctx._batchSize or 1024)
            ser = BatchedSerializer(PickleSerializer(), batchSize)
            selfCopy = self._reserialize(ser)
            jrdd_deserializer = selfCopy._jrdd_deserializer
            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)
        else:
            jrdd_deserializer = self._jrdd_deserializer
            jrdd = self._jrdd.coalesce(numPartitions, shuffle)
        return RDD(jrdd, self.ctx, jrdd_deserializer)
--------------------------------------------------

--- SAMPLE #57 ---
DOCSTRING: Zips this RDD with its element indices.
CODE:
def zipWithIndex(self):
        starts = [0]
        if self.getNumPartitions() > 1:
            nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
            for i in range(len(nums) - 1):
                starts.append(starts[-1] + nums[i])
        def func(k, it):
            for i, v in enumerate(it, starts[k]):
                yield v, i
        return self.mapPartitionsWithIndex(func)
--------------------------------------------------

--- SAMPLE #58 ---
DOCSTRING: Zips this RDD with generated unique Long ids.
CODE:
def zipWithUniqueId(self):
        n = self.getNumPartitions()
        def func(k, it):
            for i, v in enumerate(it):
                yield v, i * n + k
        return self.mapPartitionsWithIndex(func)
--------------------------------------------------

--- SAMPLE #59 ---
DOCSTRING: Return the list of values in the RDD for key `key`. This operation is done efficiently if the RDD has a known partitioner by only searching the partition that the key maps to.
CODE:
def lookup(self, key):
        values = self.filter(lambda kv: kv[0] == key).values()
        if self.partitioner is not None:
            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])
        return values.collect()
--------------------------------------------------

--- SAMPLE #60 ---
DOCSTRING: .. note:: Experimental
CODE:
def sumApprox(self, timeout, confidence=0.95):
        jrdd = self.mapPartitions(lambda it: [float(sum(it))])._to_java_object_rdd()
        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())
        r = jdrdd.sumApprox(timeout, confidence).getFinalValue()
        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())
--------------------------------------------------

--- SAMPLE #61 ---
DOCSTRING: .. note:: Experimental
CODE:
def countApproxDistinct(self, relativeSD=0.05):
        if relativeSD < 0.000017:
            raise ValueError("relativeSD should be greater than 0.000017")
        hashRDD = self.map(lambda x: portable_hash(x) & 0xFFFFFFFF)
        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)
--------------------------------------------------

--- SAMPLE #62 ---
DOCSTRING: Create a method for given binary operator
CODE:
def _bin_op(name, doc="binary operator"):
    def _(self, other):
        jc = other._jc if isinstance(other, Column) else other
        njc = getattr(self._jc, name)(jc)
        return Column(njc)
    _.__doc__ = doc
    return _
--------------------------------------------------

--- SAMPLE #63 ---
DOCSTRING: Create a method for binary operator (this object is on right side)
CODE:
def _reverse_op(name, doc="binary operator"):
    def _(self, other):
        jother = _create_column_from_literal(other)
        jc = getattr(jother, name)(self._jc)
        return Column(jc)
    _.__doc__ = doc
    return _
--------------------------------------------------

--- SAMPLE #64 ---
DOCSTRING: Return a :class:`Column` which is a substring of the column.
CODE:
def substr(self, startPos, length):
        if type(startPos) != type(length):
            raise TypeError(
                "startPos and length must be the same type. "
                "Got {startPos_t} and {length_t}, respectively."
                .format(
                    startPos_t=type(startPos),
                    length_t=type(length),
                ))
        if isinstance(startPos, int):
            jc = self._jc.substr(startPos, length)
        elif isinstance(startPos, Column):
            jc = self._jc.substr(startPos._jc, length._jc)
        else:
            raise TypeError("Unexpected type: %s" % type(startPos))
        return Column(jc)
--------------------------------------------------

--- SAMPLE #65 ---
DOCSTRING: A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.
CODE:
def isin(self, *cols):
        if len(cols) == 1 and isinstance(cols[0], (list, set)):
            cols = cols[0]
        cols = [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols]
        sc = SparkContext._active_spark_context
        jc = getattr(self._jc, "isin")(_to_seq(sc, cols))
        return Column(jc)
--------------------------------------------------

--- SAMPLE #66 ---
DOCSTRING: Returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).
CODE:
def alias(self, *alias, **kwargs):
        metadata = kwargs.pop('metadata', None)
        assert not kwargs, 'Unexpected kwargs where passed: %s' % kwargs
        sc = SparkContext._active_spark_context
        if len(alias) == 1:
            if metadata:
                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(
                    json.dumps(metadata))
                return Column(getattr(self._jc, "as")(alias[0], jmeta))
            else:
                return Column(getattr(self._jc, "as")(alias[0]))
        else:
            if metadata:
                raise ValueError('metadata can only be provided for a single column')
            return Column(getattr(self._jc, "as")(_to_seq(sc, list(alias))))
--------------------------------------------------

--- SAMPLE #67 ---
DOCSTRING: Convert the column into type ``dataType``.
CODE:
def cast(self, dataType):
        if isinstance(dataType, basestring):
            jc = self._jc.cast(dataType)
        elif isinstance(dataType, DataType):
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.getOrCreate()
            jdt = spark._jsparkSession.parseDataType(dataType.json())
            jc = self._jc.cast(jdt)
        else:
            raise TypeError("unexpected type: %s" % type(dataType))
        return Column(jc)
--------------------------------------------------

--- SAMPLE #68 ---
DOCSTRING: Evaluates a list of conditions and returns one of multiple possible result expressions. If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.
CODE:
def when(self, condition, value):
        if not isinstance(condition, Column):
            raise TypeError("condition should be a Column")
        v = value._jc if isinstance(value, Column) else value
        jc = self._jc.when(condition._jc, v)
        return Column(jc)
--------------------------------------------------

--- SAMPLE #69 ---
DOCSTRING: Define a windowing column.
CODE:
def over(self, window):
        from pyspark.sql.window import WindowSpec
        if not isinstance(window, WindowSpec):
            raise TypeError("window should be WindowSpec")
        jc = self._jc.over(window._jspec)
        return Column(jc)
--------------------------------------------------

--- SAMPLE #70 ---
DOCSTRING: Transforms the input document (list of terms) to term frequency vectors, or transform the RDD of document to RDD of term frequency vectors.
CODE:
def transform(self, document):
        if isinstance(document, RDD):
            return document.map(self.transform)
        freq = {}
        for term in document:
            i = self.indexOf(term)
            freq[i] = 1.0 if self.binary else freq.get(i, 0) + 1.0
        return Vectors.sparse(self.numFeatures, freq.items())
--------------------------------------------------

--- SAMPLE #71 ---
DOCSTRING: Computes the inverse document frequency.
CODE:
def fit(self, dataset):
        if not isinstance(dataset, RDD):
            raise TypeError("dataset should be an RDD of term frequency vectors")
        jmodel = callMLlibFunc("fitIDF", self.minDocFreq, dataset.map(_convert_to_vector))
        return IDFModel(jmodel)
--------------------------------------------------

--- SAMPLE #72 ---
DOCSTRING: Find synonyms of a word
CODE:
def findSynonyms(self, word, num):
        if not isinstance(word, basestring):
            word = _convert_to_vector(word)
        words, similarity = self.call("findSynonyms", word, num)
        return zip(words, similarity)
--------------------------------------------------

--- SAMPLE #73 ---
DOCSTRING: Train a decision tree model for classification.
CODE:
def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,
                        impurity="gini", maxDepth=5, maxBins=32, minInstancesPerNode=1,
                        minInfoGain=0.0):
        return cls._train(data, "classification", numClasses, categoricalFeaturesInfo,
                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)
--------------------------------------------------

--- SAMPLE #74 ---
DOCSTRING: Train a random forest model for binary or multiclass classification.
CODE:
def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, numTrees,
                        featureSubsetStrategy="auto", impurity="gini", maxDepth=4, maxBins=32,
                        seed=None):
        return cls._train(data, "classification", numClasses,
                          categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity,
                          maxDepth, maxBins, seed)
--------------------------------------------------

--- SAMPLE #75 ---
DOCSTRING: Train a random forest model for regression.
CODE:
def trainRegressor(cls, data, categoricalFeaturesInfo, numTrees, featureSubsetStrategy="auto",
                       impurity="variance", maxDepth=4, maxBins=32, seed=None):
        return cls._train(data, "regression", 0, categoricalFeaturesInfo, numTrees,
                          featureSubsetStrategy, impurity, maxDepth, maxBins, seed)
--------------------------------------------------

--- SAMPLE #76 ---
DOCSTRING: Set an environment variable to be passed to executors.
CODE:
def setExecutorEnv(self, key=None, value=None, pairs=None):
        if (key is not None and pairs is not None) or (key is None and pairs is None):
            raise Exception("Either pass one key-value pair or a list of pairs")
        elif key is not None:
            self.set("spark.executorEnv." + key, value)
        elif pairs is not None:
            for (k, v) in pairs:
                self.set("spark.executorEnv." + k, v)
        return self
--------------------------------------------------

--- SAMPLE #77 ---
DOCSTRING: Get the configured value for some key, or return a default otherwise.
CODE:
def get(self, key, defaultValue=None):
        if defaultValue is None:   
            if self._jconf is not None:
                if not self._jconf.contains(key):
                    return None
                return self._jconf.get(key)
            else:
                if key not in self._conf:
                    return None
                return self._conf[key]
        else:
            if self._jconf is not None:
                return self._jconf.get(key, defaultValue)
            else:
                return self._conf.get(key, defaultValue)
--------------------------------------------------

--- SAMPLE #78 ---
DOCSTRING: Returns a printable version of the configuration, as a list of key=value pairs, one per line.
CODE:
def toDebugString(self):
        if self._jconf is not None:
            return self._jconf.toDebugString()
        else:
            return '\n'.join('%s=%s' % (k, v) for k, v in self._conf.items())
--------------------------------------------------

--- SAMPLE #79 ---
DOCSTRING: Returns a list of tables/views in the specified database.
CODE:
def listTables(self, dbName=None):
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listTables(dbName).toLocalIterator()
        tables = []
        while iter.hasNext():
            jtable = iter.next()
            tables.append(Table(
                name=jtable.name(),
                database=jtable.database(),
                description=jtable.description(),
                tableType=jtable.tableType(),
                isTemporary=jtable.isTemporary()))
        return tables
--------------------------------------------------

--- SAMPLE #80 ---
DOCSTRING: Returns a list of functions registered in the specified database.
CODE:
def listFunctions(self, dbName=None):
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listFunctions(dbName).toLocalIterator()
        functions = []
        while iter.hasNext():
            jfunction = iter.next()
            functions.append(Function(
                name=jfunction.name(),
                description=jfunction.description(),
                className=jfunction.className(),
                isTemporary=jfunction.isTemporary()))
        return functions
--------------------------------------------------

--- SAMPLE #81 ---
DOCSTRING: Returns a list of columns for the given table/view in the specified database.
CODE:
def listColumns(self, tableName, dbName=None):
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listColumns(dbName, tableName).toLocalIterator()
        columns = []
        while iter.hasNext():
            jcolumn = iter.next()
            columns.append(Column(
                name=jcolumn.name(),
                description=jcolumn.description(),
                dataType=jcolumn.dataType(),
                nullable=jcolumn.nullable(),
                isPartition=jcolumn.isPartition(),
                isBucket=jcolumn.isBucket()))
        return columns
--------------------------------------------------

--- SAMPLE #82 ---
DOCSTRING: Creates a table based on the dataset in a data source.
CODE:
def createExternalTable(self, tableName, path=None, source=None, schema=None, **options):
        warnings.warn(
            "createExternalTable is deprecated since Spark 2.2, please use createTable instead.",
            DeprecationWarning)
        return self.createTable(tableName, path, source, schema, **options)
--------------------------------------------------

--- SAMPLE #83 ---
DOCSTRING: Creates a table based on the dataset in a data source.
CODE:
def createTable(self, tableName, path=None, source=None, schema=None, **options):
        if path is not None:
            options["path"] = path
        if source is None:
            source = self._sparkSession._wrapped._conf.defaultDataSourceName()
        if schema is None:
            df = self._jcatalog.createTable(tableName, source, options)
        else:
            if not isinstance(schema, StructType):
                raise TypeError("schema should be StructType")
            scala_datatype = self._jsparkSession.parseDataType(schema.json())
            df = self._jcatalog.createTable(tableName, source, scala_datatype, options)
        return DataFrame(df, self._sparkSession._wrapped)
--------------------------------------------------

--- SAMPLE #84 ---
DOCSTRING: .. note:: Experimental
CODE:
def barrier(self):
        if self._port is None or self._secret is None:
            raise Exception("Not supported to call barrier() before initialize " +
                            "BarrierTaskContext.")
        else:
            _load_from_socket(self._port, self._secret)
--------------------------------------------------

--- SAMPLE #85 ---
DOCSTRING: .. note:: Experimental
CODE:
def getTaskInfos(self):
        if self._port is None or self._secret is None:
            raise Exception("Not supported to call getTaskInfos() before initialize " +
                            "BarrierTaskContext.")
        else:
            addresses = self._localProperties.get("addresses", "")
            return [BarrierTaskInfo(h.strip()) for h in addresses.split(",")]
--------------------------------------------------

--- SAMPLE #86 ---
DOCSTRING: A decorator that annotates a function to append the version of Spark the function was added.
CODE:
def since(version):
    import re
    indent_p = re.compile(r'\n( +)')
    def deco(f):
        indents = indent_p.findall(f.__doc__)
        indent = ' ' * (min(len(m) for m in indents) if indents else 0)
        f.__doc__ = f.__doc__.rstrip() + "\n\n%s.. versionadded:: %s" % (indent, version)
        return f
    return deco
--------------------------------------------------

--- SAMPLE #87 ---
DOCSTRING: Returns a function with same code, globals, defaults, closure, and name (or provide a new name).
CODE:
def copy_func(f, name=None, sinceversion=None, doc=None):
    fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__,
                            f.__closure__)
    fn.__dict__.update(f.__dict__)
    if doc is not None:
        fn.__doc__ = doc
    if sinceversion is not None:
        fn = since(sinceversion)(fn)
    return fn
--------------------------------------------------

--- SAMPLE #88 ---
DOCSTRING: A decorator that forces keyword arguments in the wrapped method and saves actual input keyword arguments in `_input_kwargs`.
CODE:
def keyword_only(func):
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        if len(args) > 0:
            raise TypeError("Method %s forces keyword arguments." % func.__name__)
        self._input_kwargs = kwargs
        return func(self, **kwargs)
    return wrapper
--------------------------------------------------

--- SAMPLE #89 ---
DOCSTRING: Generates the header part for shared variables
CODE:
def _gen_param_header(name, doc, defaultValueStr, typeConverter):
    template = '''class Has$Name(Params):
    """
    Mixin for param $name: $doc
    """
    $name = Param(Params._dummy(), "$name", "$doc", typeConverter=$typeConverter)
    def __init__(self):
        super(Has$Name, self).__init__()'''
    if defaultValueStr is not None:
        template += '''
        self._setDefault($name=$defaultValueStr)'''
    Name = name[0].upper() + name[1:]
    if typeConverter is None:
        typeConverter = str(None)
    return template \
        .replace("$name", name) \
        .replace("$Name", Name) \
        .replace("$doc", doc) \
        .replace("$defaultValueStr", str(defaultValueStr)) \
        .replace("$typeConverter", typeConverter)
--------------------------------------------------

--- SAMPLE #90 ---
DOCSTRING: Generates Python code for a shared param class.
CODE:
def _gen_param_code(name, doc, defaultValueStr):
    template = '''
    def set$Name(self, value):
        """
        Sets the value of :py:attr:`$name`.
        """
        return self._set($name=value)
    def get$Name(self):
        """
        Gets the value of $name or its default value.
        """
        return self.getOrDefault(self.$name)'''
    Name = name[0].upper() + name[1:]
    return template \
        .replace("$name", name) \
        .replace("$Name", Name) \
        .replace("$doc", doc) \
        .replace("$defaultValueStr", str(defaultValueStr))
--------------------------------------------------

--- SAMPLE #91 ---
DOCSTRING: Train a k-means clustering model.
CODE:
def train(cls, rdd, k, maxIterations=100, runs=1, initializationMode="k-means||",
              seed=None, initializationSteps=2, epsilon=1e-4, initialModel=None):
        if runs != 1:
            warnings.warn("The param `runs` has no effect since Spark 2.0.0.")
        clusterInitialModel = []
        if initialModel is not None:
            if not isinstance(initialModel, KMeansModel):
                raise Exception("initialModel is of "+str(type(initialModel))+". It needs "
                                "to be of <type 'KMeansModel'>")
            clusterInitialModel = [_convert_to_vector(c) for c in initialModel.clusterCenters]
        model = callMLlibFunc("trainKMeansModel", rdd.map(_convert_to_vector), k, maxIterations,
                              runs, initializationMode, seed, initializationSteps, epsilon,
                              clusterInitialModel)
        centers = callJavaFunc(rdd.context, model.clusterCenters)
        return KMeansModel([c.toArray() for c in centers])
--------------------------------------------------

--- SAMPLE #92 ---
DOCSTRING: Train a Gaussian Mixture clustering model.
CODE:
def train(cls, rdd, k, convergenceTol=1e-3, maxIterations=100, seed=None, initialModel=None):
        initialModelWeights = None
        initialModelMu = None
        initialModelSigma = None
        if initialModel is not None:
            if initialModel.k != k:
                raise Exception("Mismatched cluster count, initialModel.k = %s, however k = %s"
                                % (initialModel.k, k))
            initialModelWeights = list(initialModel.weights)
            initialModelMu = [initialModel.gaussians[i].mu for i in range(initialModel.k)]
            initialModelSigma = [initialModel.gaussians[i].sigma for i in range(initialModel.k)]
        java_model = callMLlibFunc("trainGaussianMixtureModel", rdd.map(_convert_to_vector),
                                   k, convergenceTol, maxIterations, seed,
                                   initialModelWeights, initialModelMu, initialModelSigma)
        return GaussianMixtureModel(java_model)
--------------------------------------------------

--- SAMPLE #93 ---
DOCSTRING: Update the centroids, according to data
CODE:
def update(self, data, decayFactor, timeUnit):
        if not isinstance(data, RDD):
            raise TypeError("Data should be of an RDD, got %s." % type(data))
        data = data.map(_convert_to_vector)
        decayFactor = float(decayFactor)
        if timeUnit not in ["batches", "points"]:
            raise ValueError(
                "timeUnit should be 'batches' or 'points', got %s." % timeUnit)
        vectorCenters = [_convert_to_vector(center) for center in self.centers]
        updatedModel = callMLlibFunc(
            "updateStreamingKMeansModel", vectorCenters, self._clusterWeights,
            data, decayFactor, timeUnit)
        self.centers = array(updatedModel[0])
        self._clusterWeights = list(updatedModel[1])
        return self
--------------------------------------------------

--- SAMPLE #94 ---
DOCSTRING: Set the initial centres to be random samples from a gaussian population with constant weights.
CODE:
def setRandomCenters(self, dim, weight, seed):
        rng = random.RandomState(seed)
        clusterCenters = rng.randn(self._k, dim)
        clusterWeights = tile(weight, self._k)
        self._model = StreamingKMeansModel(clusterCenters, clusterWeights)
        return self
--------------------------------------------------

--- SAMPLE #95 ---
DOCSTRING: Load the LDAModel from disk.
CODE:
def load(cls, sc, path):
        if not isinstance(sc, SparkContext):
            raise TypeError("sc should be a SparkContext, got type %s" % type(sc))
        if not isinstance(path, basestring):
            raise TypeError("path should be a basestring, got type %s" % type(path))
        model = callMLlibFunc("loadLDAModel", sc, path)
        return LDAModel(model)
--------------------------------------------------

--- SAMPLE #96 ---
DOCSTRING: Train a LDA model.
CODE:
def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,
              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer="em"):
        model = callMLlibFunc("trainLDAModel", rdd, k, maxIterations,
                              docConcentration, topicConcentration, seed,
                              checkpointInterval, optimizer)
        return LDAModel(model)
--------------------------------------------------

--- SAMPLE #97 ---
DOCSTRING: Convert Python object into Java
CODE:
def _py2java(sc, obj):
    if isinstance(obj, RDD):
        obj = _to_java_object_rdd(obj)
    elif isinstance(obj, DataFrame):
        obj = obj._jdf
    elif isinstance(obj, SparkContext):
        obj = obj._jsc
    elif isinstance(obj, list):
        obj = [_py2java(sc, x) for x in obj]
    elif isinstance(obj, JavaObject):
        pass
    elif isinstance(obj, (int, long, float, bool, bytes, unicode)):
        pass
    else:
        data = bytearray(PickleSerializer().dumps(obj))
        obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)
    return obj
--------------------------------------------------

--- SAMPLE #98 ---
DOCSTRING: A decorator that makes a class inherit documentation from its parents.
CODE:
def inherit_doc(cls):
    for name, func in vars(cls).items():
        if name.startswith("_"):
            continue
        if not func.__doc__:
            for parent in cls.__bases__:
                parent_func = getattr(parent, name, None)
                if parent_func and getattr(parent_func, "__doc__", None):
                    func.__doc__ = parent_func.__doc__
                    break
    return cls
--------------------------------------------------

--- SAMPLE #99 ---
DOCSTRING: Return a new DStream by applying combineByKey to each RDD.
CODE:
def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None):
        if numPartitions is None:
            numPartitions = self._sc.defaultParallelism
        def func(rdd):
            return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)
        return self.transform(func)
--------------------------------------------------

--- SAMPLE #100 ---
DOCSTRING: Apply a function to each RDD in this DStream.
CODE:
def foreachRDD(self, func):
        if func.__code__.co_argcount == 1:
            old_func = func
            func = lambda t, rdd: old_func(rdd)
        jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer)
        api = self._ssc._jvm.PythonDStream
        api.callForeachRDD(self._jdstream, jfunc)
--------------------------------------------------

