# ==============================================================================
# MLSA PROJECT - R&D MASTER PIPELINE (COLAB EDITION)
# ==============================================================================

# 1. AGGANCIO AL CLOUD STORAGE (Persistenza Risultati)
from google.colab import drive
import os
import sys
import torch

print("--- [FASE 1] MOUNT E MAPPATURA PERCORSI ---")
drive.mount('/content/drive', force_remount=True)

# Percorso validato dallo Scanner (Chirurgicamente esatto)
PROJECT_ROOT = "/content/drive/MyDrive/LifeLineV4/PROGETTI/Universita/Software_Science_And_Technology/MACHINE LEARNING FOR SOFTWARE ANALYSIS/PROJECT__MLSA_MMA/COLAB_Marco/Project_MLSA_MMA/Project"

if not os.path.exists(PROJECT_ROOT):
    print(f"âŒ ERRORE CRITICO: Il percorso {PROJECT_ROOT} non esiste.")
    print("Controlla se hai cambiato i nomi delle cartelle (es. PROGETTI vs Progetti).")
else:
    # Cambiamo directory di lavoro (Context Shift)
    %cd "{PROJECT_ROOT}"
    # Iniezione nel path di sistema per risolvere i moduli 'models' e 'data'
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)
    print(f"âœ… Directory di lavoro: {os.getcwd()}")

# 2. AUDIT HARDWARE (Hardware-Software Alignment)
print("\n--- [FASE 2] AUDIT HARDWARE ---")
device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cuda":
    print(f"ðŸš€ GPU RILEVATA: {torch.cuda.get_device_name(0)}")
    print(f"ðŸ“Š VRAM DISPONIBILE: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("âš ï¸ ATTENZIONE: GPU NON RILEVATA! Controlla 'Runtime -> Cambia tipo di runtime'.")

# 3. PROVISIONING DIPENDENZE (Ambiente Isolato)
print("\n--- [FASE 3] INSTALLAZIONE STRUMENTI ---")
!pip install tokenizers tqdm nltk rouge-score -q

# 4. ESECUZIONE ORCHESTRATORE (L'Attacco Finale)
# Nota Ingegneristica: Decommenta SOLO la riga dell'esperimento desiderato.
# I parametri sono calibrati per saturare la VRAM di una Tesla T4 senza crash.

print("\n--- [FASE 4] AVVIO ADDESTRAMENTO PESANTE ---")

# --- OPZIONE A: TRANSFORMER (Il Titano Parallelo) ---
# Ideale per catturare relazioni a lungo raggio nel codice.
!python C2Orchestrator.py --mode train --model transformer --subset 50000 --epochs 10 --batch_size 128 --evaluation fast

# --- OPZIONE B: LSTM + BAHDANAU (Attenzione Additiva) ---
# Il modello "Sarto" che impara dinamicamente dove guardare.
#!python C2Orchestrator.py --mode train --model lstm_bahdanau --subset 50000 --epochs 10 --batch_size 64 --evaluation fast

# --- OPZIONE C: LSTM + DOTPRODUCT (Attenzione del Prof) ---
# Implementazione basata sul prodotto scalare scalato (Scaled Dot-Product).
#!python C2Orchestrator.py --mode train --model lstm_dotproduct --subset 50000 --epochs 10 --batch_size 64 --evaluation fast

print("\n--- PIPELINE COMPLETATA CON SUCCESSO ---")