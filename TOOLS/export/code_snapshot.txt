================================================================================
Code Snapshot for Project: Project
Generated on: 2026-02-05 00:31:30
================================================================================

--- FILE: C2Orchestrator.py | PATH: Project\C2Orchestrator.py ---
-----------------------------------------------------------------

"""
================================================================================
C2 MASTER ORCHESTRATOR - (Code Analysis & Summarization System)
================================================================================
ROLE: Central Management Unit (CMU) for the Neural Pipeline.

DESIGN PATTERN: 
- Modular Pipeline: Strictly separates Data Ingestion, Refinery, and Training.
- Factory Pattern: Decouples architecture selection from the training loop.
- Phase-Gated Execution: Ensures prerequisites (e.g., tokenization) are met 
  before high-compute stages (training).

SYSTEM GOAL: Transform raw Python source code into concise natural language 
summaries by optimizing the signal-to-noise ratio in latent space.
================================================================================
"""

import os
import argparse
import logging
import torch
import time
import sys
from datetime import datetime
from tokenizers import Tokenizer

# --- [GLOBAL CONFIGURATION] ---
# Tracking logic evolution. 4.4.0 introduces Internal Target Shifting 
# and differentiated regularization (Dropout/Weight Decay) for Transformers.
PIPELINE_VERSION = "4.4.6-Origin" 
AUTHORS = "Matteo Bollecchino, Marco Pietri, Alessandro Nesti."

# --- [DOMAIN ISOLATION: MODULE IMPORTS] ---
from data.download_dataset import download_codesearchnet_robust
from data.inspect_dataset import save_human_readable_samples
from data.tokenizer import build_tokenizer
from data.dataset import get_dataloader
from models.factory import get_model_architecture
from scripts.train import train_model 
from scripts.log_manager import ExecutionLogger 
from evaluate import BatchEvaluator 
from data.dataset_cleaner import DatasetCleaner

# Standard Logger setup to track pipeline progress and critical errors.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- [MASTER PIPELINE CLASS] ---
class CodeSummarizationPipeline:
    """ 
    Orchestrator Class managing the end-to-end Machine Learning lifecycle.
    Encapsulates filesystem hooks, hardware affinity, and phase transitions.
    """

    def __init__(self, config):
        """
        Constructor: Configures paths and audits local hardware capabilities.
        
        Args:
            config (argparse.Namespace): Object containing runtime flags 
                                         (model type, epochs, batch size, etc.)
        """
        self.config = config
        self.root = os.path.dirname(os.path.abspath(__file__))

        # Path Definitions: Strictly defined to ensure "Source of Truth" for data flow.
        self.dataset_root = os.path.join(self.root, "Datasets")
        self.jsonl_base = os.path.join(self.dataset_root, "python", "final", "jsonl")
        self.tokenizer_path = os.path.join(self.root, "tokenizer.json")
        self.checkpoint_dir = os.path.join(self.root, "models", "checkpoints")
        self.processed_dir = os.path.join(self.dataset_root, "processed")

        print(f"\n" + "="*120)
        print(f"CODE SUMMARIZATION PROJECT | Version: {PIPELINE_VERSION} | Authors: {AUTHORS}")
        print(f"powered by PyTorch {torch.__version__} |  C2 Orchestrator Initialized")
        print(f"Vaswani: 'Attention Is All You Need'. NVIDIA: 'Hold my beer...'")
        print(f"Session Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*120)

        # --- HARDWARE AUDIT & ISOLATION ---
        # Logic to detect and isolate compute resources.
        # Index '1' is often forced for dedicated high-performance GPUs.
        print(f"[*] Inspecting Hardware capabilities...")
        device_count = torch.cuda.device_count()
        if device_count > 0:
            for i in range(device_count):
                print(f"    - GPU Index {i}: {torch.cuda.get_device_name(i)}")
            
            # Forced Device Affinity: Ensures the pipeline targets the correct VRAM bank.
            os.environ["CUDA_VISIBLE_DEVICES"] = "1"
            print(f"[!] Strict Device Isolation: CUDA_VISIBLE_DEVICES forced to '1'")
        else:
            print("    - No GPU detected. Falling back to CPU (Throughput limited).")

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"[*] Target Compute Device: {self.device}")
        
        # Audit Fidelity Mapping: Controls the statistical significance of the evaluation.
        # 'deep' mode provides higher confidence but increases inference latency.
        self.eval_map = {"instant": 100, "fast": 200, "deep": 1000}
        self.eval_samples = self.eval_map[self.config.evaluation]

    def run(self):
        """ 
        Main Entry Point: Directs the flow between Training and Audit modes.
        """
        logger.info(f"--- MASTER PIPELINE START | v{PIPELINE_VERSION} | Mode: {self.config.mode.upper()} ---")
        
        if self.config.mode == "train":
            # Safety Interlock: Training requires a defined target architecture.
            if not self.config.model:
                logger.error("Error: In 'train' mode the --model parameter is MANDATORY.")
                return
            self._execute_training()
        else:
            self._execute_audit()

    def _execute_training(self):
        """ 
        The "Training State Machine": Executes five sequential phases of ML development.
        Implements detailed telemetry logging for post-mortem performance analysis.
        """
        # Initialize telemetry manager to capture system logs and loss curves.
        telemetry = ExecutionLogger(self.root, self.config.model, self.config.subset)
        
        # Record Pipeline Metadata for reproducibility.
        telemetry._write_to_file(f"PIPELINE_VERSION: {PIPELINE_VERSION}\n")
        telemetry.log_sys_info()

        # --- PHASE 1: DATA INFRASTRUCTURE ---
        # Goal: Ensure raw data availability. Implements reliable retrieval from mirrors.
        logger.info("Phase 1: Validating Data Infrastructure...")
        t0 = time.time()

        if not os.path.exists(self.jsonl_base) or self.config.force_download:
            download_codesearchnet_robust(self.dataset_root)

        # Generate readable previews of raw data for qualitative human inspection.
        output_dir = os.path.join(self.dataset_root, "Human_readable_sample")
        save_human_readable_samples(self.jsonl_base, output_dir, "data_preview_raw.txt")
        telemetry.log_phase("data_infrastructure", time.time() - t0)

        # --- PHASE 2: DATA REFINERY & MD5 DEDUPLICATION ---
        # Goal: Integrity. MD5 hashing prevents 'Data Contamination' (training on test data).
        # Sanitization removes comments/docstrings to prevent 'Model Cheating'.
        logger.info("Phase 2: Sanitizing Source Code & Target Summaries...")
        t1 = time.time()
        expected_files = ["train.jsonl.gz", "valid.jsonl.gz", "test.jsonl.gz"]
        is_processed = all(os.path.exists(os.path.join(self.processed_dir, f)) for f in expected_files)
        
        if is_processed and not self.config.force_download:
            logger.info("Verified Clean Data found. Skipping cleaning phase.")
        else:
            cleaner = DatasetCleaner()
            cleaner.run(self.jsonl_base, self.processed_dir)
            save_human_readable_samples(self.processed_dir, output_dir, "data_preview_processed.txt")
            
        telemetry.log_phase("data_cleaning", time.time() - t1)

        # --- PHASE 3: TOKENIZATION ---
        # Goal: Symbolic Representation. Converts text into high-dimensional integer IDs.
        # Uses Byte-Pair Encoding (BPE) to handle sub-word units and Python syntax.
        logger.info(f"Phase 3: Building Symbolic Mapping (BPE Vocab: 20000)...")
        t0 = time.time()
        reused = os.path.exists(self.tokenizer_path) and not self.config.force_preprocess

        if not reused:
            build_tokenizer(self.processed_dir, self.tokenizer_path, vocab_size=20000)
        
        tokenizer_duration = time.time() - t0
        tmp_tok = Tokenizer.from_file(self.tokenizer_path)
        vocab_size = tmp_tok.get_vocab_size()
        telemetry.log_tokenizer_info(vocab_size, reused=reused, duration=tokenizer_duration)

        # --- PHASE 4: ARCHITECTURAL SYNTHESIS & TRAINING ---
        # Goal: Optimization. Translates data into neural weights.
        # Differentiated regularizers (Dropout 0.3 for Transformers) are injected here.
        logger.info(f"Phase 4: Initializing {self.config.model.upper()}...")
        model_tag = self.config.model
        filename = f"{datetime.now().strftime('%Y%m%d_%H%M')}_{model_tag}_sub{self.config.subset if self.config.subset else 'all'}.pt"
        model_save_path = os.path.join(self.checkpoint_dir, filename)

        # Dataloader Factory: Optimized with dynamic padding to minimize null computations.
        train_loader = get_dataloader(self.processed_dir, "train", self.tokenizer_path, self.config.batch_size, subset=self.config.subset)
        valid_loader = get_dataloader(self.processed_dir, "valid", self.tokenizer_path, self.config.batch_size, subset=self.config.subset // 5 if self.config.subset else None)

        try:
            t_train_start = time.time()
            # Construct model via Factory pattern to isolate architecture details.
            model = get_model_architecture(model_tag, self.device, vocab_size=vocab_size)
            
            # CORE OPTIMIZATION LOOP: Performs backpropagation and weight updates.
            train_model(model, train_loader, valid_loader, self.config, self.device, telemetry=telemetry)
            
            # Persist optimized weights to disk.
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            logger.info(f"Checkpoint saved: {filename}")
            telemetry.log_phase("model_training", time.time() - t_train_start)

            # --- PHASE 5: AUTO-EVALUATION ---
            # Goal: Metric Validation. Calculates BLEU and ROUGE-L on unseen data.
            # Crucial: Assesses the actual utility of the current session's model.
            logger.info(f"Phase 5: Executing {self.config.evaluation.upper()} Audit...")
            t_eval_start = time.time()
            evaluator = BatchEvaluator(self.device, self.tokenizer_path, self.checkpoint_dir, self.processed_dir, subset_size=self.eval_samples)
            df_results = evaluator.run_all(specific_file=filename)
            telemetry.log_phase(f"evaluation_{self.config.evaluation}", time.time() - t_eval_start)
            
            if df_results is not None:
                telemetry.log_final_metrics(df_results, self.config.evaluation)

        except Exception as e:
            # Exception Handler: Catches and logs runtime errors to prevent telemetry loss.
            logger.error(f"CRITICAL PIPELINE FAILURE: {e}")
            telemetry._write_to_file(f"CRITICAL ERROR | v{PIPELINE_VERSION}: {str(e)}\n")
        finally:
            # Atomic Log Finalization: Renames temp logs with metrics and metadata tags.
            telemetry.finalize()
            print(f"\n[!] Session Finalized. Pipeline Version {PIPELINE_VERSION} signing off.\n")

    def _execute_audit(self):
        """ 
        Audit Module: Executes comparative performance analysis on existing weights.
        Designed for LIFO (Last-In, First-Out) checkpoint review.
        """
        logger.info(f"Audit Mode Active | Pipeline v{PIPELINE_VERSION}")
        
        if not os.path.exists(self.checkpoint_dir):
            logger.error(f"Checkpoint directory not found at {self.checkpoint_dir}")
            return

        # Retrieve and sort available .pt weights chronologically.
        all_ckpts = sorted([f for f in os.listdir(self.checkpoint_dir) if f.endswith(".pt")], reverse=True)
        if not all_ckpts:
            logger.error("No checkpoints available for evaluation.")
            return

        # Target Selection Logic: Allows batch auditing (e.g., '1,2,5' or 'all').
        targets = []
        if self.config.neval.lower() == "all":
            targets = all_ckpts
        else:
            try:
                indices = [int(i.strip()) - 1 for i in self.config.neval.split(",")]
                targets = [all_ckpts[i] for i in indices if 0 <= i < len(all_ckpts)]
            except Exception as e:
                logger.error(f"Invalid index format: {e}")
                return

        # Run Audit: Re-initializes architecture and runs inference on test datasets.
        evaluator = BatchEvaluator(self.device, self.tokenizer_path, self.checkpoint_dir, self.processed_dir, subset_size=self.eval_samples)
        
        for ckpt in targets:
            print(f"\nAnalyzing: {ckpt}")
            df_res = evaluator.run_all(specific_file=ckpt)
            if df_res is not None:
                print(df_res.to_string(index=False))
        
        logger.info(f"Audit Mode Completed.")

# --- ENTRY POINT ---
if __name__ == "__main__":
    # Command Line Interface (CLI) Definition.
    parser = argparse.ArgumentParser(description=f"C2 Master Orchestrator - v{PIPELINE_VERSION}")
    
    # Fundamental Mode Switching.
    parser.add_argument("--mode", type=str, default="train", choices=["train", "eval"], 
                        help="Select training session or checkpoint auditing.")
    
    # Architectural Selection.
    parser.add_argument("--model", type=str, choices=["transformer", "lstm_bahdanau", "lstm_dotproduct"],
                        help="Neural backbone to be synthesized.")
    
    # Hyperparameter & Data Pressure Control.
    parser.add_argument("--subset", type=int, default=None, help="Number of samples to pull from refinery.")
    parser.add_argument("--epochs", type=int, default=10, help="Number of complete passes through training data.")
    parser.add_argument("--batch_size", type=int, default=128, help="Number of samples per stochastic gradient step.")
    
    # Audit Control.
    parser.add_argument("--neval", type=str, default="1", help="Indices of checkpoints to audit (e.g. 1,2,5).")
    parser.add_argument("--evaluation", type=str, default="fast", choices=["instant", "fast", "deep"],
                        help="Depth of evaluation samples (higher is more accurate).")
    
    # Pipeline Flags.
    parser.add_argument("--force_download", action="store_true", help="Ignore local cache and pull raw data.")
    parser.add_argument("--force_preprocess", action="store_true", help="Regenerate tokenizer vocabulary.")
    parser.add_argument("--force_train", action="store_true", help="Overwrite existing sessions.")
    
    # Initialize execution logic.
    args = parser.parse_args()
    pipeline = CodeSummarizationPipeline(args)
    pipeline.run()

================================================================================

--- FILE: README.md | PATH: Project\README.md ---
-------------------------------------------------

# Project Core

This directory contains the core source code for the Code Summarization project.

## Project Structure

- `data/`: Scripts for data downloading, preprocessing, and loading (`dataset.py`, `download_dataset.py`, `preprocess.py`).
- `models/`: Contains the implementation of the neural network models, such as `transformer.py` and `seq2seq.py`.
- `scripts/`: Main scripts to execute training (`train.py`) and other utilities.
- `C2Orchestrator.py`: The main orchestrator script to run training and evaluation tasks.
- `evaluate.py`: Script to evaluate a trained model.
- `requirements.txt`: A list of the Python dependencies for the project.
- `logs/`: Contains log files from training runs.
- `Datasets/`: Default directory where the datasets are stored.

## How to Run

### Prerequisites

1.  Make sure you are in this (`Project`) directory.
2.  Install the required dependencies:
    ```sh
    pip install -r requirements.txt
    ```

### Training Commands

**Transformer Model:**
To train the Transformer model with a subset of 50,000 samples for 10 epochs and a batch size of 128, run:
```sh
python C2Orchestrator.py --model transformer --subset 50000 --epochs 10 --batch_size 128 --force_train
```

**LSTM with Attention Model:**
To train the LSTM with Attention model with a subset of 50,000 samples for 5 epochs and a batch size of 64, run:
```sh
python C2Orchestrator.py --model lstm_attention --subset 50000 --epochs 5 --batch_size 64
```

## Command-line Arguments

The `C2Orchestrator.py` script accepts several arguments to control its behavior:

### Main Modes
-   `--mode`: Choose the operational mode.
    -   `train` (default): Train a new model.
    -   `eval`: Audit and evaluate existing checkpoints.
-   `--model`: (Required for `train` mode) Specifies the model architecture.
    -   `transformer`: Use the Transformer model.
    -   `lstm_attention`: Use the LSTM with Attention model.

### Training Parameters
-   `--subset`: Specifies the number of samples from the dataset to use for training. (e.g., `50000`). If not provided, the entire dataset is used.
-   `--epochs`: The number of complete passes through the training dataset. (default: `10`).
-   `--batch_size`: The number of samples per batch to load. (default: `128`).

### Evaluation Parameters
-   `--neval`: (For `eval` mode) Specifies which saved models to evaluate.
    -   `1` (default): Evaluates the most recent model.
    -   `all`: Evaluates all models found in the checkpoints directory.
    -   `1,2,4`: Evaluates specific models by their index in a reverse-chronological list.
-   `--evaluation`: Determines the number of samples to use for the evaluation process.
    -   `instant`: 100 samples.
    -   `fast` (default): 200 samples.
    -   `deep`: 1000 samples.

### Infrastructure Flags
-   `--force_download`: A flag that, when present, forces the re-download of the dataset even if it already exists.
-   `--force_preprocess`: A flag that forces the re-creation of the tokenizer vocabulary.


================================================================================

--- FILE: colab_remote_exec.txt | PATH: Project\colab_remote_exec.txt ---
-------------------------------------------------------------------------

# ==============================================================================
# MLSA PROJECT - R&D MASTER PIPELINE (COLAB EDITION)
# ==============================================================================

# 1. AGGANCIO AL CLOUD STORAGE (Persistenza Risultati)
from google.colab import drive
import os
import sys
import torch

print("--- [FASE 1] MOUNT E MAPPATURA PERCORSI ---")
drive.mount('/content/drive', force_remount=True)

# Percorso validato dallo Scanner (Chirurgicamente esatto)
PROJECT_ROOT = "/content/drive/MyDrive/LifeLineV4/PROGETTI/Universita/Software_Science_And_Technology/MACHINE LEARNING FOR SOFTWARE ANALYSIS/PROJECT__MLSA_MMA/COLAB_Marco/Project_MLSA_MMA/Project"

if not os.path.exists(PROJECT_ROOT):
    print(f"âŒ ERRORE CRITICO: Il percorso {PROJECT_ROOT} non esiste.")
    print("Controlla se hai cambiato i nomi delle cartelle (es. PROGETTI vs Progetti).")
else:
    # Cambiamo directory di lavoro (Context Shift)
    %cd "{PROJECT_ROOT}"
    # Iniezione nel path di sistema per risolvere i moduli 'models' e 'data'
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)
    print(f"âœ… Directory di lavoro: {os.getcwd()}")

# 2. AUDIT HARDWARE (Hardware-Software Alignment)
print("\n--- [FASE 2] AUDIT HARDWARE ---")
device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cuda":
    print(f"ðŸš€ GPU RILEVATA: {torch.cuda.get_device_name(0)}")
    print(f"ðŸ“Š VRAM DISPONIBILE: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("âš ï¸ ATTENZIONE: GPU NON RILEVATA! Controlla 'Runtime -> Cambia tipo di runtime'.")

# 3. PROVISIONING DIPENDENZE (Ambiente Isolato)
print("\n--- [FASE 3] INSTALLAZIONE STRUMENTI ---")
!pip install tokenizers tqdm nltk rouge-score -q

# 4. ESECUZIONE ORCHESTRATORE (L'Attacco Finale)
# Nota Ingegneristica: Decommenta SOLO la riga dell'esperimento desiderato.
# I parametri sono calibrati per saturare la VRAM di una Tesla T4 senza crash.

print("\n--- [FASE 4] AVVIO ADDESTRAMENTO PESANTE ---")

# --- OPZIONE A: TRANSFORMER (Il Titano Parallelo) ---
# Ideale per catturare relazioni a lungo raggio nel codice.
!python C2Orchestrator.py --mode train --model transformer --subset 50000 --epochs 10 --batch_size 128 --evaluation fast

# --- OPZIONE B: LSTM + BAHDANAU (Attenzione Additiva) ---
# Il modello "Sarto" che impara dinamicamente dove guardare.
#!python C2Orchestrator.py --mode train --model lstm_bahdanau --subset 50000 --epochs 10 --batch_size 64 --evaluation fast

# --- OPZIONE C: LSTM + DOTPRODUCT (Attenzione del Prof) ---
# Implementazione basata sul prodotto scalare scalato (Scaled Dot-Product).
#!python C2Orchestrator.py --mode train --model lstm_dotproduct --subset 50000 --epochs 10 --batch_size 64 --evaluation fast

print("\n--- PIPELINE COMPLETATA CON SUCCESSO ---")

================================================================================

--- FILE: configs.txt | PATH: Project\configs.txt ---
-----------------------------------------------------

# Specifiche Tecniche: ML-Based Python Code Summarization

metadata:
  data: 13 Febbraio 2026
  deadline: 
    code_report: 13 Febbraio 2026
    discussione: 20 Febbraio 2026
  corso: Machine Learning for Software Analysis

obiettivo_core:
  descrizione: Sviluppare un tool basato su PyTorch che esegua la "code summarization".
  input: Codice sorgente Python (es. funzioni, metodi).
  output: Descrizione testuale (docstring/summary).

vincoli_tecnici:
  non_permesso:
    - Utilizzo di Full Language Models pre-addestrati (es. BERT, GPT-2, CodeBERT, T5 caricati con pesi pre-esistenti).
    - Utilizzo di modelli giÃ  addestrati specificamente su sintassi o semantica Python.
    - Il modello core deve essere addestrato da zero (from scratch) sui dati scelti.
  permesso:
    - Utilizzo di Tokenizer pre-addestrati (es. Byte-Pair Encoding, WordPiece).
    - Utilizzo di Matrici di Embedding pre-addestrate (es. GloVe, Word2Vec), purchÃ© non siano parte di un modello Transformer intero.
    - Utilizzo di LLM (ChatGPT, Claude) per assistenza alla scrittura del codice o debugging (da documentare esplicitamente nel report).
    - Adattamento di codice open-source (con attribuzione).

architettura_richiesta:
  framework: PyTorch
  tipologia: Sequence-to-Sequence (Encoder-Decoder) o Transformer-based (Custom).
  componenti:
    - preprocessing: Tokenizzazione codice sorgente e riassunti.
    - encoder: Processa la sequenza di codice.
    - decoder: Genera la sequenza di testo (riassunto).
    - training_loop: Implementazione standard (Forward pass, Loss calculation, Backward pass, Optimizer step).

dati:
  fonte: Dataset pubblici contenenti coppie (codice python, docstring/riassunto).
  suggerimento: CodeSearchNet (o subset filtrati da GitHub/StackOverflow).
  requisito: Il dataset deve essere sufficientemente ampio per permettere la convergenza del training, ma gestibile entro i tempi di calcolo disponibili.

deliverables:
  codebase:
    linguaggio: Python/PyTorch
    struttura_raccomandata:
      - src/: Codice sorgente.
      - data/: Script per il download/processing dati.
      - models/: Definizioni delle classi PyTorch (Encoder, Decoder, Seq2Seq).
      - scripts/: Script eseguibili.
    scripts_funzionali:
      - training: python train.py
      - evaluation: python evaluate.py
      - inference: python summarize.py --input "def func(): ..."
  report:
    formato: PDF, 5-6 Pagine
    sezioni:
      - Introduzione e Obiettivi.
      - Metodologia: Descrizione dataset, preprocessing, architettura modello (giustificazione scelte).
      - Risultati: Grafici loss (train/val), tabelle metriche.
      - Discussione: Analisi errori, limitazioni.
      - Appendice: Contributi membri gruppo, Riferimenti (incluso uso LLM).
  readme:
    contenuto: Istruzioni chiare per installare dipendenze, preparare dati e avviare training, valutazione, inferenza.

metriche_valutazione:
  - Cross-Entropy Loss (Durante il training/validation).
  - BLEU Score (Metrica standard per traduzione automatica/generazione testo).
  - ROUGE Score (Metrica per la qualitÃ  del riassunto).
  - Perplexity (Opzionale ma raccomandata per modelli linguistici).

roadmap_operativa:
  - Day 1 (Oggi): Setup ambiente, scelta dataset (es. CodeSearchNet slice), pipeline di preprocessing funzionante (Text -> Tokens -> IDs).
  - Day 2: Implementazione modello Seq2Seq semplice (LSTM o Transformer base). Primo run di training "dummy" per verificare che non crashi.
  - Day 3: Training serio (overnight se necessario). Implementazione script di valutazione (BLEU/ROUGE).
  - Day 4: Scrittura Report, cleanup codice, creazione README.


================================================================================

--- FILE: evaluate.py | PATH: Project\evaluate.py ---
-----------------------------------------------------

"""
================================================================================
BATCH EVALUATION & PROBABILISTIC AUDIT UNIT
================================================================================
ROLE: Assessing Model Generalization and Statistical Confidence.

DESIGN RATIONALE:
- Probabilistic Integrity: Integrates Perplexity (PPL) and Cross-Entropy Loss
  to measure the model's internal uncertainty (branching factor).
- Search Space Exploration: Implements Beam Search (k=3) for high-quality 
  sequence synthesis during qualitative assessment.
- Numerical Stability: Operates in log-probability space to prevent underflow 
  and ensure accurate cumulative scoring.
================================================================================
"""

import sys
import os
import torch
import logging
import math
import pandas as pd
import torch.nn.functional as F
from tqdm import tqdm
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from tokenizers import Tokenizer
from models.factory import get_model_architecture
from data.dataset import get_dataloader

# Path resolution to ensure the factory and data modules are discoverable.
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

logger = logging.getLogger(__name__)

class BatchEvaluator:
    """
    Automated Audit Engine. 
    Performs a deep statistical analysis on model checkpoints using the Test split.
    """
    def __init__(self, device, tokenizer_path, checkpoint_dir, data_path, subset_size=200):
        self.device = device
        self.tokenizer = Tokenizer.from_file(tokenizer_path)
        self.vocab_size = self.tokenizer.get_vocab_size()
        self.checkpoint_dir = checkpoint_dir
        self.data_path = data_path
        self.tokenizer_path = tokenizer_path
        self.subset_size = subset_size
        self.results = []

    def beam_decode(self, model, src, model_tag, beam_width=3, max_len=30):
        """ Heuristic search for the most likely sequence. """
        sos_id = self.tokenizer.token_to_id("<SOS>")
        eos_id = self.tokenizer.token_to_id("<EOS>")
        
        if "lstm" in model_tag:
            encoder_outputs, hidden, cell = model.encoder(src)
            beams = [([sos_id], 0.0, hidden, cell)]
        else:
            beams = [([sos_id], 0.0)]
            encoder_outputs = None

        for step in range(max_len):
            new_candidates = []
            for b in beams:
                if "lstm" in model_tag:
                    seq, score, h, c = b
                else:
                    seq, score = b
                
                if seq[-1] == eos_id:
                    new_candidates.append(b)
                    continue
                
                try:
                    if "lstm" in model_tag:
                        input_token = torch.LongTensor([seq[-1]]).to(self.device)
                        output, next_h, next_c = model.decoder(input_token, h, c, encoder_outputs)
                    else:
                        input_tensor = torch.LongTensor([seq]).to(self.device)
                        output = model(src, input_tensor)[:, -1, :]
                    
                    log_probs = F.log_softmax(output, dim=-1)
                    top_v, top_i = log_probs.topk(beam_width)
                    
                    for i in range(beam_width):
                        next_token = top_i[0, i].item()
                        new_score = score + top_v[0, i].item()
                        
                        if "lstm" in model_tag:
                            new_candidates.append((seq + [next_token], new_score, next_h, next_c))
                        else:
                            new_candidates.append((seq + [next_token], new_score))
                except Exception as e:
                    logger.error(f"Search Failure at step {step}: {e}")
                    raise e
            
            beams = sorted(new_candidates, key=lambda x: x[1], reverse=True)[:beam_width]
            if all(b[0][-1] == eos_id for b in beams):
                break
                
        return beams[0][0]

    def evaluate_single_checkpoint(self, checkpoint_name):
        """ Performs full metric audit: BLEU, ROUGE, Loss, and Perplexity. """
        if "transformer" in checkpoint_name.lower(): model_tag = "transformer"
        elif "lstm_dotproduct" in checkpoint_name.lower(): model_tag = "lstm_dotproduct"
        else: model_tag = "lstm_bahdanau"

        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        print(f"\nAuditing Performance: {model_tag} | {checkpoint_name}")
        
        try:
            model = get_model_architecture(model_tag, self.device, vocab_size=self.vocab_size)
            state_dict = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
            model.load_state_dict(state_dict)
            model.eval()
        except Exception as e:
            return {"file": checkpoint_name, "error": str(e)}
        
        test_loader = get_dataloader(self.data_path, "test", self.tokenizer_path, batch_size=1, subset=self.subset_size)
        references, hypotheses = [], []
        total_loss = 0
        total_tokens = 0

        with torch.no_grad():
            for src, trg in tqdm(test_loader, desc=f"Audit {model_tag}", leave=False):
                src, trg = src.to(self.device), trg.to(self.device)
                
                # --- STATISTICAL ANALYSIS (Loss & PPL) ---
                # We perform a forward pass with the ground truth to measure 'Surprise'
                if "lstm" in model_tag:
                    output = model(src, trg[:, :-1]) # Standard teacher forcing pass
                else:
                    output = model(src, trg[:, :-1])
                
                # Reshape for CrossEntropy: [Batch * Seq, Vocab]
                output_dim = output.shape[-1]
                output = output.contiguous().view(-1, output_dim)
                trg_loss = trg[:, 1:].contiguous().view(-1)
                
                # output shape: [Batch * Seq, Vocab_Size]
                # trg_loss shape: [Batch * Seq]
                
                loss = F.cross_entropy(output, trg_loss, reduction='sum')
                total_loss += loss.item()
                total_tokens += trg_loss.numel()


                # --- QUALITATIVE ANALYSIS (Decoding) ---
                predicted_indices = self.beam_decode(model, src, model_tag, beam_width=3)
                pred_text = self.tokenizer.decode(predicted_indices, skip_special_tokens=True).strip()
                real_text = self.tokenizer.decode(trg.squeeze().tolist(), skip_special_tokens=True).strip()
                
                hypotheses.append(pred_text.split())
                references.append([real_text.split()])

        # --- METRIC SYNTHESIS ---
        # 1. Linguistic Metrics
        bleu = corpus_bleu(references, hypotheses, smoothing_function=SmoothingFunction().method1)
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        total_rouge = sum(scorer.score(" ".join(r[0]), " ".join(h))['rougeL'].fmeasure for h, r in zip(hypotheses, references))
        rougeL = total_rouge / len(hypotheses) if hypotheses else 0
        
        # 2. Probabilistic Metrics
        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0
        perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')
        
        return {
            "file": checkpoint_name, 
            "model": model_tag, 
            "bleu": bleu, 
            "rougeL": rougeL, 
            "loss": avg_loss, 
            "perplexity": perplexity
        }

    def run_all(self, specific_file=None):
        files = [specific_file] if specific_file else [f for f in os.listdir(self.checkpoint_dir) if f.endswith(".pt")]
        if not files: return None

        print(f"\nSTART BATCH EVALUATION (Probabilistic Audit)")
        for ckpt in sorted(files):
            res = self.evaluate_single_checkpoint(ckpt)
            if "error" not in res: 
                self.results.append(res)
        
        return pd.DataFrame(self.results)

================================================================================

--- FILE: requirements.txt | PATH: Project\requirements.txt ---
---------------------------------------------------------------

--index-url https://download.pytorch.org/whl/cu124

torch
torchvision
torchaudio
tokenizers
tqdm
pandas
nltk
rouge-score
requests

================================================================================

--- FILE: test.py | PATH: Project\test.py ---
---------------------------------------------

import torch
from tokenizers import Tokenizer
from models.factory import get_model_architecture

def run_quick_test():
    # --- CONFIGURATION ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_tag = "lstm_attention"
    tokenizer_path = "Project/tokenizer.json"
    checkpoint_path = f"Project/models/checkpoints/best_{model_tag}.pt"
    
    # --- YOUR CODE TO TEST ---
    # I write a function that filters even numbers from a list
    code_to_summarize = """
def is_adult(age):
    if age >= 18:
        return True
    else:
        return False
    """

    print(f"[*] Loading components for {model_tag}...")
    
    # 1. Loading Tokenizer and Model
    tokenizer = Tokenizer.from_file(tokenizer_path)
    # We use the factory to have the same structure used in C2
    model = get_model_architecture(model_tag, device, vocab_size=10000)
    
    # 2. Loading Weights (Frozen State)
    try:
        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        print(f"[+] Weights loaded successfully from {checkpoint_path}")
    except FileNotFoundError:
        print(f"[!] ERROR: Checkpoint not found. Did you run the C2Orchestrator?")
        return

    model.eval()
    # 3. Input Transformation (Symbolic Mapping)
    # Manually adding start (1) and end (2) tags
    encoded = tokenizer.encode(code_to_summarize).ids
    src_tensor = torch.LongTensor([1] + encoded + [2]).unsqueeze(0).to(device)

    print(f"[*] Code analysis in progress...")

    # 4. Generation Loop (Greedy Search)
    with torch.no_grad():
        # The Encoder creates the abstract thought (context)
        encoder_outputs, hidden, cell = model.encoder(src_tensor)
        
        # We start the sentence with <SOS>
        input_token = torch.LongTensor([tokenizer.token_to_id("<SOS>")]).to(device)
        predicted_indices = []

        for _ in range(30): # Maximum 30 words for the summary
            output, hidden, cell = model.decoder(input_token, hidden, cell, encoder_outputs)
            
            # We take the most probable token
            top1 = output.argmax(1)
            token_id = top1.item()
            
            if token_id == tokenizer.token_to_id("<EOS>"):
                break
                
            predicted_indices.append(token_id)
            input_token = top1 # The predicted token becomes the next input

    # 5. Final Output
    prediction = tokenizer.decode(predicted_indices)
    
    print("\n" + "="*30)
    print("INPUT CODE:")
    print(code_to_summarize.strip())
    print("-" * 30)
    print("PREDICTED SUMMARY:")
    print(f">>> {prediction}")
    print("="*30)

if __name__ == "__main__":
    run_quick_test()

================================================================================

--- FILE: test_audit.py | PATH: Project\test_audit.py ---
---------------------------------------------------------

"""
================================================================================
DEEP AUDIT & QUALITATIVE INFERENCE UNIT
================================================================================
ROLE: High-Fidelity Model Assessment via Beam Search and Suite Testing.

DESIGN RATIONALE:
- Formal Verification: Generates a persistent audit log for post-mortem analysis.
- Deterministic Evaluation: Removes visual noise (emojis) to focus on 
  statistical consistency and sequence quality.
- Throughput Monitoring: Implements progress tracking for large checkpoint
  batches to monitor hardware efficiency during inference.
================================================================================
"""

import torch
import torch.nn.functional as F
import json
import gzip
import os
import sys
import random
from datetime import datetime
from tqdm import tqdm
from tokenizers import Tokenizer

# Path resolution to enable the internal 'models' package discovery.
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

from models.factory import get_model_architecture

def clean_output(text):
    """ Post-processing to normalize BPE spacing artifacts. """
    return text.replace('Ä ', ' ').replace('  ', ' ').strip()

def beam_search_decode(model, src_tensor, tokenizer, model_tag, beam_width=5, max_len=40, device="cpu", penalty=1.2):
    """ 
    Heuristic tree search for non-greedy sequence generation. 
    Implements repetition penalty to increase output diversity.
    """
    model.eval()
    sos_id = tokenizer.token_to_id("<SOS>")
    eos_id = tokenizer.token_to_id("<EOS>")
    pad_id = tokenizer.token_to_id("<PAD>")
    
    if "lstm" in model_tag:
        with torch.no_grad():
            encoder_outputs, hidden, cell = model.encoder(src_tensor)
        beams = [([sos_id], 0.0, hidden, cell)]
    else:
        beams = [([sos_id], 0.0)]
        encoder_outputs = None

    with torch.no_grad():
        for _ in range(max_len):
            all_candidates = []
            for b in beams:
                if "lstm" in model_tag:
                    seq, score, h, c = b
                else:
                    seq, score = b
                
                if seq[-1] == eos_id:
                    all_candidates.append(b)
                    continue
                
                if "lstm" in model_tag:
                    input_token = torch.LongTensor([seq[-1]]).to(device)
                    output, next_h, next_c = model.decoder(input_token, h, c, encoder_outputs)
                else:
                    input_tensor = torch.LongTensor([seq]).to(device)
                    output = model(src_tensor, input_tensor)[:, -1, :] 
                
                log_probs = F.log_softmax(output, dim=-1).squeeze(0)
                
                # Apply repetition penalty to existing tokens in sequence.
                for idx in set(seq):
                    if idx not in [sos_id, eos_id, pad_id]:
                        log_probs[idx] -= penalty
                
                top_log_probs, top_indices = log_probs.topk(beam_width)
                for i in range(beam_width):
                    next_token = top_indices[i].item()
                    new_score = score + top_log_probs[i].item()
                    if "lstm" in model_tag:
                        all_candidates.append((seq + [next_token], new_score, next_h, next_c))
                    else:
                        all_candidates.append((seq + [next_token], new_score))
            
            # Global pruning: keep top K candidates.
            beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]
            if all(b[0][-1] == eos_id for b in beams):
                break
                
    return beams[0][0]

def load_samples(data_dir, split, num_samples=10):
    """ Extracts random samples from processed splits for qualitative testing. """
    file_path = os.path.join(data_dir, f"{split}.jsonl.gz")
    if not os.path.exists(file_path):
        return []
    samples = []
    try:
        with gzip.open(file_path, 'rt', encoding='utf-8') as f:
            lines = f.readlines()
            chosen = random.sample(lines, min(num_samples, len(lines)))
            for c in chosen:
                item = json.loads(c)
                samples.append({'code': item.get('code', ''), 'doc': item.get('docstring', '')})
    except Exception:
        pass
    return samples

def run_deep_audit():
    """ 
    Main entry point for model stress testing and log generation.
    Iterates through all checkpoints and generates comparative reports.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer_path = os.path.join(project_root, "tokenizer.json")
    checkpoint_dir = os.path.join(project_root, "models", "checkpoints")
    data_root = os.path.join(project_root, "Datasets", "processed")
    log_file = os.path.join(project_root, f"audit_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

    if not os.path.exists(tokenizer_path):
        print("Error: Tokenizer file not found. Inference aborted.")
        return

    tokenizer = Tokenizer.from_file(tokenizer_path)
    vocab_size = tokenizer.get_vocab_size()
    
    # Define multi-domain test suites.
    suites = {
        "TRAIN (Memorization)": load_samples(data_root, "train", 10),
        "TEST (Generalization)": load_samples(data_root, "test", 10),
        "CUSTOM (Logic Check)": [
            {"code": "def find_max(l):\n    return max(l) if l else None", "doc": "Finds the maximum value in a list."},
            {"code": "def save_file(data, path):\n    with open(path, 'w') as f:\n        f.write(data)", "doc": "Writes data to a file."}
        ]
    }

    checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(".pt")])
    
    with open(log_file, "w", encoding="utf-8") as log:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        header = f"{'='*90}\nC2 DEEP AUDIT REPORT | {timestamp}\nDEVICE: {device} | MODE: BEAM SEARCH (k=5) | PENALTY: 1.2\n{'='*90}\n"
        print(header)
        log.write(header)

        # Progress bar for checkpoint iteration.
        for ckpt in tqdm(checkpoints, desc="Analyzing Weight Shards", unit="ckpt"):
            if "transformer" in ckpt.lower(): model_tag = "transformer"
            elif "lstm_dotproduct" in ckpt.lower(): model_tag = "lstm_dotproduct"
            elif "lstm_bahdanau" in ckpt.lower(): model_tag = "lstm_bahdanau"
            else: continue

            log.write(f"\n[CHECKPOINT] {ckpt} (Architecture: {model_tag})\n")
            
            try:
                # Factory synthesis and state loading.
                model = get_model_architecture(model_tag, device, vocab_size=vocab_size)
                model.load_state_dict(torch.load(os.path.join(checkpoint_dir, ckpt), map_location=device))
                model.eval()

                for suite_name, samples in suites.items():
                    if not samples: continue
                    log.write(f"\n--- SUITE: {suite_name} ---\n")
                    
                    for i, s in enumerate(samples):
                        # Tokenization and hardware mapping.
                        ids_input = tokenizer.encode(s['code']).ids
                        src_tensor = torch.LongTensor([1] + ids_input + [2]).unsqueeze(0).to(device)
                        
                        # Sequence decoding.
                        ids_pred = beam_search_decode(model, src_tensor, tokenizer, model_tag, beam_width=5, device=device)
                        prediction = clean_output(tokenizer.decode(ids_pred, skip_special_tokens=True))
                        
                        # Writing comparative data to log.
                        log.write(f"Sample {i+1}:\n")
                        log.write(f"  CODE: {s['code'].strip().replace(chr(10), ' ')[:60]}...\n")
                        log.write(f"  REAL: {s['doc'].strip()}\n")
                        log.write(f"  PRED: {prediction}\n")
                        log.write(f"  {'-'*15}\n")
            except Exception as e:
                log.write(f"  CRITICAL ERROR: Failed to audit {ckpt}. Reason: {str(e)}\n")

    print(f"\nAudit complete. Quantitative and qualitative report finalized at: {log_file}")

if __name__ == "__main__":
    run_deep_audit()

================================================================================

--- FILE: dataset.py | PATH: Project\data\dataset.py ---
--------------------------------------------------------

"""
================================================================================
DATA PIPELINE & BATCHING UNIT
================================================================================
ROLE: High-Efficiency Data Loading and Tensor Synthesis.

DESIGN RATIONALE:
- Computational Economy: Uses Dynamic Padding via a custom Collator to minimize 
  the number of operations on <PAD> tokens.
- Latency Reduction: Implements RAM Caching for tokenized sequences, removing 
  JSON parsing and BPE encoding from the main training loop.
- Multi-Processing: Configures asynchronous data pre-fetching to ensure the 
  GPU 'never waits' for the next batch (Data Starvation prevention).
================================================================================
"""

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import json
import gzip
import os
import multiprocessing
from tokenizers import Tokenizer

# --- [PHASE 1] OPTIMIZED COLLATOR (Dynamic Padding) ---

class SmartCollator:
    """
    BATCH SYNTHESIZER: Implements the Dynamic Padding strategy.
    
    Standard padding to a fixed global length is computationally wasteful. 
    This class intercepts each batch and pads sequences to the length 
    of the LONGEST element in that specific batch.
    """
    def __init__(self, pad_id):
        """
        Args:
            pad_id (int): The integer ID used for <PAD> tokens (usually 0).
        """
        self.pad_id = pad_id

    def __call__(self, batch):
        """
        Transforms a raw list of samples into synchronized tensors.
        
        Logic:
        1. De-structure the batch into separate code and docstring streams.
        2. Apply 'pad_sequence' to create rectangular tensors [Batch, Max_Batch_Len].
        """
        code_seqs, doc_seqs = zip(*batch)

        # [OPTIMIZATION] Dynamic Padding Logic:
        # If the longest code in this batch is 50 tokens, we pad to 50, even if 
        # the global maximum is 256. This reduces Transformer/LSTM overhead significantly.
        code_padded = pad_sequence(code_seqs, batch_first=True, padding_value=self.pad_id)
        doc_padded = pad_sequence(doc_seqs, batch_first=True, padding_value=self.pad_id)

        return code_padded, doc_padded


# --- [PHASE 2] STREAMLINED DATASET (Consumer Mode) ---

class CodeSummaryDataset(Dataset):
    """
    MEM-RESIDENT DATASET: Optimized for high-speed gradient descent.
    
    Since the data has been purified by 'DatasetCleaner', this class focuses 
    on one-time tokenization and persistent RAM storage.
    """
    def __init__(self, data_dir, split_type, tokenizer_path, max_len_code=256, max_len_doc=64, subset=None):
        """
        Args:
            data_dir: Path to the 'processed/' directory.
            split_type: 'train', 'valid', or 'test'.
            tokenizer_path: Path to the pre-trained BPE model.
            max_len_code/doc: Hard safety caps for sequence length.
        """
        self.tokenizer = Tokenizer.from_file(tokenizer_path)
        self.max_len_code = max_len_code
        self.max_len_doc = max_len_doc
        self.data = []

        # Cache special IDs locally to eliminate repeated dictionary lookups.
        self.pad_id = self.tokenizer.token_to_id("<PAD>")
        self.sos_id = self.tokenizer.token_to_id("<SOS>")
        self.eos_id = self.tokenizer.token_to_id("<EOS>")

        file_path = os.path.join(data_dir, f"{split_type}.jsonl.gz")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Refinery Error: Processed data not found at {file_path}")
        
        print(f"[MEM_CACHE] Loading refined '{split_type}' data into RAM...")

        # --- FAST LOADING & TOKENIZATION LOOP ---
        with gzip.open(file_path, 'rt', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line)
                    code = item.get('code', '')
                    doc = item.get('docstring', '')
                    
                    if not code or not doc: continue

                    # TOKENIZATION: Convert text to BPE sub-word units.
                    c_tokens = self.tokenizer.encode(code).ids
                    d_tokens = self.tokenizer.encode(doc).ids
                    
                    # TENSOR SYNTHESIS: Add structural tags (<SOS>/<EOS>) and truncate.
                    # Logic: [SOS] + Tokens + [EOS]. Hard truncation at max_len - 2.
                    code_ids = [self.sos_id] + c_tokens[:self.max_len_code-2] + [self.eos_id]
                    code_tensor = torch.tensor(code_ids, dtype=torch.long)
                    
                    doc_ids = [self.sos_id] + d_tokens[:self.max_len_doc-2] + [self.eos_id]
                    doc_tensor = torch.tensor(doc_ids, dtype=torch.long)
                    
                    # Persist the tensor pair in RAM for the duration of the training session.
                    self.data.append((code_tensor, doc_tensor))
                    
                    if subset and len(self.data) >= subset: 
                        break
                        
                except json.JSONDecodeError:
                    continue

        print(f"Cache Ready: {len(self.data)} samples ready for stochastic gradient descent.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        """ Zero-overhead access: Simply retrieves pre-computed tensors from RAM. """
        return self.data[idx]


# --- [PHASE 3] DATALOADER FACTORY ---

def get_dataloader(data_dir, split_type, tokenizer_path, batch_size=32, shuffle=True, subset=None):
    """
    PIPELINE CONFIGURATOR: Fuses the Dataset, Collator, and Multiprocessing logic.
    """
    # Initialize the high-speed dataset handler.
    dataset = CodeSummaryDataset(
        data_dir, 
        split_type, 
        tokenizer_path, 
        max_len_code=256, 
        max_len_doc=64, 
        subset=subset
    )
    
    # Inject the Dynamic Padding logic.
    collator = SmartCollator(dataset.pad_id)
    
    # HARDWARE OPTIMIZATION:
    # Logic: Use 4 parallel workers for pre-fetching if a GPU is available.
    # num_workers > 0 prevents the GPU from 'stalling' while waiting for CPU batch prep.
    cpu_count = multiprocessing.cpu_count()
    workers = min(cpu_count, 4) if torch.cuda.is_available() else 0
    
    return DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=shuffle, 
        collate_fn=collator,
        num_workers=workers,
        pin_memory=True,            # Allocates tensors in 'page-locked' memory for faster CPU->GPU transfer.
        prefetch_factor=2 if workers > 0 else None, # Pre-loads 2 batches per worker.
        persistent_workers=True if workers > 0 else False
    )

================================================================================

--- FILE: dataset_cleaner.py | PATH: Project\data\dataset_cleaner.py ---
------------------------------------------------------------------------

"""
================================================================================
DATA REFINERY & SANITIZATION UNIT
================================================================================
ROLE: Ensuring High-Fidelity Training Data & Preventing Data Contamination.

DESIGN RATIONALE:
- Anti-Cheating Logic: Models often learn to "shortcut" by copying comments 
  rather than understanding code. This unit strips all existing metadata.
- Cross-Split Deduplication: Guarantees that the Test set is truly "unseen" 
  by hashing the Training corpus.
- Dimensional Filtering: Discards outliers (extremely short or long sequences) 
  that would destabilize the Transformer's attention variance.
================================================================================
"""

import os
import gzip
import json
import glob
import re
import hashlib
import logging
from tqdm import tqdm

# Configure module-level logger for refinery audit trails.
logger = logging.getLogger(__name__)

class DatasetCleaner:
    """
    Handles the preprocessing, cleaning, and deduplication of code-docstring pairs.
    Acts as a 'Firewall' between raw, noisy internet data and the neural engine.
    """

    def __init__(self, min_code=20, max_code=250, min_doc=3, max_doc=50):
        """
        Args:
            min_code/max_code: Bounds for source code word count.
            min_doc/max_doc: Bounds for summary length (Docstrings).
        """
        self.min_code = min_code
        self.max_code = max_code
        self.min_doc = min_doc
        self.max_doc = max_doc
        
        # GLOBAL HASH REGISTRY: Stores MD5 fingerprints of every processed code block. 
        # This is the primary defense against Cross-Split Leakage.
        self.global_seen_hashes = set()

    @staticmethod
    def clean_docstring(doc):
        """
        TARGET SANITIZATION: Strips noise from the ground-truth summaries.
        
        Logic:
        - Paragraph Slicing: Only keeps the high-level summary (first paragraph).
        - Tag Removal: Strips Sphinx/Doxygen artifacts (:param, @return, etc.) via Regex.
        - URL Neutralization: Removes web links to force the model into linguistic logic.
        """
        if not doc: return ""
        
        # Isolate the primary intent (summary line/paragraph).
        doc = doc.split('\n\n')[0]
        
        # REGEX SHIELD: Remove metadata tags commonly found in automated documentation.
        doc = re.sub(r'(:param|:type|:return|:rtype|@param|@return|@throws|Args:|Returns:|Raises:).*', '', doc, flags=re.MULTILINE)
        
        # Remove doctest snippets (>>> example) to prevent training on test code.
        doc = re.sub(r'>>>.*', '', doc)
        
        # Remove URLs to clean up the target vocabulary.
        doc = re.sub(r'http\S+', '', doc)
        
        return ' '.join(doc.split()).strip()
    
    @staticmethod
    def clean_code_body(code):
        """
        INPUT SANITIZATION: The "Cheat-Proof" filter for source code.
        
        Logic:
        1. Docstring Extraction: Deletes triple-quoted strings at the start of functions.
        2. Inline Comment Purge: Uses a callback-based regex to remove '#' comments 
           while safely preserving '#' characters inside actual string literals.
        3. Compactification: Removes empty lines to provide a dense signal to the Encoder.
        """
        if not code: return ""

        # STEP 1: Eliminate triple-quoted docstrings. Non-greedy match ([\s\S]*?).
        docstring_pattern = r'(\"\"\"[\s\S]*?\"\"\"|\'\'\'[\s\S]*?\'\'\')'
        code = re.sub(docstring_pattern, '', code, count=1)

        # STEP 2: Intelligent Comment removal.
        # Group 1: Captures strings ("...") - we want to KEEP these.
        # Group 2: Captures comments (#...) - we want to DELETE these.
        comment_pattern = r"(\".*?(?<!\\)\"|'.*?(?<!\\)')|(#.*)"

        def _replacer(match):
            # If the regex matched Group 2 (a comment), return an empty string.
            if match.group(2) is not None:
                return ""
            # If it matched Group 1 (a literal string), return it as is.
            else:
                return match.group(1)

        code = re.sub(comment_pattern, _replacer, code)

        # STEP 3: Structural cleanup to remove trailing whitespace and null lines.
        lines = [line for line in code.split('\n') if line.strip()]
        
        return "\n".join(lines).strip()

    def process_split(self, split_name, input_dir, output_dir, mode="filter"):
        """
        Processes a specific dataset split (train, valid, or test).
        
        Args:
            mode (str): 
                - 'build': Populates the global hash registry (Use for TRAIN).
                - 'filter': Tests against the registry to prevent leakage (Use for VALID/TEST).
        """
        search_path = os.path.join(input_dir, split_name, "*.jsonl.gz")
        files = glob.glob(search_path)
        
        if not files:
            logger.warning(f"No files found for split: {split_name}")
            return

        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{split_name}.jsonl.gz")
        
        # Metrics for refinery audit.
        stats = {"total": 0, "kept": 0, "removed_len": 0, "removed_dupe": 0, "removed_doc": 0}
        
        logger.info(f"Refining {split_name.upper()} -> {output_file}")

        # Stream directly into GZIP to manage large datasets without memory saturation.
        with gzip.open(output_file, 'wt', encoding='utf-8') as out_f:
            for file_path in tqdm(files, desc=f"Sanitizing {split_name}"):
                with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                    for line in f:
                        stats["total"] += 1
                        try:
                            data = json.loads(line)
                            
                            # Normalize key naming across different dataset versions (func_code vs code).
                            raw_code = data.get('code', data.get('func_code_string', ''))
                            raw_doc = data.get('docstring', data.get('func_documentation_string', ''))
                            
                            if not raw_code: continue

                            # 1. CLEAN TARGET: High-level summary extraction.
                            clean_doc = self.clean_docstring(raw_doc)
                            if not clean_doc or not any(c.isalnum() for c in clean_doc):
                                stats["removed_doc"] += 1
                                continue
                            
                            # 2. CLEAN INPUT: Strip comments and leakage-prone text.
                            clean_code = self.clean_code_body(raw_code)
                            if not clean_code.strip() or len(clean_code.strip()) < 10:
                                stats["removed_doc"] += 1 
                                continue

                            # 3. STATISTICAL FILTERING: Sequence length pruning.
                            code_len = len(clean_code.split())
                            doc_len = len(clean_doc.split())
                            
                            if not (self.min_code <= code_len <= self.max_code) or \
                               not (self.min_doc <= doc_len <= self.max_doc):
                                stats["removed_len"] += 1
                                continue

                            # 4. CROSS-SPLIT DEDUPLICATION: MD5 Fingerprinting.
                            # We strip all whitespace to catch formatting-only duplicates.
                            code_norm = re.sub(r'\s+', '', clean_code)
                            code_hash = hashlib.md5(code_norm.encode('utf-8')).hexdigest()
                            
                            if code_hash in self.global_seen_hashes:
                                stats["removed_dupe"] += 1
                                continue
                            
                            if mode == "build":
                                self.global_seen_hashes.add(code_hash)

                            # 5. SERIALIZATION: Save purified JSON entry.
                            entry = {"code": clean_code, "docstring": clean_doc}
                            out_f.write(json.dumps(entry) + "\n")
                            stats["kept"] += 1

                        except json.JSONDecodeError:
                            continue 
        
        logger.info(f"[{split_name.upper()}] Kept: {stats['kept']} | Removed Dupes: {stats['removed_dupe']}")

    def run(self, input_dir, output_dir):
        """
        Executes the Refinery Pipeline in a strict causal order.
        Mental Model: Training data defines the 'Universe', Valid/Test are 'Unseen'.
        """
        # Step 1: Ingest Training data to build the Hash Registry.
        self.process_split("train", input_dir, output_dir, mode="build")
        
        # Step 2 & 3: Filter Valid and Test sets against the Registry.
        self.process_split("valid", input_dir, output_dir, mode="filter")
        self.process_split("test", input_dir, output_dir, mode="filter")

# --- STANDALONE EXECUTION ENTRY ---
def main():
    """ Utility for manual dataset cleaning outside the orchestrator. """
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
    DATASET_ROOT = "Project/Datasets/python/final/jsonl" 
    OUTPUT_DIR = "Project/Datasets/processed" 

    if not os.path.exists(DATASET_ROOT):
        logger.error(f"Refinery Error: Source directory missing: {DATASET_ROOT}")
        return

    cleaner = DatasetCleaner()
    cleaner.run(DATASET_ROOT, OUTPUT_DIR)
    logger.info(f"Refinery Shutdown. Purified data ready in: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()

================================================================================

--- FILE: download_dataset.py | PATH: Project\data\download_dataset.py ---
--------------------------------------------------------------------------

"""
================================================================================
DATA ACQUISITION UNIT
================================================================================
ROLE: Automating the Ingestion of the CodeSearchNet Corpus.

DESIGN RATIONALE:
- Non-Persistent Intermediate Storage: Downloads are handled in RAM-buffers 
  (BytesIO) to reduce SSD wear and tear during the decompression phase.
- Network Robustness: Uses specific User-Agent headers to bypass CDN anti-bot 
  heuristics that frequently block vanilla Python-requests signatures.
- High-Availability Source: Targets the Hugging Face CDN as the most stable 
  and high-bandwidth 'Source of Truth' for this dataset.
================================================================================
"""

import os
import requests
import zipfile
import io

# DOWNLOAD AND EXTRACTION OF CODESEARCHNET ROBUST DATASET
def download_codesearchnet_robust(output_dir="Project/Datasets"):
    """ 
    Orchestrates the automated retrieval and extraction of the Python dataset.
    
    Args:
        output_dir (str): Final destination for the unzipped JSONL files.
    """

    # SOURCE SELECTION: Mirrors on Hugging Face Assets are preferred for 
    # their consistent uptime and integrity-checking capabilities.
    url = "https://huggingface.co/datasets/code_search_net/resolve/main/data/python.zip"
    
    # Filesystem readiness check.
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    print(f"--- Attempting acquisition via Hugging Face CDN ---")
    print(f"URL: {url}")
    print("Download in progress (about 900MB). Grab a coffee... but with attention!")

    try:
        # BOT BYPASS PROTOCOL: Vanilla requests often trigger 403 Forbidden errors 
        # from CDNs (Cloudflare/Akamai). A human-like User-Agent ensures connectivity.
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        # STREAMING INITIATION: stream=True allows us to process the incoming 
        # binary stream without loading the entire 1GB into active memory at once.
        response = requests.get(url, headers=headers, stream=True)
        response.raise_for_status() # Network-level sanity check (Exception on 4xx/5xx).

        # IN-MEMORY EXTRACTION: 
        # Mental Model: 'io.BytesIO' creates a virtual file in RAM.
        # This allows 'zipfile.ZipFile' to read the content as if it were on disk,
        # but with zero I/O latency.
        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:
            print("Download completed. Extracting...")
            # Extractall flattens the hierarchy into the target Dataset directory.
            zip_ref.extractall(output_dir)
        
        # Verification phase: Audit the resulting directory structure.
        print(f"SUCCESS: Dataset extracted to {output_dir}")
        print("Detected structure: " + str(os.listdir(output_dir)))

    except Exception as e:
        # ERROR TELEMETRY: Provides the user with a manual remediation path.
        print(f"CRITICAL FAILURE: {e}")
        print("\n--- MANUAL SOLUTION ---")
        print(f"If the script fails, manually download from here: {url}")
        print(f"And extract the contents to: {os.path.abspath(output_dir)}")

# STANDALONE EXECUTION LOGIC
if __name__ == "__main__":
    download_codesearchnet_robust()

================================================================================

--- FILE: inspect_corpus.py | PATH: Project\data\inspect_corpus.py ---
----------------------------------------------------------------------

"""
================================================================================
CORPUS INSPECTION TOOL - Debug Utility
================================================================================
ROLE: Visual Validation of Data Cleaning and Extraction Logic.

DESIGN RATIONALE:
- Transparency: Provides a real-time console preview of the 'purified' output 
  that would otherwise be hidden inside temporary text files or BPE tokens.
- Heuristic Testing: Allows the developer to verify if the length filters 
  (e.g., code < 200 words) are correctly excluding structural noise.
- Logic Decoupling: Replicates the production cleaning functions in a safe, 
  read-only environment.
================================================================================
"""

import os
import json
import gzip
import glob
import re

# NON-FUNCTIONAL FILE: Used exclusively for debugging and structural inspection.

# --- CONFIGURATION ---
# Define the root directory for raw JSONL data across various languages.
JSONL_BASE_DIR = "Project/Datasets/python/final/jsonl" 
NUM_SAMPLES = 5  # Control parameter for inspection depth.

# --- DATA PURIFICATION LOGIC (Replicated from Production) ---
def clean_docstring(doc):
    """ 
    CLEANING LAYER: Strips structural noise and metadata from docstrings. 
    
    Technical Details:
    - Slicing: Targets only the first functional paragraph to avoid bloating.
    - Regex Filtering: Removes common documentation tags (:param, @return) 
      which are statistically redundant for summarization tasks.
    - Doctest Purge: Removes lines starting with '>>>' to prevent training 
      on execution examples.
    """
    if not doc: return ""
    # Capture only the head of the documentation.
    first_line = doc.split('\n\n')[0].split('\n')[0]
    
    # Strip formal parameter definitions.
    clean = re.sub(r'(:param|:type|:return|:rtype|@param|@return).*', '', first_line)
    # Strip Python console examples.
    clean = re.sub(r'>>>.*', '', clean)
    return clean.strip()

# --- INSPECTION ENGINE ---
def inspect_temp_content(base_dir, samples=5):
    """
    Simulates the vocabulary preparation phase but redirects output to the 
    standard output (Stdout) instead of the filesystem.
    """
    # Glob-based discovery to find compressed training shards.
    search_pattern = os.path.join(base_dir, "train", "*.jsonl.gz")
    train_files = glob.glob(search_pattern)

    if not train_files:
        print(f"ERRORE: Nessun file trovato in {search_pattern}")
        return

    print(f"Trovati {len(train_files)} file. Analisi del primo file: {os.path.basename(train_files[0])}...\n")
    print("="*60)

    count = 0
    
    # OPEN STREAM: Read the compressed file in 'rt' (Read-Text) mode.
    # Logic: Efficiently decompress on-the-fly to minimize RAM footprint.
    with gzip.open(train_files[0], 'rt', encoding='utf-8') as f:
        for line in f:
            # Throttle output to the requested sample count.
            if count >= samples:
                break
            
            try:
                item = json.loads(line)
                
                # SCHEMA ALIGNMENT: Handle potential variations in JSON keys.
                code = item.get('code', item.get('func_code_string', ''))
                doc = item.get('docstring', item.get('func_documentation_string', ''))
                
                # Apply the same sanitization used in the main pipeline.
                clean_doc = clean_docstring(doc)
                
                # FILTRATION AUDIT:
                # Simulates the threshold logic: ignore short docstrings or overly complex code.
                if len(clean_doc) > 10 and len(code.split()) < 200:
                    count += 1
                    
                    # --- OUTPUT SIMULATION ---
                    # Visually separates the Code (Input) from the Docstring (Ground Truth).
                    print(f"--- SAMPLE #{count} ---")
                    print("[CONTENUTO CHE VERREBBE SCRITTO NEL FILE TEMP]:")
                    print("-" * 20)
                    print(f"{code}")      # Phase 1: Raw Code block.
                    print(f"{clean_doc}") # Phase 2: Sanitized documentation string.
                    print("-" * 20)
                    print("\n")
                    
            except json.JSONDecodeError:
                continue # Gracefully skip malformed JSON shards.

    print("="*60)
    print("Ispezione completata.")

# --- EXECUTION GATE ---
if __name__ == "__main__":
    # Ensure directory existence before initiating the audit.
    if os.path.exists(JSONL_BASE_DIR):
        inspect_temp_content(JSONL_BASE_DIR, NUM_SAMPLES)
    else:
        print(f"Attenzione: La cartella '{JSONL_BASE_DIR}' non esiste. Modifica la variabile JSONL_BASE_DIR.")

================================================================================

--- FILE: inspect_dataset.py | PATH: Project\data\inspect_dataset.py ---
------------------------------------------------------------------------

"""
================================================================================
DATASET INSPECTION UNIT
================================================================================
ROLE: Generating Human-Readable Snapshots from Compressed Binary Streams.

DESIGN RATIONALE:
- Observability: Translates high-density JSONL.GZ files into plain text formats, 
  allowing manual audit of the data refinery's effectiveness.
- Schema Agnosticism: Dynamically maps different field names to ensure 
  compatibility with both raw "CodeSearchNet" schemas and "Refined" versions.
- Defensive Slicing: Only reads a requested buffer size (num_examples) to avoid 
  unnecessary I/O overhead on large-scale datasets.
================================================================================
"""

import gzip
import json
import os
import glob

def save_human_readable_samples(input_dir, output_dir, output_filename, num_examples=100):
    """ 
    Orchestrates the extraction of compressed data for qualitative analysis. 
    
    Args:
        input_dir (str): Root path where .jsonl.gz shards are located.
        output_dir (str): Destination for the decoded text audit file.
        output_filename (str): Target filename for the preview output.
        num_examples (int): Number of top-tier samples to extract for the snapshot.
    """

    # --- [PHASE 1: DIRECTORY GOVERNANCE] ---
    # Ensure the target audit path exists to prevent OS-level I/O exceptions.
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    full_output_path = os.path.join(output_dir, output_filename)

    # --- [PHASE 2: HEURISTIC FILE DISCOVERY] ---
    # Attempting to locate 'train' specific shards using recursive glob patterns.
    # Logic: Prioritize training data as it defines the model's future internal state.
    search_pattern = os.path.join(input_dir, "**", "*train*.jsonl.gz")
    files = glob.glob(search_pattern, recursive=True)

    # Fallback Strategy: If no explicit 'train' files exist, target any available JSONL.GZ.
    if not files:
        search_pattern = os.path.join(input_dir, "*.jsonl.gz")
        files = glob.glob(search_pattern, recursive=True)

    # Guard Clause: Prevent execution if the input pipe is empty.
    if not files:
        print(f"Warning: No valid .jsonl.gz streams found in {input_dir}")
        return None

    # Deterministic Selection: Targeting the first shard discovered by the OS.
    source_file = files[0]
    print(f"Generating system preview from: {source_file} -> {full_output_path}")

    # --- [PHASE 3: STREAM DECODING & FIELD MAPPING] ---
    try:
        # Opening the target audit file in UTF-8 to preserve code symbols.
        with open(full_output_path, "w", encoding="utf-8") as out:
            out.write(f"=== DATASET PREVIEW SNAPSHOT ===\n")
            out.write(f"Source Stream: {source_file}\n\n")

            # Binary Stream Context: Decompressing the GZIP payload in text-mode ('rt').
            with gzip.open(source_file, 'rt', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    # Hard cap on sample extraction to maintain utility speed.
                    if i >= num_examples: break
                    
                    try:
                        data = json.loads(line)
                        # SCHEMA NORMALIZATION:
                        # Logic: Handle raw dataset keys (func_...) and refined keys (code/docstring).
                        # Implements a fallback to 'N/A' to avoid key-error crashes.
                        code = data.get('code', data.get('func_code_string', 'N/A'))
                        doc = data.get('docstring', data.get('func_documentation_string', 'N/A'))

                        # Structure the snapshot for human readability.
                        out.write(f"--- SAMPLE #{i+1} ---\n")
                        out.write(f"TARGET SUMMARY (DOC): {doc.strip()}\n")
                        out.write(f"INPUT LOGIC (CODE):\n{code.strip()}\n")
                        out.write("-" * 50 + "\n\n")
                    except json.JSONDecodeError:
                        # Error Telemetry: Mark corrupt JSON objects without breaking the stream.
                        out.write(f"--- SAMPLE #{i+1} [CRITICAL: JSON DECODE ERROR] ---\n\n")
                        
        return full_output_path

    except Exception as e:
        # Low-level exception handling for filesystem/permission issues.
        print(f"I/O Exception during preview generation: {e}")
        return None


# --- STANDALONE EXECUTION BLOCK ---
if __name__ == "__main__":
    """
    Main Driver: Generates dual-previews for comparative logic verification.
    Focus: Inspecting the delta between RAW data and PROCESSED data.
    """
    OUTPUT_DIR = "Project/Datasets/Human_readable_sample"
    
    # Snapshot 1: Audit of the uncleaned, raw dataset.
    save_human_readable_samples(
        input_dir="Project/Datasets/python/final/jsonl",
        output_dir=OUTPUT_DIR,
        output_filename="data_preview_raw.txt"
    )

    # Snapshot 2: Audit of the refined dataset (post-DatasetCleaner execution).
    save_human_readable_samples(
        input_dir="Project/Datasets/processed",
        output_dir=OUTPUT_DIR,
        output_filename="data_preview_processed.txt"
    )

================================================================================

--- FILE: tokenizer.py | PATH: Project\data\tokenizer.py ---
------------------------------------------------------------

"""
================================================================================
TOKENIZATION ENGINE
================================================================================
ROLE: Linguistic Pre-processing and Vocabulary Synthesis.

DESIGN RATIONALE:
- Byte-Pair Encoding (BPE): Balanced approach between character-level and 
  word-level tokenization. Essential for Python where variable names are 
  highly dynamic.
- Atomic Data Handling: Reads directly from compressed streams to build a 
  monolithic training corpus for the BPE trainer.
- Structural Integrity: Reserves specific slots for control tokens (<PAD>, 
  <SOS>, etc.) to ensure the Seq2Seq logic remains deterministic.
================================================================================
"""

import os
import json
import gzip
import logging
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

# Module-level logger for auditing the vocabulary construction process.
logger = logging.getLogger(__name__)

# Target vocabulary size: A trade-off between semantic resolution and VRAM usage.
VOCAB_SIZE = 20000

def train_bpe_tokenizer(files, save_path, vocab_size=VOCAB_SIZE):
    """ 
    Executes the training of the sub-word tokenizer using the HuggingFace library.
    
    Args:
        files (list): List of paths to plain text corpus files.
        save_path (str): Final destination for the serialized .json tokenizer.
        vocab_size (int): Total unique tokens permitted in the vocabulary.
    """
    # 1. ARCHITECTURE INITIALIZATION: 
    # Logic: Start with a BPE model that handles unknown tokens via a dedicated <UNK> tag.
    tokenizer = Tokenizer(models.BPE(unk_token="<UNK>"))
    
    # 2. PRE-TOKENIZATION LAYER:
    # ByteLevel splits by whitespace but protects unique characters. 
    # This is standard for source code to maintain operational logic symbols.
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    
    # 3. TRAINER PROTOCOL:
    # Special tokens are registered at indices 0-3 to ensure consistent tensor alignment 
    # during the padding and sequence initiation phases of the Transformer/LSTM.
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=["<PAD>", "<SOS>", "<EOS>", "<UNK>"],
        show_progress=True
    )
    
    logger.info(f"Training BPE Tokenizer (Target Vocab: {vocab_size})...")

    # 4. TRAINING EXECUTION:
    # Iteratively merges the most frequent adjacent character pairs into sub-words.
    tokenizer.train(files, trainer)

    # 5. PERSISTENCE:
    # Serialize the trained state to disk to avoid re-training in future sessions.
    directory = os.path.dirname(save_path)
    if not os.path.exists(directory):
        os.makedirs(directory)
        
    tokenizer.save(save_path)
    logger.info(f"Tokenizer saved to: {save_path}")

def build_tokenizer(processed_data_dir, save_path, vocab_size=VOCAB_SIZE):
    """ 
    CORPUS REFINERY: Prepares a monolithic text stream from pre-processed JSONL 
    data to feed the BPE trainer.
    
    Logic: Prioritizes Training Data to ensure the model's 'world-view' is based 
    solely on the allowed optimization set.
    """
    
    # TARGETING TRAIN SPLIT: Using valid/test for vocab building is a form of 
    # implicit Data Leakage. We strictly use the refined train.jsonl.gz.
    train_file_path = os.path.join(processed_data_dir, "train.jsonl.gz")
    
    if not os.path.exists(train_file_path):
        logger.error(f"Critical Error: Processed training file missing at {train_file_path}")
        return

    # INTERMEDIATE CORPUS STORAGE: A temporary text file to hold the raw text 
    # before the BPE trainer scans it.
    temp_corpus_file = "temp_corpus.txt"
    logger.info(f"Extracting corpus from {train_file_path}...")
    
    try:
        with open(temp_corpus_file, 'w', encoding='utf-8') as out_f:
            # STREAM DECOMPRESSION: Processing the GZIP stream line-by-line 
            # to minimize the memory footprint of the refinery process.
            with gzip.open(train_file_path, 'rt', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        
                        # DATA EXTRACTION:
                        # Logic: Merge both Code logic and Docstring summaries into the 
                        # corpus so the tokenizer learns common tokens for both domains.
                        code = data.get('code', '')
                        doc = data.get('docstring', '')
                        
                        if code and doc:
                            # Concatenate with newlines to ensure distinct samples.
                            out_f.write(code + "\n" + doc + "\n")
                            
                    except json.JSONDecodeError:
                        continue # Defensive skip for corrupted JSON shards.
        
        # TRANSITION TO TRAINING: Execute the mathematical merging process.
        train_bpe_tokenizer([temp_corpus_file], save_path, vocab_size)
        
    finally:
        # VOLATILE CLEANUP: Remove the large intermediate text file to free disk space.
        if os.path.exists(temp_corpus_file):
            os.remove(temp_corpus_file)

# --- MASTER EXECUTION BLOCK ---
if __name__ == "__main__":
    """
    Standalone utility configuration. 
    Can be run independently of the C2Orchestrator for rapid vocab prototyping.
    """
    logging.basicConfig(level=logging.INFO)
    
    # Path to refined data (Output of DatasetCleaner).
    PROCESSED_DIR = "Project/Datasets/processed"
    
    # Target path for the final Symbolic Mapping file.
    TOKENIZER_PATH = "Project/tokenizer.json"
    
    build_tokenizer(PROCESSED_DIR, TOKENIZER_PATH, vocab_size=VOCAB_SIZE)

================================================================================

--- FILE: data_preview.txt | PATH: Project\Datasets\Human_readable_sample\data_preview.txt ---
----------------------------------------------------------------------------------------------

=== DATASET PREVIEW - CODE SEARCH NET (PYTHON) ===
Source File: c:\Users\matth\GitHub\Project_MLSA_MMA\Project\Datasets\python\final\jsonl\train\python_train_0.jsonl.gz

--- SAMPLE #1 ---
DOCSTRING: Trains a k-nearest neighbors classifier for face recognition.

    :param train_dir: directory that contains a sub-directory for each known person, with its name.

     (View in source code to see train_dir example tree structure)

     Structure:
        <train_dir>/
        â”œâ”€â”€ <person1>/
        â”‚   â”œâ”€â”€ <somename1>.jpeg
        â”‚   â”œâ”€â”€ <somename2>.jpeg
        â”‚   â”œâ”€â”€ ...
        â”œâ”€â”€ <person2>/
        â”‚   â”œâ”€â”€ <somename1>.jpeg
        â”‚   â””â”€â”€ <somename2>.jpeg
        â””â”€â”€ ...

    :param model_save_path: (optional) path to save model on disk
    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified
    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree
    :param verbose: verbosity of training
    :return: returns knn classifier that was trained on the given data.
CODE:
def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):
    """
    Trains a k-nearest neighbors classifier for face recognition.

    :param train_dir: directory that contains a sub-directory for each known person, with its name.

     (View in source code to see train_dir example tree structure)

     Structure:
        <train_dir>/
        â”œâ”€â”€ <person1>/
        â”‚   â”œâ”€â”€ <somename1>.jpeg
        â”‚   â”œâ”€â”€ <somename2>.jpeg
        â”‚   â”œâ”€â”€ ...
        â”œâ”€â”€ <person2>/
        â”‚   â”œâ”€â”€ <somename1>.jpeg
        â”‚   â””â”€â”€ <somename2>.jpeg
        â””â”€â”€ ...

    :param model_save_path: (optional) path to save model on disk
    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified
    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree
    :param verbose: verbosity of training
    :return: returns knn classifier that was trained on the given data.
    """
    X = []
    y = []

    # Loop through each person in the training set
    for class_dir in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_dir)):
            continue

        # Loop through each training image for the current person
        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):
            image = face_recognition.load_image_file(img_path)
            face_bounding_boxes = face_recognition.face_locations(image)

            if len(face_bounding_boxes) != 1:
                # If there are no people (or too many people) in a training image, skip the image.
                if verbose:
                    print("Image {} not suitable for training: {}".format(img_path, "Didn't find a face" if len(face_bounding_boxes) < 1 else "Found more than one face"))
            else:
                # Add face encoding for current image to the training set
                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])
                y.append(class_dir)

    # Determine how many neighbors to use for weighting in the KNN classifier
    if n_neighbors is None:
        n_neighbors = int(round(math.sqrt(len(X))))
        if verbose:
            print("Chose n_neighbors automatically:", n_neighbors)

    # Create and train the KNN classifier
    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')
    knn_clf.fit(X, y)

    # Save the trained KNN classifier
    if model_save_path is not None:
        with open(model_save_path, 'wb') as f:
            pickle.dump(knn_clf, f)

    return knn_clf
--------------------------------------------------

--- SAMPLE #2 ---
DOCSTRING: Recognizes faces in given image using a trained KNN classifier

    :param X_img_path: path to image to be recognized
    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.
    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.
    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance
           of mis-classifying an unknown person as a known one.
    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
        For faces of unrecognized persons, the name 'unknown' will be returned.
CODE:
def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):
    """
    Recognizes faces in given image using a trained KNN classifier

    :param X_img_path: path to image to be recognized
    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.
    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.
    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance
           of mis-classifying an unknown person as a known one.
    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
        For faces of unrecognized persons, the name 'unknown' will be returned.
    """
    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:
        raise Exception("Invalid image path: {}".format(X_img_path))

    if knn_clf is None and model_path is None:
        raise Exception("Must supply knn classifier either thourgh knn_clf or model_path")

    # Load a trained KNN model (if one was passed in)
    if knn_clf is None:
        with open(model_path, 'rb') as f:
            knn_clf = pickle.load(f)

    # Load image file and find face locations
    X_img = face_recognition.load_image_file(X_img_path)
    X_face_locations = face_recognition.face_locations(X_img)

    # If no faces are found in the image, return an empty result.
    if len(X_face_locations) == 0:
        return []

    # Find encodings for faces in the test iamge
    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)

    # Use the KNN model to find the best matches for the test face
    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)
    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]

    # Predict classes and remove classifications that aren't within the threshold
    return [(pred, loc) if rec else ("unknown", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]
--------------------------------------------------

--- SAMPLE #3 ---
DOCSTRING: Shows the face recognition results visually.

    :param img_path: path to image to be recognized
    :param predictions: results of the predict function
    :return:
CODE:
def show_prediction_labels_on_image(img_path, predictions):
    """
    Shows the face recognition results visually.

    :param img_path: path to image to be recognized
    :param predictions: results of the predict function
    :return:
    """
    pil_image = Image.open(img_path).convert("RGB")
    draw = ImageDraw.Draw(pil_image)

    for name, (top, right, bottom, left) in predictions:
        # Draw a box around the face using the Pillow module
        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))

        # There's a bug in Pillow where it blows up with non-UTF-8 text
        # when using the default bitmap font
        name = name.encode("UTF-8")

        # Draw a label with a name below the face
        text_width, text_height = draw.textsize(name)
        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))

    # Remove the drawing library from memory as per the Pillow docs
    del draw

    # Display the resulting image
    pil_image.show()
--------------------------------------------------

--- SAMPLE #4 ---
DOCSTRING: Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order

    :param rect: a dlib 'rect' object
    :return: a plain tuple representation of the rect in (top, right, bottom, left) order
CODE:
def _rect_to_css(rect):
    """
    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order

    :param rect: a dlib 'rect' object
    :return: a plain tuple representation of the rect in (top, right, bottom, left) order
    """
    return rect.top(), rect.right(), rect.bottom(), rect.left()
--------------------------------------------------

--- SAMPLE #5 ---
DOCSTRING: Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.

    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order
    :param image_shape: numpy shape of the image array
    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order
CODE:
def _trim_css_to_bounds(css, image_shape):
    """
    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.

    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order
    :param image_shape: numpy shape of the image array
    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order
    """
    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)
--------------------------------------------------

--- SAMPLE #6 ---
DOCSTRING: Given a list of face encodings, compare them to a known face encoding and get a euclidean distance
    for each comparison face. The distance tells you how similar the faces are.

    :param faces: List of face encodings to compare
    :param face_to_compare: A face encoding to compare against
    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array
CODE:
def face_distance(face_encodings, face_to_compare):
    """
    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance
    for each comparison face. The distance tells you how similar the faces are.

    :param faces: List of face encodings to compare
    :param face_to_compare: A face encoding to compare against
    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array
    """
    if len(face_encodings) == 0:
        return np.empty((0))

    return np.linalg.norm(face_encodings - face_to_compare, axis=1)
--------------------------------------------------

--- SAMPLE #7 ---
DOCSTRING: Loads an image file (.jpg, .png, etc) into a numpy array

    :param file: image file name or file object to load
    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.
    :return: image contents as numpy array
CODE:
def load_image_file(file, mode='RGB'):
    """
    Loads an image file (.jpg, .png, etc) into a numpy array

    :param file: image file name or file object to load
    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.
    :return: image contents as numpy array
    """
    im = PIL.Image.open(file)
    if mode:
        im = im.convert(mode)
    return np.array(im)
--------------------------------------------------

--- SAMPLE #8 ---
DOCSTRING: Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
    :return: A list of dlib 'rect' objects of found face locations
CODE:
def _raw_face_locations(img, number_of_times_to_upsample=1, model="hog"):
    """
    Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
    :return: A list of dlib 'rect' objects of found face locations
    """
    if model == "cnn":
        return cnn_face_detector(img, number_of_times_to_upsample)
    else:
        return face_detector(img, number_of_times_to_upsample)
--------------------------------------------------

--- SAMPLE #9 ---
DOCSTRING: Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
CODE:
def face_locations(img, number_of_times_to_upsample=1, model="hog"):
    """
    Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
    """
    if model == "cnn":
        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, "cnn")]
    else:
        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]
--------------------------------------------------

--- SAMPLE #10 ---
DOCSTRING: Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector
    If you are using a GPU, this can give you much faster results since the GPU
    can process batches of images at once. If you aren't using a GPU, you don't need this function.

    :param img: A list of images (each as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param batch_size: How many images to include in each GPU processing batch.
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
CODE:
def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):
    """
    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector
    If you are using a GPU, this can give you much faster results since the GPU
    can process batches of images at once. If you aren't using a GPU, you don't need this function.

    :param img: A list of images (each as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param batch_size: How many images to include in each GPU processing batch.
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
    """
    def convert_cnn_detections_to_css(detections):
        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]

    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)

    return list(map(convert_cnn_detections_to_css, raw_detections_batched))
--------------------------------------------------

--- SAMPLE #11 ---
DOCSTRING: Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image

    :param face_image: image to search
    :param face_locations: Optionally provide a list of face locations to check.
    :param model: Optional - which model to use. "large" (default) or "small" which only returns 5 points but is faster.
    :return: A list of dicts of face feature locations (eyes, nose, etc)
CODE:
def face_landmarks(face_image, face_locations=None, model="large"):
    """
    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image

    :param face_image: image to search
    :param face_locations: Optionally provide a list of face locations to check.
    :param model: Optional - which model to use. "large" (default) or "small" which only returns 5 points but is faster.
    :return: A list of dicts of face feature locations (eyes, nose, etc)
    """
    landmarks = _raw_face_landmarks(face_image, face_locations, model)
    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]

    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png
    if model == 'large':
        return [{
            "chin": points[0:17],
            "left_eyebrow": points[17:22],
            "right_eyebrow": points[22:27],
            "nose_bridge": points[27:31],
            "nose_tip": points[31:36],
            "left_eye": points[36:42],
            "right_eye": points[42:48],
            "top_lip": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],
            "bottom_lip": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]
        } for points in landmarks_as_tuples]
    elif model == 'small':
        return [{
            "nose_tip": [points[4]],
            "left_eye": points[2:4],
            "right_eye": points[0:2],
        } for points in landmarks_as_tuples]
    else:
        raise ValueError("Invalid landmarks model type. Supported models are ['small', 'large'].")
--------------------------------------------------

--- SAMPLE #12 ---
DOCSTRING: Given an image, return the 128-dimension face encoding for each face in the image.

    :param face_image: The image that contains one or more faces
    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.
    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)
    :return: A list of 128-dimensional face encodings (one for each face in the image)
CODE:
def face_encodings(face_image, known_face_locations=None, num_jitters=1):
    """
    Given an image, return the 128-dimension face encoding for each face in the image.

    :param face_image: The image that contains one or more faces
    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.
    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)
    :return: A list of 128-dimensional face encodings (one for each face in the image)
    """
    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model="small")
    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]
--------------------------------------------------

--- SAMPLE #13 ---
DOCSTRING: Parses the given data type string to a :class:`DataType`. The data type string format equals
    to :class:`DataType.simpleString`, except that top level struct type can omit
    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead
    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name
    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted
    string and case-insensitive strings.

    >>> _parse_datatype_string("int ")
    IntegerType
    >>> _parse_datatype_string("INT ")
    IntegerType
    >>> _parse_datatype_string("a: byte, b: decimal(  16 , 8   ) ")
    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))
    >>> _parse_datatype_string("a DOUBLE, b STRING")
    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))
    >>> _parse_datatype_string("a: array< short>")
    StructType(List(StructField(a,ArrayType(ShortType,true),true)))
    >>> _parse_datatype_string(" map<string , string > ")
    MapType(StringType,StringType,true)

    >>> # Error cases
    >>> _parse_datatype_string("blabla") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("a: int,") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("array<int") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("map<int, boolean>>") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
CODE:
def _parse_datatype_string(s):
    """
    Parses the given data type string to a :class:`DataType`. The data type string format equals
    to :class:`DataType.simpleString`, except that top level struct type can omit
    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead
    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name
    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted
    string and case-insensitive strings.

    >>> _parse_datatype_string("int ")
    IntegerType
    >>> _parse_datatype_string("INT ")
    IntegerType
    >>> _parse_datatype_string("a: byte, b: decimal(  16 , 8   ) ")
    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))
    >>> _parse_datatype_string("a DOUBLE, b STRING")
    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))
    >>> _parse_datatype_string("a: array< short>")
    StructType(List(StructField(a,ArrayType(ShortType,true),true)))
    >>> _parse_datatype_string(" map<string , string > ")
    MapType(StringType,StringType,true)

    >>> # Error cases
    >>> _parse_datatype_string("blabla") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("a: int,") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("array<int") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("map<int, boolean>>") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    """
    sc = SparkContext._active_spark_context

    def from_ddl_schema(type_str):
        return _parse_datatype_json_string(
            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())

    def from_ddl_datatype(type_str):
        return _parse_datatype_json_string(
            sc._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.parseDataType(type_str).json())

    try:
        # DDL format, "fieldname datatype, fieldname datatype".
        return from_ddl_schema(s)
    except Exception as e:
        try:
            # For backwards compatibility, "integer", "struct<fieldname: datatype>" and etc.
            return from_ddl_datatype(s)
        except:
            try:
                # For backwards compatibility, "fieldname: datatype, fieldname: datatype" case.
                return from_ddl_datatype("struct<%s>" % s.strip())
            except:
                raise e
--------------------------------------------------

--- SAMPLE #14 ---
DOCSTRING: Return the Catalyst datatype from the size of integers.
CODE:
def _int_size_to_type(size):
    """
    Return the Catalyst datatype from the size of integers.
    """
    if size <= 8:
        return ByteType
    if size <= 16:
        return ShortType
    if size <= 32:
        return IntegerType
    if size <= 64:
        return LongType
--------------------------------------------------

--- SAMPLE #15 ---
DOCSTRING: Infer the DataType from obj
CODE:
def _infer_type(obj):
    """Infer the DataType from obj
    """
    if obj is None:
        return NullType()

    if hasattr(obj, '__UDT__'):
        return obj.__UDT__

    dataType = _type_mappings.get(type(obj))
    if dataType is DecimalType:
        # the precision and scale of `obj` may be different from row to row.
        return DecimalType(38, 18)
    elif dataType is not None:
        return dataType()

    if isinstance(obj, dict):
        for key, value in obj.items():
            if key is not None and value is not None:
                return MapType(_infer_type(key), _infer_type(value), True)
        return MapType(NullType(), NullType(), True)
    elif isinstance(obj, list):
        for v in obj:
            if v is not None:
                return ArrayType(_infer_type(obj[0]), True)
        return ArrayType(NullType(), True)
    elif isinstance(obj, array):
        if obj.typecode in _array_type_mappings:
            return ArrayType(_array_type_mappings[obj.typecode](), False)
        else:
            raise TypeError("not supported type: array(%s)" % obj.typecode)
    else:
        try:
            return _infer_schema(obj)
        except TypeError:
            raise TypeError("not supported type: %s" % type(obj))
--------------------------------------------------

--- SAMPLE #16 ---
DOCSTRING: Infer the schema from dict/namedtuple/object
CODE:
def _infer_schema(row, names=None):
    """Infer the schema from dict/namedtuple/object"""
    if isinstance(row, dict):
        items = sorted(row.items())

    elif isinstance(row, (tuple, list)):
        if hasattr(row, "__fields__"):  # Row
            items = zip(row.__fields__, tuple(row))
        elif hasattr(row, "_fields"):  # namedtuple
            items = zip(row._fields, tuple(row))
        else:
            if names is None:
                names = ['_%d' % i for i in range(1, len(row) + 1)]
            elif len(names) < len(row):
                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))
            items = zip(names, row)

    elif hasattr(row, "__dict__"):  # object
        items = sorted(row.__dict__.items())

    else:
        raise TypeError("Can not infer schema for type: %s" % type(row))

    fields = [StructField(k, _infer_type(v), True) for k, v in items]
    return StructType(fields)
--------------------------------------------------

--- SAMPLE #17 ---
DOCSTRING: Return whether there is NullType in `dt` or not
CODE:
def _has_nulltype(dt):
    """ Return whether there is NullType in `dt` or not """
    if isinstance(dt, StructType):
        return any(_has_nulltype(f.dataType) for f in dt.fields)
    elif isinstance(dt, ArrayType):
        return _has_nulltype((dt.elementType))
    elif isinstance(dt, MapType):
        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)
    else:
        return isinstance(dt, NullType)
--------------------------------------------------

--- SAMPLE #18 ---
DOCSTRING: Create a converter to drop the names of fields in obj
CODE:
def _create_converter(dataType):
    """Create a converter to drop the names of fields in obj """
    if not _need_converter(dataType):
        return lambda x: x

    if isinstance(dataType, ArrayType):
        conv = _create_converter(dataType.elementType)
        return lambda row: [conv(v) for v in row]

    elif isinstance(dataType, MapType):
        kconv = _create_converter(dataType.keyType)
        vconv = _create_converter(dataType.valueType)
        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())

    elif isinstance(dataType, NullType):
        return lambda x: None

    elif not isinstance(dataType, StructType):
        return lambda x: x

    # dataType must be StructType
    names = [f.name for f in dataType.fields]
    converters = [_create_converter(f.dataType) for f in dataType.fields]
    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)

    def convert_struct(obj):
        if obj is None:
            return

        if isinstance(obj, (tuple, list)):
            if convert_fields:
                return tuple(conv(v) for v, conv in zip(obj, converters))
            else:
                return tuple(obj)

        if isinstance(obj, dict):
            d = obj
        elif hasattr(obj, "__dict__"):  # object
            d = obj.__dict__
        else:
            raise TypeError("Unexpected obj type: %s" % type(obj))

        if convert_fields:
            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])
        else:
            return tuple([d.get(name) for name in names])

    return convert_struct
--------------------------------------------------

--- SAMPLE #19 ---
DOCSTRING: Make a verifier that checks the type of obj against dataType and raises a TypeError if they do
    not match.

    This verifier also checks the value of obj against datatype and raises a ValueError if it's not
    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is
    not checked, so it will become infinity when cast to Java float if it overflows.

    >>> _make_type_verifier(StructType([]))(None)
    >>> _make_type_verifier(StringType())("")
    >>> _make_type_verifier(LongType())(0)
    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))
    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TypeError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})
    >>> _make_type_verifier(StructType([]))(())
    >>> _make_type_verifier(StructType([]))([])
    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> # Check if numeric values are within the allowed range.
    >>> _make_type_verifier(ByteType())(12)
    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(
    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> schema = StructType().add("a", IntegerType()).add("b", StringType(), False)
    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
CODE:
def _make_type_verifier(dataType, nullable=True, name=None):
    """
    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do
    not match.

    This verifier also checks the value of obj against datatype and raises a ValueError if it's not
    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is
    not checked, so it will become infinity when cast to Java float if it overflows.

    >>> _make_type_verifier(StructType([]))(None)
    >>> _make_type_verifier(StringType())("")
    >>> _make_type_verifier(LongType())(0)
    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))
    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TypeError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})
    >>> _make_type_verifier(StructType([]))(())
    >>> _make_type_verifier(StructType([]))([])
    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> # Check if numeric values are within the allowed range.
    >>> _make_type_verifier(ByteType())(12)
    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(
    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> schema = StructType().add("a", IntegerType()).add("b", StringType(), False)
    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    """

    if name is None:
        new_msg = lambda msg: msg
        new_name = lambda n: "field %s" % n
    else:
        new_msg = lambda msg: "%s: %s" % (name, msg)
        new_name = lambda n: "field %s in %s" % (n, name)

    def verify_nullability(obj):
        if obj is None:
            if nullable:
                return True
            else:
                raise ValueError(new_msg("This field is not nullable, but got None"))
        else:
            return False

    _type = type(dataType)

    def assert_acceptable_types(obj):
        assert _type in _acceptable_types, \
            new_msg("unknown datatype: %s for object %r" % (dataType, obj))

    def verify_acceptable_types(obj):
        # subclass of them can not be fromInternal in JVM
        if type(obj) not in _acceptable_types[_type]:
            raise TypeError(new_msg("%s can not accept object %r in type %s"
                                    % (dataType, obj, type(obj))))

    if isinstance(dataType, StringType):
        # StringType can work with any types
        verify_value = lambda _: _

    elif isinstance(dataType, UserDefinedType):
        verifier = _make_type_verifier(dataType.sqlType(), name=name)

        def verify_udf(obj):
            if not (hasattr(obj, '__UDT__') and obj.__UDT__ == dataType):
                raise ValueError(new_msg("%r is not an instance of type %r" % (obj, dataType)))
            verifier(dataType.toInternal(obj))

        verify_value = verify_udf

    elif isinstance(dataType, ByteType):
        def verify_byte(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -128 or obj > 127:
                raise ValueError(new_msg("object of ByteType out of range, got: %s" % obj))

        verify_value = verify_byte

    elif isinstance(dataType, ShortType):
        def verify_short(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -32768 or obj > 32767:
                raise ValueError(new_msg("object of ShortType out of range, got: %s" % obj))

        verify_value = verify_short

    elif isinstance(dataType, IntegerType):
        def verify_integer(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -2147483648 or obj > 2147483647:
                raise ValueError(
                    new_msg("object of IntegerType out of range, got: %s" % obj))

        verify_value = verify_integer

    elif isinstance(dataType, ArrayType):
        element_verifier = _make_type_verifier(
            dataType.elementType, dataType.containsNull, name="element in array %s" % name)

        def verify_array(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            for i in obj:
                element_verifier(i)

        verify_value = verify_array

    elif isinstance(dataType, MapType):
        key_verifier = _make_type_verifier(dataType.keyType, False, name="key of map %s" % name)
        value_verifier = _make_type_verifier(
            dataType.valueType, dataType.valueContainsNull, name="value of map %s" % name)

        def verify_map(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            for k, v in obj.items():
                key_verifier(k)
                value_verifier(v)

        verify_value = verify_map

    elif isinstance(dataType, StructType):
        verifiers = []
        for f in dataType.fields:
            verifier = _make_type_verifier(f.dataType, f.nullable, name=new_name(f.name))
            verifiers.append((f.name, verifier))

        def verify_struct(obj):
            assert_acceptable_types(obj)

            if isinstance(obj, dict):
                for f, verifier in verifiers:
                    verifier(obj.get(f))
            elif isinstance(obj, Row) and getattr(obj, "__from_dict__", False):
                # the order in obj could be different than dataType.fields
                for f, verifier in verifiers:
                    verifier(obj[f])
            elif isinstance(obj, (tuple, list)):
                if len(obj) != len(verifiers):
                    raise ValueError(
                        new_msg("Length of object (%d) does not match with "
                                "length of fields (%d)" % (len(obj), len(verifiers))))
                for v, (_, verifier) in zip(obj, verifiers):
                    verifier(v)
            elif hasattr(obj, "__dict__"):
                d = obj.__dict__
                for f, verifier in verifiers:
                    verifier(d.get(f))
            else:
                raise TypeError(new_msg("StructType can not accept object %r in type %s"
                                        % (obj, type(obj))))
        verify_value = verify_struct

    else:
        def verify_default(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)

        verify_value = verify_default

    def verify(obj):
        if not verify_nullability(obj):
            verify_value(obj)

    return verify
--------------------------------------------------

--- SAMPLE #20 ---
DOCSTRING: Convert Spark data type to pyarrow type
CODE:
def to_arrow_type(dt):
    """ Convert Spark data type to pyarrow type
    """
    import pyarrow as pa
    if type(dt) == BooleanType:
        arrow_type = pa.bool_()
    elif type(dt) == ByteType:
        arrow_type = pa.int8()
    elif type(dt) == ShortType:
        arrow_type = pa.int16()
    elif type(dt) == IntegerType:
        arrow_type = pa.int32()
    elif type(dt) == LongType:
        arrow_type = pa.int64()
    elif type(dt) == FloatType:
        arrow_type = pa.float32()
    elif type(dt) == DoubleType:
        arrow_type = pa.float64()
    elif type(dt) == DecimalType:
        arrow_type = pa.decimal128(dt.precision, dt.scale)
    elif type(dt) == StringType:
        arrow_type = pa.string()
    elif type(dt) == BinaryType:
        arrow_type = pa.binary()
    elif type(dt) == DateType:
        arrow_type = pa.date32()
    elif type(dt) == TimestampType:
        # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read
        arrow_type = pa.timestamp('us', tz='UTC')
    elif type(dt) == ArrayType:
        if type(dt.elementType) in [StructType, TimestampType]:
            raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
        arrow_type = pa.list_(to_arrow_type(dt.elementType))
    elif type(dt) == StructType:
        if any(type(field.dataType) == StructType for field in dt):
            raise TypeError("Nested StructType not supported in conversion to Arrow")
        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
                  for field in dt]
        arrow_type = pa.struct(fields)
    else:
        raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
    return arrow_type
--------------------------------------------------

--- SAMPLE #21 ---
DOCSTRING: Convert a schema from Spark to Arrow
CODE:
def to_arrow_schema(schema):
    """ Convert a schema from Spark to Arrow
    """
    import pyarrow as pa
    fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
              for field in schema]
    return pa.schema(fields)
--------------------------------------------------

--- SAMPLE #22 ---
DOCSTRING: Convert pyarrow type to Spark data type.
CODE:
def from_arrow_type(at):
    """ Convert pyarrow type to Spark data type.
    """
    import pyarrow.types as types
    if types.is_boolean(at):
        spark_type = BooleanType()
    elif types.is_int8(at):
        spark_type = ByteType()
    elif types.is_int16(at):
        spark_type = ShortType()
    elif types.is_int32(at):
        spark_type = IntegerType()
    elif types.is_int64(at):
        spark_type = LongType()
    elif types.is_float32(at):
        spark_type = FloatType()
    elif types.is_float64(at):
        spark_type = DoubleType()
    elif types.is_decimal(at):
        spark_type = DecimalType(precision=at.precision, scale=at.scale)
    elif types.is_string(at):
        spark_type = StringType()
    elif types.is_binary(at):
        spark_type = BinaryType()
    elif types.is_date32(at):
        spark_type = DateType()
    elif types.is_timestamp(at):
        spark_type = TimestampType()
    elif types.is_list(at):
        if types.is_timestamp(at.value_type):
            raise TypeError("Unsupported type in conversion from Arrow: " + str(at))
        spark_type = ArrayType(from_arrow_type(at.value_type))
    elif types.is_struct(at):
        if any(types.is_struct(field.type) for field in at):
            raise TypeError("Nested StructType not supported in conversion from Arrow: " + str(at))
        return StructType(
            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
             for field in at])
    else:
        raise TypeError("Unsupported type in conversion from Arrow: " + str(at))
    return spark_type
--------------------------------------------------

--- SAMPLE #23 ---
DOCSTRING: Convert schema from Arrow to Spark.
CODE:
def from_arrow_schema(arrow_schema):
    """ Convert schema from Arrow to Spark.
    """
    return StructType(
        [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
         for field in arrow_schema])
--------------------------------------------------

--- SAMPLE #24 ---
DOCSTRING: Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.

    If the input series is not a timestamp series, then the same series is returned. If the input
    series is a timestamp series, then a converted series is returned.

    :param s: pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series that have been converted to tz-naive
CODE:
def _check_series_localize_timestamps(s, timezone):
    """
    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.

    If the input series is not a timestamp series, then the same series is returned. If the input
    series is a timestamp series, then a converted series is returned.

    :param s: pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series that have been converted to tz-naive
    """
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    from pandas.api.types import is_datetime64tz_dtype
    tz = timezone or _get_local_timezone()
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(tz).dt.tz_localize(None)
    else:
        return s
--------------------------------------------------

--- SAMPLE #25 ---
DOCSTRING: Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone

    :param pdf: pandas.DataFrame
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive
CODE:
def _check_dataframe_localize_timestamps(pdf, timezone):
    """
    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone

    :param pdf: pandas.DataFrame
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive
    """
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    for column, series in pdf.iteritems():
        pdf[column] = _check_series_localize_timestamps(series, timezone)
    return pdf
--------------------------------------------------

--- SAMPLE #26 ---
DOCSTRING: Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for
    Spark internal storage

    :param s: a pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone
CODE:
def _check_series_convert_timestamps_internal(s, timezone):
    """
    Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for
    Spark internal storage

    :param s: a pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone
    """
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64_dtype(s.dtype):
        # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive
        # timestamp is during the hour when the clock is adjusted backward during due to
        # daylight saving time (dst).
        # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to
        # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize
        # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either
        # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).
        #
        # Here we explicit choose to use standard time. This matches the default behavior of
        # pytz.
        #
        # Here are some code to help understand this behavior:
        # >>> import datetime
        # >>> import pandas as pd
        # >>> import pytz
        # >>>
        # >>> t = datetime.datetime(2015, 11, 1, 1, 30)
        # >>> ts = pd.Series([t])
        # >>> tz = pytz.timezone('America/New_York')
        # >>>
        # >>> ts.dt.tz_localize(tz, ambiguous=True)
        # 0   2015-11-01 01:30:00-04:00
        # dtype: datetime64[ns, America/New_York]
        # >>>
        # >>> ts.dt.tz_localize(tz, ambiguous=False)
        # 0   2015-11-01 01:30:00-05:00
        # dtype: datetime64[ns, America/New_York]
        # >>>
        # >>> str(tz.localize(t))
        # '2015-11-01 01:30:00-05:00'
        tz = timezone or _get_local_timezone()
        return s.dt.tz_localize(tz, ambiguous=False).dt.tz_convert('UTC')
    elif is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert('UTC')
    else:
        return s
--------------------------------------------------

--- SAMPLE #27 ---
DOCSTRING: Convert timestamp to timezone-naive in the specified timezone or local timezone

    :param s: a pandas.Series
    :param from_timezone: the timezone to convert from. if None then use local timezone
    :param to_timezone: the timezone to convert to. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been converted to tz-naive
CODE:
def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):
    """
    Convert timestamp to timezone-naive in the specified timezone or local timezone

    :param s: a pandas.Series
    :param from_timezone: the timezone to convert from. if None then use local timezone
    :param to_timezone: the timezone to convert to. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been converted to tz-naive
    """
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    import pandas as pd
    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype
    from_tz = from_timezone or _get_local_timezone()
    to_tz = to_timezone or _get_local_timezone()
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(to_tz).dt.tz_localize(None)
    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:
        # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.
        return s.apply(
            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)
            if ts is not pd.NaT else pd.NaT)
    else:
        return s
--------------------------------------------------

--- SAMPLE #28 ---
DOCSTRING: Construct a StructType by adding new elements to it to define the schema. The method accepts
        either:

            a) A single parameter which is a StructField object.
            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),
               metadata(optional). The data_type parameter may be either a String or a
               DataType object.

        >>> struct1 = StructType().add("f1", StringType(), True).add("f2", StringType(), True, None)
        >>> struct2 = StructType([StructField("f1", StringType(), True), \\
        ...     StructField("f2", StringType(), True, None)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(StructField("f1", StringType(), True))
        >>> struct2 = StructType([StructField("f1", StringType(), True)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add("f1", "string", True)
        >>> struct2 = StructType([StructField("f1", StringType(), True)])
        >>> struct1 == struct2
        True

        :param field: Either the name of the field or a StructField object
        :param data_type: If present, the DataType of the StructField to create
        :param nullable: Whether the field to add should be nullable (default True)
        :param metadata: Any additional metadata (default None)
        :return: a new updated StructType
CODE:
def add(self, field, data_type=None, nullable=True, metadata=None):
        """
        Construct a StructType by adding new elements to it to define the schema. The method accepts
        either:

            a) A single parameter which is a StructField object.
            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),
               metadata(optional). The data_type parameter may be either a String or a
               DataType object.

        >>> struct1 = StructType().add("f1", StringType(), True).add("f2", StringType(), True, None)
        >>> struct2 = StructType([StructField("f1", StringType(), True), \\
        ...     StructField("f2", StringType(), True, None)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(StructField("f1", StringType(), True))
        >>> struct2 = StructType([StructField("f1", StringType(), True)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add("f1", "string", True)
        >>> struct2 = StructType([StructField("f1", StringType(), True)])
        >>> struct1 == struct2
        True

        :param field: Either the name of the field or a StructField object
        :param data_type: If present, the DataType of the StructField to create
        :param nullable: Whether the field to add should be nullable (default True)
        :param metadata: Any additional metadata (default None)
        :return: a new updated StructType
        """
        if isinstance(field, StructField):
            self.fields.append(field)
            self.names.append(field.name)
        else:
            if isinstance(field, str) and data_type is None:
                raise ValueError("Must specify DataType if passing name of struct_field to create.")

            if isinstance(data_type, str):
                data_type_f = _parse_datatype_json_value(data_type)
            else:
                data_type_f = data_type
            self.fields.append(StructField(field, data_type_f, nullable, metadata))
            self.names.append(field)
        # Precalculated list of fields that need conversion with fromInternal/toInternal functions
        self._needConversion = [f.needConversion() for f in self]
        self._needSerializeAnyField = any(self._needConversion)
        return self
--------------------------------------------------

--- SAMPLE #29 ---
DOCSTRING: Cache the sqlType() into class, because it's heavy used in `toInternal`.
CODE:
def _cachedSqlType(cls):
        """
        Cache the sqlType() into class, because it's heavy used in `toInternal`.
        """
        if not hasattr(cls, "_cached_sql_type"):
            cls._cached_sql_type = cls.sqlType()
        return cls._cached_sql_type
--------------------------------------------------

--- SAMPLE #30 ---
DOCSTRING: Return as an dict

        :param recursive: turns the nested Row as dict (default: False).

        >>> Row(name="Alice", age=11).asDict() == {'name': 'Alice', 'age': 11}
        True
        >>> row = Row(key=1, value=Row(name='a', age=2))
        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}
        True
        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}
        True
CODE:
def asDict(self, recursive=False):
        """
        Return as an dict

        :param recursive: turns the nested Row as dict (default: False).

        >>> Row(name="Alice", age=11).asDict() == {'name': 'Alice', 'age': 11}
        True
        >>> row = Row(key=1, value=Row(name='a', age=2))
        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}
        True
        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}
        True
        """
        if not hasattr(self, "__fields__"):
            raise TypeError("Cannot convert a Row class into dict")

        if recursive:
            def conv(obj):
                if isinstance(obj, Row):
                    return obj.asDict(True)
                elif isinstance(obj, list):
                    return [conv(o) for o in obj]
                elif isinstance(obj, dict):
                    return dict((k, conv(v)) for k, v in obj.items())
                else:
                    return obj
            return dict(zip(self.__fields__, (conv(o) for o in self)))
        else:
            return dict(zip(self.__fields__, self))
--------------------------------------------------

--- SAMPLE #31 ---
DOCSTRING: Gets summary (e.g. residuals, mse, r-squared ) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
CODE:
def summary(self):
        """
        Gets summary (e.g. residuals, mse, r-squared ) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
        """
        if self.hasSummary:
            return LinearRegressionTrainingSummary(super(LinearRegressionModel, self).summary)
        else:
            raise RuntimeError("No training summary available for this %s" %
                               self.__class__.__name__)
--------------------------------------------------

--- SAMPLE #32 ---
DOCSTRING: Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
CODE:
def evaluate(self, dataset):
        """
        Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
        """
        if not isinstance(dataset, DataFrame):
            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))
        java_lr_summary = self._call_java("evaluate", dataset)
        return LinearRegressionSummary(java_lr_summary)
--------------------------------------------------

--- SAMPLE #33 ---
DOCSTRING: Gets summary (e.g. residuals, deviance, pValues) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
CODE:
def summary(self):
        """
        Gets summary (e.g. residuals, deviance, pValues) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
        """
        if self.hasSummary:
            return GeneralizedLinearRegressionTrainingSummary(
                super(GeneralizedLinearRegressionModel, self).summary)
        else:
            raise RuntimeError("No training summary available for this %s" %
                               self.__class__.__name__)
--------------------------------------------------

--- SAMPLE #34 ---
DOCSTRING: Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
CODE:
def evaluate(self, dataset):
        """
        Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
        """
        if not isinstance(dataset, DataFrame):
            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))
        java_glr_summary = self._call_java("evaluate", dataset)
        return GeneralizedLinearRegressionSummary(java_glr_summary)
--------------------------------------------------

--- SAMPLE #35 ---
DOCSTRING: Get all the directories
CODE:
def _get_local_dirs(sub):
    """ Get all the directories """
    path = os.environ.get("SPARK_LOCAL_DIRS", "/tmp")
    dirs = path.split(",")
    if len(dirs) > 1:
        # different order in different processes and instances
        rnd = random.Random(os.getpid() + id(dirs))
        random.shuffle(dirs, rnd.random)
    return [os.path.join(d, "python", str(os.getpid()), sub) for d in dirs]
--------------------------------------------------

--- SAMPLE #36 ---
DOCSTRING: Choose one directory for spill by number n
CODE:
def _get_spill_dir(self, n):
        """ Choose one directory for spill by number n """
        return os.path.join(self.localdirs[n % len(self.localdirs)], str(n))
--------------------------------------------------

--- SAMPLE #37 ---
DOCSTRING: Combine the items by creator and combiner
CODE:
def mergeValues(self, iterator):
        """ Combine the items by creator and combiner """
        # speedup attribute lookup
        creator, comb = self.agg.createCombiner, self.agg.mergeValue
        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch
        limit = self.memory_limit

        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else creator(v)

            c += 1
            if c >= batch:
                if get_used_memory() >= limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5

        if get_used_memory() >= limit:
            self._spill()
--------------------------------------------------

--- SAMPLE #38 ---
DOCSTRING: Merge (K,V) pair by mergeCombiner
CODE:
def mergeCombiners(self, iterator, limit=None):
        """ Merge (K,V) pair by mergeCombiner """
        if limit is None:
            limit = self.memory_limit
        # speedup attribute lookup
        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size
        c, data, pdata, batch = 0, self.data, self.pdata, self.batch
        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else v
            if not limit:
                continue

            c += objsize(v)
            if c > batch:
                if get_used_memory() > limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5

        if limit and get_used_memory() >= limit:
            self._spill()
--------------------------------------------------

--- SAMPLE #39 ---
DOCSTRING: dump already partitioned data into disks.

        It will dump the data in batch for better performance.
CODE:
def _spill(self):
        """
        dump already partitioned data into disks.

        It will dump the data in batch for better performance.
        """
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)

        used_memory = get_used_memory()
        if not self.pdata:
            # The data has not been partitioned, it will iterator the
            # dataset once, write them into different files, has no
            # additional memory. It only called when the memory goes
            # above limit at the first time.

            # open all the files for writing
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]

            for k, v in self.data.items():
                h = self._partition(k)
                # put one item in batch, make it compatible with load_stream
                # it will increase the memory if dump them in batch
                self.serializer.dump_stream([(k, v)], streams[h])

            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()

            self.data.clear()
            self.pdata.extend([{} for i in range(self.partitions)])

        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, "wb") as f:
                    # dump items in batch
                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)

        self.spills += 1
        gc.collect()  # release the memory as much as possible
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #40 ---
DOCSTRING: Return all merged items as iterator
CODE:
def items(self):
        """ Return all merged items as iterator """
        if not self.pdata and not self.spills:
            return iter(self.data.items())
        return self._external_items()
--------------------------------------------------

--- SAMPLE #41 ---
DOCSTRING: Return all partitioned items as iterator
CODE:
def _external_items(self):
        """ Return all partitioned items as iterator """
        assert not self.data
        if any(self.pdata):
            self._spill()
        # disable partitioning and spilling when merge combiners from disk
        self.pdata = []

        try:
            for i in range(self.partitions):
                for v in self._merged_items(i):
                    yield v
                self.data.clear()

                # remove the merged partition
                for j in range(self.spills):
                    path = self._get_spill_dir(j)
                    os.remove(os.path.join(path, str(i)))
        finally:
            self._cleanup()
--------------------------------------------------

--- SAMPLE #42 ---
DOCSTRING: merge the partitioned items and return the as iterator

        If one partition can not be fit in memory, then them will be
        partitioned and merged recursively.
CODE:
def _recursive_merged_items(self, index):
        """
        merge the partitioned items and return the as iterator

        If one partition can not be fit in memory, then them will be
        partitioned and merged recursively.
        """
        subdirs = [os.path.join(d, "parts", str(index)) for d in self.localdirs]
        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,
                           self.scale * self.partitions, self.partitions, self.batch)
        m.pdata = [{} for _ in range(self.partitions)]
        limit = self._next_limit()

        for j in range(self.spills):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb') as f:
                m.mergeCombiners(self.serializer.load_stream(f), 0)

            if get_used_memory() > limit:
                m._spill()
                limit = self._next_limit()

        return m._external_items()
--------------------------------------------------

--- SAMPLE #43 ---
DOCSTRING: Choose one directory for spill by number n
CODE:
def _get_path(self, n):
        """ Choose one directory for spill by number n """
        d = self.local_dirs[n % len(self.local_dirs)]
        if not os.path.exists(d):
            os.makedirs(d)
        return os.path.join(d, str(n))
--------------------------------------------------

--- SAMPLE #44 ---
DOCSTRING: Sort the elements in iterator, do external sort when the memory
        goes above the limit.
CODE:
def sorted(self, iterator, key=None, reverse=False):
        """
        Sort the elements in iterator, do external sort when the memory
        goes above the limit.
        """
        global MemoryBytesSpilled, DiskBytesSpilled
        batch, limit = 100, self._next_limit()
        chunks, current_chunk = [], []
        iterator = iter(iterator)
        while True:
            # pick elements in batch
            chunk = list(itertools.islice(iterator, batch))
            current_chunk.extend(chunk)
            if len(chunk) < batch:
                break

            used_memory = get_used_memory()
            if used_memory > limit:
                # sort them inplace will save memory
                current_chunk.sort(key=key, reverse=reverse)
                path = self._get_path(len(chunks))
                with open(path, 'wb') as f:
                    self.serializer.dump_stream(current_chunk, f)

                def load(f):
                    for v in self.serializer.load_stream(f):
                        yield v
                    # close the file explicit once we consume all the items
                    # to avoid ResourceWarning in Python3
                    f.close()
                chunks.append(load(open(path, 'rb')))
                current_chunk = []
                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
                DiskBytesSpilled += os.path.getsize(path)
                os.unlink(path)  # data will be deleted after close

            elif not chunks:
                batch = min(int(batch * 1.5), 10000)

        current_chunk.sort(key=key, reverse=reverse)
        if not chunks:
            return current_chunk

        if current_chunk:
            chunks.append(iter(current_chunk))

        return heapq.merge(chunks, key=key, reverse=reverse)
--------------------------------------------------

--- SAMPLE #45 ---
DOCSTRING: dump the values into disk
CODE:
def _spill(self):
        """ dump the values into disk """
        global MemoryBytesSpilled, DiskBytesSpilled
        if self._file is None:
            self._open_file()

        used_memory = get_used_memory()
        pos = self._file.tell()
        self._ser.dump_stream(self.values, self._file)
        self.values = []
        gc.collect()
        DiskBytesSpilled += self._file.tell() - pos
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #46 ---
DOCSTRING: dump already partitioned data into disks.
CODE:
def _spill(self):
        """
        dump already partitioned data into disks.
        """
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)

        used_memory = get_used_memory()
        if not self.pdata:
            # The data has not been partitioned, it will iterator the
            # data once, write them into different files, has no
            # additional memory. It only called when the memory goes
            # above limit at the first time.

            # open all the files for writing
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]

            # If the number of keys is small, then the overhead of sort is small
            # sort them before dumping into disks
            self._sorted = len(self.data) < self.SORT_KEY_LIMIT
            if self._sorted:
                self.serializer = self.flattened_serializer()
                for k in sorted(self.data.keys()):
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, self.data[k])], streams[h])
            else:
                for k, v in self.data.items():
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, v)], streams[h])

            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()

            self.data.clear()
            # self.pdata is cached in `mergeValues` and `mergeCombiners`
            self.pdata.extend([{} for i in range(self.partitions)])

        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, "wb") as f:
                    # dump items in batch
                    if self._sorted:
                        # sort by key only (stable)
                        sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))
                        self.serializer.dump_stream(sorted_items, f)
                    else:
                        self.serializer.dump_stream(self.pdata[i].items(), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)

        self.spills += 1
        gc.collect()  # release the memory as much as possible
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #47 ---
DOCSTRING: load a partition from disk, then sort and group by key
CODE:
def _merge_sorted_items(self, index):
        """ load a partition from disk, then sort and group by key """
        def load_partition(j):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb', 65536) as f:
                for v in self.serializer.load_stream(f):
                    yield v

        disk_items = [load_partition(j) for j in range(self.spills)]

        if self._sorted:
            # all the partitions are already sorted
            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))

        else:
            # Flatten the combined values, so it will not consume huge
            # memory during merging sort.
            ser = self.flattened_serializer()
            sorter = ExternalSorter(self.memory_limit, ser)
            sorted_items = sorter.sorted(itertools.chain(*disk_items),
                                         key=operator.itemgetter(0))
        return ((k, vs) for k, vs in GroupByKey(sorted_items))
--------------------------------------------------

--- SAMPLE #48 ---
DOCSTRING: Called by a worker process after the fork().
CODE:
def worker(sock, authenticated):
    """
    Called by a worker process after the fork().
    """
    signal.signal(SIGHUP, SIG_DFL)
    signal.signal(SIGCHLD, SIG_DFL)
    signal.signal(SIGTERM, SIG_DFL)
    # restore the handler for SIGINT,
    # it's useful for debugging (show the stacktrace before exit)
    signal.signal(SIGINT, signal.default_int_handler)

    # Read the socket using fdopen instead of socket.makefile() because the latter
    # seems to be very slow; note that we need to dup() the file descriptor because
    # otherwise writes also cause a seek that makes us miss data on the read side.
    infile = os.fdopen(os.dup(sock.fileno()), "rb", 65536)
    outfile = os.fdopen(os.dup(sock.fileno()), "wb", 65536)

    if not authenticated:
        client_secret = UTF8Deserializer().loads(infile)
        if os.environ["PYTHON_WORKER_FACTORY_SECRET"] == client_secret:
            write_with_length("ok".encode("utf-8"), outfile)
            outfile.flush()
        else:
            write_with_length("err".encode("utf-8"), outfile)
            outfile.flush()
            sock.close()
            return 1

    exit_code = 0
    try:
        worker_main(infile, outfile)
    except SystemExit as exc:
        exit_code = compute_real_exit_code(exc.code)
    finally:
        try:
            outfile.flush()
        except Exception:
            pass
    return exit_code
--------------------------------------------------

--- SAMPLE #49 ---
DOCSTRING: This function returns consistent hash code for builtin types, especially
    for None and tuple with None.

    The algorithm is similar to that one used by CPython 2.7

    >>> portable_hash(None)
    0
    >>> portable_hash((None, 1)) & 0xffffffff
    219750521
CODE:
def portable_hash(x):
    """
    This function returns consistent hash code for builtin types, especially
    for None and tuple with None.

    The algorithm is similar to that one used by CPython 2.7

    >>> portable_hash(None)
    0
    >>> portable_hash((None, 1)) & 0xffffffff
    219750521
    """

    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:
        raise Exception("Randomness of hash of string should be disabled via PYTHONHASHSEED")

    if x is None:
        return 0
    if isinstance(x, tuple):
        h = 0x345678
        for i in x:
            h ^= portable_hash(i)
            h *= 1000003
            h &= sys.maxsize
        h ^= len(x)
        if h == -1:
            h = -2
        return int(h)
    return hash(x)
--------------------------------------------------

--- SAMPLE #50 ---
DOCSTRING: Parse a memory string in the format supported by Java (e.g. 1g, 200m) and
    return the value in MiB

    >>> _parse_memory("256m")
    256
    >>> _parse_memory("2g")
    2048
CODE:
def _parse_memory(s):
    """
    Parse a memory string in the format supported by Java (e.g. 1g, 200m) and
    return the value in MiB

    >>> _parse_memory("256m")
    256
    >>> _parse_memory("2g")
    2048
    """
    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}
    if s[-1].lower() not in units:
        raise ValueError("invalid format: " + s)
    return int(float(s[:-1]) * units[s[-1].lower()])
--------------------------------------------------

--- SAMPLE #51 ---
DOCSTRING: Ignore the 'u' prefix of string in doc tests, to make it works
    in both python 2 and 3
CODE:
def ignore_unicode_prefix(f):
    """
    Ignore the 'u' prefix of string in doc tests, to make it works
    in both python 2 and 3
    """
    if sys.version >= '3':
        # the representation of unicode string in Python 3 does not have prefix 'u',
        # so remove the prefix 'u' for doc tests
        literal_re = re.compile(r"(\W|^)[uU](['])", re.UNICODE)
        f.__doc__ = literal_re.sub(r'\1\2', f.__doc__)
    return f
--------------------------------------------------

--- SAMPLE #52 ---
DOCSTRING: Persist this RDD with the default storage level (C{MEMORY_ONLY}).
CODE:
def cache(self):
        """
        Persist this RDD with the default storage level (C{MEMORY_ONLY}).
        """
        self.is_cached = True
        self.persist(StorageLevel.MEMORY_ONLY)
        return self
--------------------------------------------------

--- SAMPLE #53 ---
DOCSTRING: Set this RDD's storage level to persist its values across operations
        after the first time it is computed. This can only be used to assign
        a new storage level if the RDD does not have a storage level set yet.
        If no storage level is specified defaults to (C{MEMORY_ONLY}).

        >>> rdd = sc.parallelize(["b", "a", "c"])
        >>> rdd.persist().is_cached
        True
CODE:
def persist(self, storageLevel=StorageLevel.MEMORY_ONLY):
        """
        Set this RDD's storage level to persist its values across operations
        after the first time it is computed. This can only be used to assign
        a new storage level if the RDD does not have a storage level set yet.
        If no storage level is specified defaults to (C{MEMORY_ONLY}).

        >>> rdd = sc.parallelize(["b", "a", "c"])
        >>> rdd.persist().is_cached
        True
        """
        self.is_cached = True
        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
        self._jrdd.persist(javaStorageLevel)
        return self
--------------------------------------------------

--- SAMPLE #54 ---
DOCSTRING: Mark the RDD as non-persistent, and remove all blocks for it from
        memory and disk.

        .. versionchanged:: 3.0.0
           Added optional argument `blocking` to specify whether to block until all
           blocks are deleted.
CODE:
def unpersist(self, blocking=False):
        """
        Mark the RDD as non-persistent, and remove all blocks for it from
        memory and disk.

        .. versionchanged:: 3.0.0
           Added optional argument `blocking` to specify whether to block until all
           blocks are deleted.
        """
        self.is_cached = False
        self._jrdd.unpersist(blocking)
        return self
--------------------------------------------------

--- SAMPLE #55 ---
DOCSTRING: Gets the name of the file to which this RDD was checkpointed

        Not defined if RDD is checkpointed locally.
CODE:
def getCheckpointFile(self):
        """
        Gets the name of the file to which this RDD was checkpointed

        Not defined if RDD is checkpointed locally.
        """
        checkpointFile = self._jrdd.rdd().getCheckpointFile()
        if checkpointFile.isDefined():
            return checkpointFile.get()
--------------------------------------------------

--- SAMPLE #56 ---
DOCSTRING: Return a new RDD by applying a function to each element of this RDD.

        >>> rdd = sc.parallelize(["b", "a", "c"])
        >>> sorted(rdd.map(lambda x: (x, 1)).collect())
        [('a', 1), ('b', 1), ('c', 1)]
CODE:
def map(self, f, preservesPartitioning=False):
        """
        Return a new RDD by applying a function to each element of this RDD.

        >>> rdd = sc.parallelize(["b", "a", "c"])
        >>> sorted(rdd.map(lambda x: (x, 1)).collect())
        [('a', 1), ('b', 1), ('c', 1)]
        """
        def func(_, iterator):
            return map(fail_on_stopiteration(f), iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)
--------------------------------------------------

--- SAMPLE #57 ---
DOCSTRING: Return a new RDD by first applying a function to all elements of this
        RDD, and then flattening the results.

        >>> rdd = sc.parallelize([2, 3, 4])
        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())
        [1, 1, 1, 2, 2, 3]
        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())
        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]
CODE:
def flatMap(self, f, preservesPartitioning=False):
        """
        Return a new RDD by first applying a function to all elements of this
        RDD, and then flattening the results.

        >>> rdd = sc.parallelize([2, 3, 4])
        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())
        [1, 1, 1, 2, 2, 3]
        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())
        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]
        """
        def func(s, iterator):
            return chain.from_iterable(map(fail_on_stopiteration(f), iterator))
        return self.mapPartitionsWithIndex(func, preservesPartitioning)
--------------------------------------------------

--- SAMPLE #58 ---
DOCSTRING: Return a new RDD by applying a function to each partition of this RDD.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
        >>> def f(iterator): yield sum(iterator)
        >>> rdd.mapPartitions(f).collect()
        [3, 7]
CODE:
def mapPartitions(self, f, preservesPartitioning=False):
        """
        Return a new RDD by applying a function to each partition of this RDD.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
        >>> def f(iterator): yield sum(iterator)
        >>> rdd.mapPartitions(f).collect()
        [3, 7]
        """
        def func(s, iterator):
            return f(iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)
--------------------------------------------------

--- SAMPLE #59 ---
DOCSTRING: Deprecated: use mapPartitionsWithIndex instead.

        Return a new RDD by applying a function to each partition of this RDD,
        while tracking the index of the original partition.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
        >>> def f(splitIndex, iterator): yield splitIndex
        >>> rdd.mapPartitionsWithSplit(f).sum()
        6
CODE:
def mapPartitionsWithSplit(self, f, preservesPartitioning=False):
        """
        Deprecated: use mapPartitionsWithIndex instead.

        Return a new RDD by applying a function to each partition of this RDD,
        while tracking the index of the original partition.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
        >>> def f(splitIndex, iterator): yield splitIndex
        >>> rdd.mapPartitionsWithSplit(f).sum()
        6
        """
        warnings.warn("mapPartitionsWithSplit is deprecated; "
                      "use mapPartitionsWithIndex instead", DeprecationWarning, stacklevel=2)
        return self.mapPartitionsWithIndex(f, preservesPartitioning)
--------------------------------------------------

--- SAMPLE #60 ---
DOCSTRING: Return a new RDD containing the distinct elements in this RDD.

        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())
        [1, 2, 3]
CODE:
def distinct(self, numPartitions=None):
        """
        Return a new RDD containing the distinct elements in this RDD.

        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())
        [1, 2, 3]
        """
        return self.map(lambda x: (x, None)) \
                   .reduceByKey(lambda x, _: x, numPartitions) \
                   .map(lambda x: x[0])
--------------------------------------------------

--- SAMPLE #61 ---
DOCSTRING: Return a sampled subset of this RDD.

        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)
        :param fraction: expected size of the sample as a fraction of this RDD's size
            without replacement: probability that each element is chosen; fraction must be [0, 1]
            with replacement: expected number of times each element is chosen; fraction must be >= 0
        :param seed: seed for the random number generator

        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
            count of the given :class:`DataFrame`.

        >>> rdd = sc.parallelize(range(100), 4)
        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14
        True
CODE:
def sample(self, withReplacement, fraction, seed=None):
        """
        Return a sampled subset of this RDD.

        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)
        :param fraction: expected size of the sample as a fraction of this RDD's size
            without replacement: probability that each element is chosen; fraction must be [0, 1]
            with replacement: expected number of times each element is chosen; fraction must be >= 0
        :param seed: seed for the random number generator

        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
            count of the given :class:`DataFrame`.

        >>> rdd = sc.parallelize(range(100), 4)
        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14
        True
        """
        assert fraction >= 0.0, "Negative fraction value: %s" % fraction
        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)
--------------------------------------------------

--- SAMPLE #62 ---
DOCSTRING: Randomly splits this RDD with the provided weights.

        :param weights: weights for splits, will be normalized if they don't sum to 1
        :param seed: random seed
        :return: split RDDs in a list

        >>> rdd = sc.parallelize(range(500), 1)
        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)
        >>> len(rdd1.collect() + rdd2.collect())
        500
        >>> 150 < rdd1.count() < 250
        True
        >>> 250 < rdd2.count() < 350
        True
CODE:
def randomSplit(self, weights, seed=None):
        """
        Randomly splits this RDD with the provided weights.

        :param weights: weights for splits, will be normalized if they don't sum to 1
        :param seed: random seed
        :return: split RDDs in a list

        >>> rdd = sc.parallelize(range(500), 1)
        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)
        >>> len(rdd1.collect() + rdd2.collect())
        500
        >>> 150 < rdd1.count() < 250
        True
        >>> 250 < rdd2.count() < 350
        True
        """
        s = float(sum(weights))
        cweights = [0.0]
        for w in weights:
            cweights.append(cweights[-1] + w / s)
        if seed is None:
            seed = random.randint(0, 2 ** 32 - 1)
        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)
                for lb, ub in zip(cweights, cweights[1:])]
--------------------------------------------------

--- SAMPLE #63 ---
DOCSTRING: Return a fixed-size sampled subset of this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> rdd = sc.parallelize(range(0, 10))
        >>> len(rdd.takeSample(True, 20, 1))
        20
        >>> len(rdd.takeSample(False, 5, 2))
        5
        >>> len(rdd.takeSample(False, 15, 3))
        10
CODE:
def takeSample(self, withReplacement, num, seed=None):
        """
        Return a fixed-size sampled subset of this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> rdd = sc.parallelize(range(0, 10))
        >>> len(rdd.takeSample(True, 20, 1))
        20
        >>> len(rdd.takeSample(False, 5, 2))
        5
        >>> len(rdd.takeSample(False, 15, 3))
        10
        """
        numStDev = 10.0

        if num < 0:
            raise ValueError("Sample size cannot be negative.")
        elif num == 0:
            return []

        initialCount = self.count()
        if initialCount == 0:
            return []

        rand = random.Random(seed)

        if (not withReplacement) and num >= initialCount:
            # shuffle current RDD and return
            samples = self.collect()
            rand.shuffle(samples)
            return samples

        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))
        if num > maxSampleSize:
            raise ValueError(
                "Sample size cannot be greater than %d." % maxSampleSize)

        fraction = RDD._computeFractionForSampleSize(
            num, initialCount, withReplacement)
        samples = self.sample(withReplacement, fraction, seed).collect()

        # If the first sample didn't turn out large enough, keep trying to take samples;
        # this shouldn't happen often because we use a big multiplier for their initial size.
        # See: scala/spark/RDD.scala
        while len(samples) < num:
            # TODO: add log warning for when more than one iteration was run
            seed = rand.randint(0, sys.maxsize)
            samples = self.sample(withReplacement, fraction, seed).collect()

        rand.shuffle(samples)

        return samples[0:num]
--------------------------------------------------

--- SAMPLE #64 ---
DOCSTRING: Returns a sampling rate that guarantees a sample of
        size >= sampleSizeLowerBound 99.99% of the time.

        How the sampling rate is determined:
        Let p = num / total, where num is the sample size and total is the
        total number of data points in the RDD. We're trying to compute
        q > p such that
          - when sampling with replacement, we're drawing each data point
            with prob_i ~ Pois(q), where we want to guarantee
            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to
            total), i.e. the failure rate of not having a sufficiently large
            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient
            to guarantee 0.9999 success rate for num > 12, but we need a
            slightly larger q (9 empirically determined).
          - when sampling without replacement, we're drawing each data point
            with prob_i ~ Binomial(total, fraction) and our choice of q
            guarantees 1-delta, or 0.9999 success rate, where success rate is
            defined the same as in sampling with replacement.
CODE:
def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):
        """
        Returns a sampling rate that guarantees a sample of
        size >= sampleSizeLowerBound 99.99% of the time.

        How the sampling rate is determined:
        Let p = num / total, where num is the sample size and total is the
        total number of data points in the RDD. We're trying to compute
        q > p such that
          - when sampling with replacement, we're drawing each data point
            with prob_i ~ Pois(q), where we want to guarantee
            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to
            total), i.e. the failure rate of not having a sufficiently large
            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient
            to guarantee 0.9999 success rate for num > 12, but we need a
            slightly larger q (9 empirically determined).
          - when sampling without replacement, we're drawing each data point
            with prob_i ~ Binomial(total, fraction) and our choice of q
            guarantees 1-delta, or 0.9999 success rate, where success rate is
            defined the same as in sampling with replacement.
        """
        fraction = float(sampleSizeLowerBound) / total
        if withReplacement:
            numStDev = 5
            if (sampleSizeLowerBound < 12):
                numStDev = 9
            return fraction + numStDev * sqrt(fraction / total)
        else:
            delta = 0.00005
            gamma = - log(delta) / total
            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))
--------------------------------------------------

--- SAMPLE #65 ---
DOCSTRING: Return the union of this RDD and another one.

        >>> rdd = sc.parallelize([1, 1, 2, 3])
        >>> rdd.union(rdd).collect()
        [1, 1, 2, 3, 1, 1, 2, 3]
CODE:
def union(self, other):
        """
        Return the union of this RDD and another one.

        >>> rdd = sc.parallelize([1, 1, 2, 3])
        >>> rdd.union(rdd).collect()
        [1, 1, 2, 3, 1, 1, 2, 3]
        """
        if self._jrdd_deserializer == other._jrdd_deserializer:
            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,
                      self._jrdd_deserializer)
        else:
            # These RDDs contain data in different serialized formats, so we
            # must normalize them to the default serializer.
            self_copy = self._reserialize()
            other_copy = other._reserialize()
            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,
                      self.ctx.serializer)
        if (self.partitioner == other.partitioner and
                self.getNumPartitions() == rdd.getNumPartitions()):
            rdd.partitioner = self.partitioner
        return rdd
--------------------------------------------------

--- SAMPLE #66 ---
DOCSTRING: Return the intersection of this RDD and another one. The output will
        not contain any duplicate elements, even if the input RDDs did.

        .. note:: This method performs a shuffle internally.

        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
        >>> rdd1.intersection(rdd2).collect()
        [1, 2, 3]
CODE:
def intersection(self, other):
        """
        Return the intersection of this RDD and another one. The output will
        not contain any duplicate elements, even if the input RDDs did.

        .. note:: This method performs a shuffle internally.

        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
        >>> rdd1.intersection(rdd2).collect()
        [1, 2, 3]
        """
        return self.map(lambda v: (v, None)) \
            .cogroup(other.map(lambda v: (v, None))) \
            .filter(lambda k_vs: all(k_vs[1])) \
            .keys()
--------------------------------------------------

--- SAMPLE #67 ---
DOCSTRING: Repartition the RDD according to the given partitioner and, within each resulting partition,
        sort records by their keys.

        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])
        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)
        >>> rdd2.glom().collect()
        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]
CODE:
def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,
                                           ascending=True, keyfunc=lambda x: x):
        """
        Repartition the RDD according to the given partitioner and, within each resulting partition,
        sort records by their keys.

        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])
        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)
        >>> rdd2.glom().collect()
        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]
        """
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        memory = _parse_memory(self.ctx._conf.get("spark.python.worker.memory", "512m"))
        serializer = self._jrdd_deserializer

        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))

        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)
--------------------------------------------------

--- SAMPLE #68 ---
DOCSTRING: Sorts this RDD, which is assumed to consist of (key, value) pairs.

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortByKey().first()
        ('1', 3)
        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]
        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])
        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]
CODE:
def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):
        """
        Sorts this RDD, which is assumed to consist of (key, value) pairs.

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortByKey().first()
        ('1', 3)
        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]
        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])
        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]
        """
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        memory = self._memory_limit()
        serializer = self._jrdd_deserializer

        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))

        if numPartitions == 1:
            if self.getNumPartitions() > 1:
                self = self.coalesce(1)
            return self.mapPartitions(sortPartition, True)

        # first compute the boundary of each part via sampling: we want to partition
        # the key-space into bins such that the bins have roughly the same
        # number of (key, value) pairs falling into them
        rddSize = self.count()
        if not rddSize:
            return self  # empty RDD
        maxSampleSize = numPartitions * 20.0  # constant from Spark's RangePartitioner
        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)
        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()
        samples = sorted(samples, key=keyfunc)

        # we have numPartitions many parts but one of the them has
        # an implicit boundary
        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]
                  for i in range(0, numPartitions - 1)]

        def rangePartitioner(k):
            p = bisect.bisect_left(bounds, keyfunc(k))
            if ascending:
                return p
            else:
                return numPartitions - 1 - p

        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)
--------------------------------------------------

--- SAMPLE #69 ---
DOCSTRING: Sorts this RDD by the given keyfunc

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()
        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
CODE:
def sortBy(self, keyfunc, ascending=True, numPartitions=None):
        """
        Sorts this RDD by the given keyfunc

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()
        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        """
        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()
--------------------------------------------------

--- SAMPLE #70 ---
DOCSTRING: Return the Cartesian product of this RDD and another one, that is, the
        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and
        C{b} is in C{other}.

        >>> rdd = sc.parallelize([1, 2])
        >>> sorted(rdd.cartesian(rdd).collect())
        [(1, 1), (1, 2), (2, 1), (2, 2)]
CODE:
def cartesian(self, other):
        """
        Return the Cartesian product of this RDD and another one, that is, the
        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and
        C{b} is in C{other}.

        >>> rdd = sc.parallelize([1, 2])
        >>> sorted(rdd.cartesian(rdd).collect())
        [(1, 1), (1, 2), (2, 1), (2, 2)]
        """
        # Due to batching, we can't use the Java cartesian method.
        deserializer = CartesianDeserializer(self._jrdd_deserializer,
                                             other._jrdd_deserializer)
        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)
--------------------------------------------------

--- SAMPLE #71 ---
DOCSTRING: Return an RDD of grouped items.

        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])
        >>> result = rdd.groupBy(lambda x: x % 2).collect()
        >>> sorted([(x, sorted(y)) for (x, y) in result])
        [(0, [2, 8]), (1, [1, 1, 3, 5])]
CODE:
def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash):
        """
        Return an RDD of grouped items.

        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])
        >>> result = rdd.groupBy(lambda x: x % 2).collect()
        >>> sorted([(x, sorted(y)) for (x, y) in result])
        [(0, [2, 8]), (1, [1, 1, 3, 5])]
        """
        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)
--------------------------------------------------

--- SAMPLE #72 ---
DOCSTRING: Return an RDD created by piping elements to a forked external process.

        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()
        [u'1', u'2', u'', u'3']

        :param checkCode: whether or not to check the return value of the shell command.
CODE:
def pipe(self, command, env=None, checkCode=False):
        """
        Return an RDD created by piping elements to a forked external process.

        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()
        [u'1', u'2', u'', u'3']

        :param checkCode: whether or not to check the return value of the shell command.
        """
        if env is None:
            env = dict()

        def func(iterator):
            pipe = Popen(
                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)

            def pipe_objs(out):
                for obj in iterator:
                    s = unicode(obj).rstrip('\n') + '\n'
                    out.write(s.encode('utf-8'))
                out.close()
            Thread(target=pipe_objs, args=[pipe.stdin]).start()

            def check_return_code():
                pipe.wait()
                if checkCode and pipe.returncode:
                    raise Exception("Pipe function `%s' exited "
                                    "with error code %d" % (command, pipe.returncode))
                else:
                    for i in range(0):
                        yield i
            return (x.rstrip(b'\n').decode('utf-8') for x in
                    chain(iter(pipe.stdout.readline, b''), check_return_code()))
        return self.mapPartitions(func)
--------------------------------------------------

--- SAMPLE #73 ---
DOCSTRING: Applies a function to all elements of this RDD.

        >>> def f(x): print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)
CODE:
def foreach(self, f):
        """
        Applies a function to all elements of this RDD.

        >>> def f(x): print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)
        """
        f = fail_on_stopiteration(f)

        def processPartition(iterator):
            for x in iterator:
                f(x)
            return iter([])
        self.mapPartitions(processPartition).count()
--------------------------------------------------

--- SAMPLE #74 ---
DOCSTRING: Applies a function to each partition of this RDD.

        >>> def f(iterator):
        ...     for x in iterator:
        ...          print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)
CODE:
def foreachPartition(self, f):
        """
        Applies a function to each partition of this RDD.

        >>> def f(iterator):
        ...     for x in iterator:
        ...          print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)
        """
        def func(it):
            r = f(it)
            try:
                return iter(r)
            except TypeError:
                return iter([])
        self.mapPartitions(func).count()
--------------------------------------------------

--- SAMPLE #75 ---
DOCSTRING: Return a list that contains all of the elements in this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.
CODE:
def collect(self):
        """
        Return a list that contains all of the elements in this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.
        """
        with SCCallSiteSync(self.context) as css:
            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
        return list(_load_from_socket(sock_info, self._jrdd_deserializer))
--------------------------------------------------

--- SAMPLE #76 ---
DOCSTRING: Reduces the elements of this RDD using the specified commutative and
        associative binary operator. Currently reduces partitions locally.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)
        15
        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)
        10
        >>> sc.parallelize([]).reduce(add)
        Traceback (most recent call last):
            ...
        ValueError: Can not reduce() empty RDD
CODE:
def reduce(self, f):
        """
        Reduces the elements of this RDD using the specified commutative and
        associative binary operator. Currently reduces partitions locally.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)
        15
        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)
        10
        >>> sc.parallelize([]).reduce(add)
        Traceback (most recent call last):
            ...
        ValueError: Can not reduce() empty RDD
        """
        f = fail_on_stopiteration(f)

        def func(iterator):
            iterator = iter(iterator)
            try:
                initial = next(iterator)
            except StopIteration:
                return
            yield reduce(f, iterator, initial)

        vals = self.mapPartitions(func).collect()
        if vals:
            return reduce(f, vals)
        raise ValueError("Can not reduce() empty RDD")
--------------------------------------------------

--- SAMPLE #77 ---
DOCSTRING: Reduces the elements of this RDD in a multi-level tree pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeReduce(add)
        -5
        >>> rdd.treeReduce(add, 1)
        -5
        >>> rdd.treeReduce(add, 2)
        -5
        >>> rdd.treeReduce(add, 5)
        -5
        >>> rdd.treeReduce(add, 10)
        -5
CODE:
def treeReduce(self, f, depth=2):
        """
        Reduces the elements of this RDD in a multi-level tree pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeReduce(add)
        -5
        >>> rdd.treeReduce(add, 1)
        -5
        >>> rdd.treeReduce(add, 2)
        -5
        >>> rdd.treeReduce(add, 5)
        -5
        >>> rdd.treeReduce(add, 10)
        -5
        """
        if depth < 1:
            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)

        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.

        def op(x, y):
            if x[1]:
                return y
            elif y[1]:
                return x
            else:
                return f(x[0], y[0]), False

        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)
        if reduced[1]:
            raise ValueError("Cannot reduce empty RDD.")
        return reduced[0]
--------------------------------------------------

--- SAMPLE #78 ---
DOCSTRING: Aggregate the elements of each partition, and then the results for all
        the partitions, using a given associative function and a neutral "zero value."

        The function C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        This behaves somewhat differently from fold operations implemented
        for non-distributed collections in functional languages like Scala.
        This fold operation may be applied to partitions individually, and then
        fold those results into the final result, rather than apply the fold
        to each element sequentially in some defined ordering. For functions
        that are not commutative, the result may differ from that of a fold
        applied to a non-distributed collection.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
        15
CODE:
def fold(self, zeroValue, op):
        """
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given associative function and a neutral "zero value."

        The function C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        This behaves somewhat differently from fold operations implemented
        for non-distributed collections in functional languages like Scala.
        This fold operation may be applied to partitions individually, and then
        fold those results into the final result, rather than apply the fold
        to each element sequentially in some defined ordering. For functions
        that are not commutative, the result may differ from that of a fold
        applied to a non-distributed collection.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
        15
        """
        op = fail_on_stopiteration(op)

        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = op(acc, obj)
            yield acc
        # collecting result of mapPartitions here ensures that the copy of
        # zeroValue provided to each partition is unique from the one provided
        # to the final reduce call
        vals = self.mapPartitions(func).collect()
        return reduce(op, vals, zeroValue)
--------------------------------------------------

--- SAMPLE #79 ---
DOCSTRING: Aggregate the elements of each partition, and then the results for all
        the partitions, using a given combine functions and a neutral "zero
        value."

        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        The first function (seqOp) can return a different result type, U, than
        the type of this RDD. Thus, we need one operation for merging a T into
        an U and one operation for merging two U

        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
        (10, 4)
        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
        (0, 0)
CODE:
def aggregate(self, zeroValue, seqOp, combOp):
        """
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given combine functions and a neutral "zero
        value."

        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        The first function (seqOp) can return a different result type, U, than
        the type of this RDD. Thus, we need one operation for merging a T into
        an U and one operation for merging two U

        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
        (10, 4)
        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
        (0, 0)
        """
        seqOp = fail_on_stopiteration(seqOp)
        combOp = fail_on_stopiteration(combOp)

        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        # collecting result of mapPartitions here ensures that the copy of
        # zeroValue provided to each partition is unique from the one provided
        # to the final reduce call
        vals = self.mapPartitions(func).collect()
        return reduce(combOp, vals, zeroValue)
--------------------------------------------------

--- SAMPLE #80 ---
DOCSTRING: Aggregates the elements of this RDD in a multi-level tree
        pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeAggregate(0, add, add)
        -5
        >>> rdd.treeAggregate(0, add, add, 1)
        -5
        >>> rdd.treeAggregate(0, add, add, 2)
        -5
        >>> rdd.treeAggregate(0, add, add, 5)
        -5
        >>> rdd.treeAggregate(0, add, add, 10)
        -5
CODE:
def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):
        """
        Aggregates the elements of this RDD in a multi-level tree
        pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeAggregate(0, add, add)
        -5
        >>> rdd.treeAggregate(0, add, add, 1)
        -5
        >>> rdd.treeAggregate(0, add, add, 2)
        -5
        >>> rdd.treeAggregate(0, add, add, 5)
        -5
        >>> rdd.treeAggregate(0, add, add, 10)
        -5
        """
        if depth < 1:
            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)

        if self.getNumPartitions() == 0:
            return zeroValue

        def aggregatePartition(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc

        partiallyAggregated = self.mapPartitions(aggregatePartition)
        numPartitions = partiallyAggregated.getNumPartitions()
        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)
        # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree
        # aggregation.
        while numPartitions > scale + numPartitions / scale:
            numPartitions /= scale
            curNumPartitions = int(numPartitions)

            def mapPartition(i, iterator):
                for obj in iterator:
                    yield (i % curNumPartitions, obj)

            partiallyAggregated = partiallyAggregated \
                .mapPartitionsWithIndex(mapPartition) \
                .reduceByKey(combOp, curNumPartitions) \
                .values()

        return partiallyAggregated.reduce(combOp)
--------------------------------------------------

--- SAMPLE #81 ---
DOCSTRING: Find the maximum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])
        >>> rdd.max()
        43.0
        >>> rdd.max(key=str)
        5.0
CODE:
def max(self, key=None):
        """
        Find the maximum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])
        >>> rdd.max()
        43.0
        >>> rdd.max(key=str)
        5.0
        """
        if key is None:
            return self.reduce(max)
        return self.reduce(lambda a, b: max(a, b, key=key))
--------------------------------------------------

--- SAMPLE #82 ---
DOCSTRING: Find the minimum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])
        >>> rdd.min()
        2.0
        >>> rdd.min(key=str)
        10.0
CODE:
def min(self, key=None):
        """
        Find the minimum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])
        >>> rdd.min()
        2.0
        >>> rdd.min(key=str)
        10.0
        """
        if key is None:
            return self.reduce(min)
        return self.reduce(lambda a, b: min(a, b, key=key))
--------------------------------------------------

--- SAMPLE #83 ---
DOCSTRING: Add up the elements in this RDD.

        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()
        6.0
CODE:
def sum(self):
        """
        Add up the elements in this RDD.

        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()
        6.0
        """
        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)
--------------------------------------------------

--- SAMPLE #84 ---
DOCSTRING: Return a L{StatCounter} object that captures the mean, variance
        and count of the RDD's elements in one operation.
CODE:
def stats(self):
        """
        Return a L{StatCounter} object that captures the mean, variance
        and count of the RDD's elements in one operation.
        """
        def redFunc(left_counter, right_counter):
            return left_counter.mergeStats(right_counter)

        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)
--------------------------------------------------

--- SAMPLE #85 ---
DOCSTRING: Compute a histogram using the provided buckets. The buckets
        are all open to the right except for the last which is closed.
        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],
        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1
        and 50 we would have a histogram of 1,0,1.

        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),
        this can be switched from an O(log n) inseration to O(1) per
        element (where n is the number of buckets).

        Buckets must be sorted, not contain any duplicates, and have
        at least two elements.

        If `buckets` is a number, it will generate buckets which are
        evenly spaced between the minimum and maximum of the RDD. For
        example, if the min value is 0 and the max is 100, given `buckets`
        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must
        be at least 1. An exception is raised if the RDD contains infinity.
        If the elements in the RDD do not vary (max == min), a single bucket
        will be used.

        The return value is a tuple of buckets and histogram.

        >>> rdd = sc.parallelize(range(51))
        >>> rdd.histogram(2)
        ([0, 25, 50], [25, 26])
        >>> rdd.histogram([0, 5, 25, 50])
        ([0, 5, 25, 50], [5, 20, 26])
        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets
        ([0, 15, 30, 45, 60], [15, 15, 15, 6])
        >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])
        >>> rdd.histogram(("a", "b", "c"))
        (('a', 'b', 'c'), [2, 2])
CODE:
def histogram(self, buckets):
        """
        Compute a histogram using the provided buckets. The buckets
        are all open to the right except for the last which is closed.
        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],
        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1
        and 50 we would have a histogram of 1,0,1.

        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),
        this can be switched from an O(log n) inseration to O(1) per
        element (where n is the number of buckets).

        Buckets must be sorted, not contain any duplicates, and have
        at least two elements.

        If `buckets` is a number, it will generate buckets which are
        evenly spaced between the minimum and maximum of the RDD. For
        example, if the min value is 0 and the max is 100, given `buckets`
        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must
        be at least 1. An exception is raised if the RDD contains infinity.
        If the elements in the RDD do not vary (max == min), a single bucket
        will be used.

        The return value is a tuple of buckets and histogram.

        >>> rdd = sc.parallelize(range(51))
        >>> rdd.histogram(2)
        ([0, 25, 50], [25, 26])
        >>> rdd.histogram([0, 5, 25, 50])
        ([0, 5, 25, 50], [5, 20, 26])
        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets
        ([0, 15, 30, 45, 60], [15, 15, 15, 6])
        >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])
        >>> rdd.histogram(("a", "b", "c"))
        (('a', 'b', 'c'), [2, 2])
        """

        if isinstance(buckets, int):
            if buckets < 1:
                raise ValueError("number of buckets must be >= 1")

            # filter out non-comparable elements
            def comparable(x):
                if x is None:
                    return False
                if type(x) is float and isnan(x):
                    return False
                return True

            filtered = self.filter(comparable)

            # faster than stats()
            def minmax(a, b):
                return min(a[0], b[0]), max(a[1], b[1])
            try:
                minv, maxv = filtered.map(lambda x: (x, x)).reduce(minmax)
            except TypeError as e:
                if " empty " in str(e):
                    raise ValueError("can not generate buckets from empty RDD")
                raise

            if minv == maxv or buckets == 1:
                return [minv, maxv], [filtered.count()]

            try:
                inc = (maxv - minv) / buckets
            except TypeError:
                raise TypeError("Can not generate buckets with non-number in RDD")

            if isinf(inc):
                raise ValueError("Can not generate buckets with infinite value")

            # keep them as integer if possible
            inc = int(inc)
            if inc * buckets != maxv - minv:
                inc = (maxv - minv) * 1.0 / buckets

            buckets = [i * inc + minv for i in range(buckets)]
            buckets.append(maxv)  # fix accumulated error
            even = True

        elif isinstance(buckets, (list, tuple)):
            if len(buckets) < 2:
                raise ValueError("buckets should have more than one value")

            if any(i is None or isinstance(i, float) and isnan(i) for i in buckets):
                raise ValueError("can not have None or NaN in buckets")

            if sorted(buckets) != list(buckets):
                raise ValueError("buckets should be sorted")

            if len(set(buckets)) != len(buckets):
                raise ValueError("buckets should not contain duplicated values")

            minv = buckets[0]
            maxv = buckets[-1]
            even = False
            inc = None
            try:
                steps = [buckets[i + 1] - buckets[i] for i in range(len(buckets) - 1)]
            except TypeError:
                pass  # objects in buckets do not support '-'
            else:
                if max(steps) - min(steps) < 1e-10:  # handle precision errors
                    even = True
                    inc = (maxv - minv) / (len(buckets) - 1)

        else:
            raise TypeError("buckets should be a list or tuple or number(int or long)")

        def histogram(iterator):
            counters = [0] * len(buckets)
            for i in iterator:
                if i is None or (type(i) is float and isnan(i)) or i > maxv or i < minv:
                    continue
                t = (int((i - minv) / inc) if even
                     else bisect.bisect_right(buckets, i) - 1)
                counters[t] += 1
            # add last two together
            last = counters.pop()
            counters[-1] += last
            return [counters]

        def mergeCounters(a, b):
            return [i + j for i, j in zip(a, b)]

        return buckets, self.mapPartitions(histogram).reduce(mergeCounters)
--------------------------------------------------

--- SAMPLE #86 ---
DOCSTRING: Return the count of each unique value in this RDD as a dictionary of
        (value, count) pairs.

        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
        [(1, 2), (2, 3)]
CODE:
def countByValue(self):
        """
        Return the count of each unique value in this RDD as a dictionary of
        (value, count) pairs.

        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
        [(1, 2), (2, 3)]
        """
        def countPartition(iterator):
            counts = defaultdict(int)
            for obj in iterator:
                counts[obj] += 1
            yield counts

        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] += v
            return m1
        return self.mapPartitions(countPartition).reduce(mergeMaps)
--------------------------------------------------

--- SAMPLE #87 ---
DOCSTRING: Get the top N elements from an RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        .. note:: It returns the list sorted in descending order.

        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
        [12]
        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
        [6, 5]
        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
        [4, 3, 2]
CODE:
def top(self, num, key=None):
        """
        Get the top N elements from an RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        .. note:: It returns the list sorted in descending order.

        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
        [12]
        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
        [6, 5]
        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
        [4, 3, 2]
        """
        def topIterator(iterator):
            yield heapq.nlargest(num, iterator, key=key)

        def merge(a, b):
            return heapq.nlargest(num, a + b, key=key)

        return self.mapPartitions(topIterator).reduce(merge)
--------------------------------------------------

--- SAMPLE #88 ---
DOCSTRING: Get the N elements from an RDD ordered in ascending order or as
        specified by the optional key function.

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
        [1, 2, 3, 4, 5, 6]
        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
        [10, 9, 7, 6, 5, 4]
CODE:
def takeOrdered(self, num, key=None):
        """
        Get the N elements from an RDD ordered in ascending order or as
        specified by the optional key function.

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
        [1, 2, 3, 4, 5, 6]
        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
        [10, 9, 7, 6, 5, 4]
        """

        def merge(a, b):
            return heapq.nsmallest(num, a + b, key)

        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
--------------------------------------------------

--- SAMPLE #89 ---
DOCSTRING: Take the first num elements of the RDD.

        It works by first scanning one partition, and use the results from
        that partition to estimate the number of additional partitions needed
        to satisfy the limit.

        Translated from the Scala implementation in RDD#take().

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
        [2, 3]
        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)
        [2, 3, 4, 5, 6]
        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
        [91, 92, 93]
CODE:
def take(self, num):
        """
        Take the first num elements of the RDD.

        It works by first scanning one partition, and use the results from
        that partition to estimate the number of additional partitions needed
        to satisfy the limit.

        Translated from the Scala implementation in RDD#take().

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
        [2, 3]
        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)
        [2, 3, 4, 5, 6]
        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
        [91, 92, 93]
        """
        items = []
        totalParts = self.getNumPartitions()
        partsScanned = 0

        while len(items) < num and partsScanned < totalParts:
            # The number of partitions to try in this iteration.
            # It is ok for this number to be greater than totalParts because
            # we actually cap it at totalParts in runJob.
            numPartsToTry = 1
            if partsScanned > 0:
                # If we didn't find any rows after the previous iteration,
                # quadruple and retry.  Otherwise, interpolate the number of
                # partitions we need to try, but overestimate it by 50%.
                # We also cap the estimation in the end.
                if len(items) == 0:
                    numPartsToTry = partsScanned * 4
                else:
                    # the first parameter of max is >=1 whenever partsScanned >= 2
                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned
                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)

            left = num - len(items)

            def takeUpToNumLeft(iterator):
                iterator = iter(iterator)
                taken = 0
                while taken < left:
                    try:
                        yield next(iterator)
                    except StopIteration:
                        return
                    taken += 1

            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
            res = self.context.runJob(self, takeUpToNumLeft, p)

            items += res
            partsScanned += numPartsToTry

        return items[:num]
--------------------------------------------------

--- SAMPLE #90 ---
DOCSTRING: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are
        converted for output using either user specified converters or, by default,
        L{org.apache.spark.api.python.JavaToWritableConverter}.

        :param conf: Hadoop job configuration, passed in as a dict
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
CODE:
def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):
        """
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are
        converted for output using either user specified converters or, by default,
        L{org.apache.spark.api.python.JavaToWritableConverter}.

        :param conf: Hadoop job configuration, passed in as a dict
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        """
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,
                                                    keyConverter, valueConverter, True)
--------------------------------------------------

--- SAMPLE #91 ---
DOCSTRING: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types
        will be inferred if not specified. Keys and values are converted for output using either
        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The
        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext
        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.

        :param path: path to Hadoop file
        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat
               (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")
        :param keyClass: fully qualified classname of key Writable class
               (e.g. "org.apache.hadoop.io.IntWritable", None by default)
        :param valueClass: fully qualified classname of value Writable class
               (e.g. "org.apache.hadoop.io.Text", None by default)
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        :param conf: Hadoop job configuration, passed in as a dict (None by default)
CODE:
def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,
                               keyConverter=None, valueConverter=None, conf=None):
        """
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types
        will be inferred if not specified. Keys and values are converted for output using either
        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The
        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext
        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.

        :param path: path to Hadoop file
        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat
               (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")
        :param keyClass: fully qualified classname of key Writable class
               (e.g. "org.apache.hadoop.io.IntWritable", None by default)
        :param valueClass: fully qualified classname of value Writable class
               (e.g. "org.apache.hadoop.io.Text", None by default)
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        :param conf: Hadoop job configuration, passed in as a dict (None by default)
        """
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,
                                                       outputFormatClass,
                                                       keyClass, valueClass,
                                                       keyConverter, valueConverter, jconf)
--------------------------------------------------

--- SAMPLE #92 ---
DOCSTRING: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the
        RDD's key and value types. The mechanism is as follows:

            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.
            2. Keys and values of this Java RDD are converted to Writables and written out.

        :param path: path to sequence file
        :param compressionCodecClass: (None by default)
CODE:
def saveAsSequenceFile(self, path, compressionCodecClass=None):
        """
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the
        RDD's key and value types. The mechanism is as follows:

            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.
            2. Keys and values of this Java RDD are converted to Writables and written out.

        :param path: path to sequence file
        :param compressionCodecClass: (None by default)
        """
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,
                                                   path, compressionCodecClass)
--------------------------------------------------

--- SAMPLE #93 ---
DOCSTRING: Save this RDD as a SequenceFile of serialized objects. The serializer
        used is L{pyspark.serializers.PickleSerializer}, default batch size
        is 10.

        >>> tmpFile = NamedTemporaryFile(delete=True)
        >>> tmpFile.close()
        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)
        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())
        ['1', '2', 'rdd', 'spark']
CODE:
def saveAsPickleFile(self, path, batchSize=10):
        """
        Save this RDD as a SequenceFile of serialized objects. The serializer
        used is L{pyspark.serializers.PickleSerializer}, default batch size
        is 10.

        >>> tmpFile = NamedTemporaryFile(delete=True)
        >>> tmpFile.close()
        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)
        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())
        ['1', '2', 'rdd', 'spark']
        """
        if batchSize == 0:
            ser = AutoBatchedSerializer(PickleSerializer())
        else:
            ser = BatchedSerializer(PickleSerializer(), batchSize)
        self._reserialize(ser)._jrdd.saveAsObjectFile(path)
--------------------------------------------------

--- SAMPLE #94 ---
DOCSTRING: Save this RDD as a text file, using string representations of elements.

        @param path: path to text file
        @param compressionCodecClass: (None by default) string i.e.
            "org.apache.hadoop.io.compress.GzipCodec"

        >>> tempFile = NamedTemporaryFile(delete=True)
        >>> tempFile.close()
        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)
        >>> from fileinput import input
        >>> from glob import glob
        >>> ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))
        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'

        Empty lines are tolerated when saving to text files.

        >>> tempFile2 = NamedTemporaryFile(delete=True)
        >>> tempFile2.close()
        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)
        >>> ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))
        '\\n\\n\\nbar\\nfoo\\n'

        Using compressionCodecClass

        >>> tempFile3 = NamedTemporaryFile(delete=True)
        >>> tempFile3.close()
        >>> codec = "org.apache.hadoop.io.compress.GzipCodec"
        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)
        >>> from fileinput import input, hook_compressed
        >>> result = sorted(input(glob(tempFile3.name + "/part*.gz"), openhook=hook_compressed))
        >>> b''.join(result).decode('utf-8')
        u'bar\\nfoo\\n'
CODE:
def saveAsTextFile(self, path, compressionCodecClass=None):
        """
        Save this RDD as a text file, using string representations of elements.

        @param path: path to text file
        @param compressionCodecClass: (None by default) string i.e.
            "org.apache.hadoop.io.compress.GzipCodec"

        >>> tempFile = NamedTemporaryFile(delete=True)
        >>> tempFile.close()
        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)
        >>> from fileinput import input
        >>> from glob import glob
        >>> ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))
        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'

        Empty lines are tolerated when saving to text files.

        >>> tempFile2 = NamedTemporaryFile(delete=True)
        >>> tempFile2.close()
        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)
        >>> ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))
        '\\n\\n\\nbar\\nfoo\\n'

        Using compressionCodecClass

        >>> tempFile3 = NamedTemporaryFile(delete=True)
        >>> tempFile3.close()
        >>> codec = "org.apache.hadoop.io.compress.GzipCodec"
        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)
        >>> from fileinput import input, hook_compressed
        >>> result = sorted(input(glob(tempFile3.name + "/part*.gz"), openhook=hook_compressed))
        >>> b''.join(result).decode('utf-8')
        u'bar\\nfoo\\n'
        """
        def func(split, iterator):
            for x in iterator:
                if not isinstance(x, (unicode, bytes)):
                    x = unicode(x)
                if isinstance(x, unicode):
                    x = x.encode("utf-8")
                yield x
        keyed = self.mapPartitionsWithIndex(func)
        keyed._bypass_serializer = True
        if compressionCodecClass:
            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)
        else:
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)
--------------------------------------------------

--- SAMPLE #95 ---
DOCSTRING: Merge the values for each key using an associative and commutative reduce function.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a "combiner" in MapReduce.

        Output will be partitioned with C{numPartitions} partitions, or
        the default parallelism level if C{numPartitions} is not specified.
        Default partitioner is hash-partition.

        >>> from operator import add
        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> sorted(rdd.reduceByKey(add).collect())
        [('a', 2), ('b', 1)]
CODE:
def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash):
        """
        Merge the values for each key using an associative and commutative reduce function.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a "combiner" in MapReduce.

        Output will be partitioned with C{numPartitions} partitions, or
        the default parallelism level if C{numPartitions} is not specified.
        Default partitioner is hash-partition.

        >>> from operator import add
        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> sorted(rdd.reduceByKey(add).collect())
        [('a', 2), ('b', 1)]
        """
        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)
--------------------------------------------------

--- SAMPLE #96 ---
DOCSTRING: Merge the values for each key using an associative and commutative reduce function, but
        return the results immediately to the master as a dictionary.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a "combiner" in MapReduce.

        >>> from operator import add
        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> sorted(rdd.reduceByKeyLocally(add).items())
        [('a', 2), ('b', 1)]
CODE:
def reduceByKeyLocally(self, func):
        """
        Merge the values for each key using an associative and commutative reduce function, but
        return the results immediately to the master as a dictionary.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a "combiner" in MapReduce.

        >>> from operator import add
        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> sorted(rdd.reduceByKeyLocally(add).items())
        [('a', 2), ('b', 1)]
        """
        func = fail_on_stopiteration(func)

        def reducePartition(iterator):
            m = {}
            for k, v in iterator:
                m[k] = func(m[k], v) if k in m else v
            yield m

        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] = func(m1[k], v) if k in m1 else v
            return m1
        return self.mapPartitions(reducePartition).reduce(mergeMaps)
--------------------------------------------------

--- SAMPLE #97 ---
DOCSTRING: Return a copy of the RDD partitioned using the specified partitioner.

        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))
        >>> sets = pairs.partitionBy(2).glom().collect()
        >>> len(set(sets[0]).intersection(set(sets[1])))
        0
CODE:
def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        """
        Return a copy of the RDD partitioned using the specified partitioner.

        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))
        >>> sets = pairs.partitionBy(2).glom().collect()
        >>> len(set(sets[0]).intersection(set(sets[1])))
        0
        """
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        partitioner = Partitioner(numPartitions, partitionFunc)
        if self.partitioner == partitioner:
            return self

        # Transferring O(n) objects to Java is too expensive.
        # Instead, we'll form the hash buckets in Python,
        # transferring O(numPartitions) objects to Java.
        # Each object is a (splitNumber, [objects]) pair.
        # In order to avoid too huge objects, the objects are
        # grouped into chunks.
        outputSerializer = self.ctx._unbatched_serializer

        limit = (_parse_memory(self.ctx._conf.get(
            "spark.python.worker.memory", "512m")) / 2)

        def add_shuffle_key(split, iterator):

            buckets = defaultdict(list)
            c, batch = 0, min(10 * numPartitions, 1000)

            for k, v in iterator:
                buckets[partitionFunc(k) % numPartitions].append((k, v))
                c += 1

                # check used memory and avg size of chunk of objects
                if (c % 1000 == 0 and get_used_memory() > limit
                        or c > batch):
                    n, size = len(buckets), 0
                    for split in list(buckets.keys()):
                        yield pack_long(split)
                        d = outputSerializer.dumps(buckets[split])
                        del buckets[split]
                        yield d
                        size += len(d)

                    avg = int(size / n) >> 20
                    # let 1M < avg < 10M
                    if avg < 1:
                        batch *= 1.5
                    elif avg > 10:
                        batch = max(int(batch / 1.5), 1)
                    c = 0

            for split, items in buckets.items():
                yield pack_long(split)
                yield outputSerializer.dumps(items)

        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)
        keyed._bypass_serializer = True
        with SCCallSiteSync(self.context) as css:
            pairRDD = self.ctx._jvm.PairwiseRDD(
                keyed._jrdd.rdd()).asJavaPairRDD()
            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,
                                                           id(partitionFunc))
        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))
        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))
        rdd.partitioner = partitioner
        return rdd
--------------------------------------------------

--- SAMPLE #98 ---
DOCSTRING: Generic function to combine the elements for each key using a custom
        set of aggregation functions.

        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined
        type" C.

        Users provide three functions:

            - C{createCombiner}, which turns a V into a C (e.g., creates
              a one-element list)
            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of
              a list)
            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges
              the lists)

        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to
        modify and return their first argument instead of creating a new C.

        In addition, users can control the partitioning of the output RDD.

        .. note:: V and C can be different -- for example, one might group an RDD of type
            (Int, Int) into an RDD of type (Int, List[Int]).

        >>> x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)])
        >>> def to_list(a):
        ...     return [a]
        ...
        >>> def append(a, b):
        ...     a.append(b)
        ...     return a
        ...
        >>> def extend(a, b):
        ...     a.extend(b)
        ...     return a
        ...
        >>> sorted(x.combineByKey(to_list, append, extend).collect())
        [('a', [1, 2]), ('b', [1])]
CODE:
def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None, partitionFunc=portable_hash):
        """
        Generic function to combine the elements for each key using a custom
        set of aggregation functions.

        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined
        type" C.

        Users provide three functions:

            - C{createCombiner}, which turns a V into a C (e.g., creates
              a one-element list)
            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of
              a list)
            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges
              the lists)

        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to
        modify and return their first argument instead of creating a new C.

        In addition, users can control the partitioning of the output RDD.

        .. note:: V and C can be different -- for example, one might group an RDD of type
            (Int, Int) into an RDD of type (Int, List[Int]).

        >>> x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)])
        >>> def to_list(a):
        ...     return [a]
        ...
        >>> def append(a, b):
        ...     a.append(b)
        ...     return a
        ...
        >>> def extend(a, b):
        ...     a.extend(b)
        ...     return a
        ...
        >>> sorted(x.combineByKey(to_list, append, extend).collect())
        [('a', [1, 2]), ('b', [1])]
        """
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        serializer = self.ctx.serializer
        memory = self._memory_limit()
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)

        def combineLocally(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()

        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)

        def _mergeCombiners(iterator):
            merger = ExternalMerger(agg, memory, serializer)
            merger.mergeCombiners(iterator)
            return merger.items()

        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)
--------------------------------------------------

--- SAMPLE #99 ---
DOCSTRING: Aggregate the values of each key, using given combine functions and a neutral
        "zero value". This function can return a different result type, U, than the type
        of the values in this RDD, V. Thus, we need one operation for merging a V into
        a U and one operation for merging two U's, The former operation is used for merging
        values within a partition, and the latter is used for merging values between
        partitions. To avoid memory allocation, both of these functions are
        allowed to modify and return their first argument instead of creating a new U.
CODE:
def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,
                       partitionFunc=portable_hash):
        """
        Aggregate the values of each key, using given combine functions and a neutral
        "zero value". This function can return a different result type, U, than the type
        of the values in this RDD, V. Thus, we need one operation for merging a V into
        a U and one operation for merging two U's, The former operation is used for merging
        values within a partition, and the latter is used for merging values between
        partitions. To avoid memory allocation, both of these functions are
        allowed to modify and return their first argument instead of creating a new U.
        """
        def createZero():
            return copy.deepcopy(zeroValue)

        return self.combineByKey(
            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)
--------------------------------------------------

--- SAMPLE #100 ---
DOCSTRING: Merge the values for each key using an associative function "func"
        and a neutral "zeroValue" which may be added to the result an
        arbitrary number of times, and must not change the result
        (e.g., 0 for addition, or 1 for multiplication.).

        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> from operator import add
        >>> sorted(rdd.foldByKey(0, add).collect())
        [('a', 2), ('b', 1)]
CODE:
def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):
        """
        Merge the values for each key using an associative function "func"
        and a neutral "zeroValue" which may be added to the result an
        arbitrary number of times, and must not change the result
        (e.g., 0 for addition, or 1 for multiplication.).

        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> from operator import add
        >>> sorted(rdd.foldByKey(0, add).collect())
        [('a', 2), ('b', 1)]
        """
        def createZero():
            return copy.deepcopy(zeroValue)

        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,
                                 partitionFunc)
--------------------------------------------------



================================================================================

--- FILE: data_preview_processed.txt | PATH: Project\Datasets\Human_readable_sample\data_preview_processed.txt ---
------------------------------------------------------------------------------------------------------------------

=== DATASET PREVIEW ===
Source File: Project/Datasets/processed\train.jsonl.gz

--- SAMPLE #1 ---
DOCSTRING: Trains a k-nearest neighbors classifier for face recognition.
CODE:
def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):
    X = []
    y = []
    for class_dir in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_dir)):
            continue
        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):
            image = face_recognition.load_image_file(img_path)
            face_bounding_boxes = face_recognition.face_locations(image)
            if len(face_bounding_boxes) != 1:
                if verbose:
                    print("Image {} not suitable for training: {}".format(img_path, "Didn't find a face" if len(face_bounding_boxes) < 1 else "Found more than one face"))
            else:
                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])
                y.append(class_dir)
    if n_neighbors is None:
        n_neighbors = int(round(math.sqrt(len(X))))
        if verbose:
            print("Chose n_neighbors automatically:", n_neighbors)
    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')
    knn_clf.fit(X, y)
    if model_save_path is not None:
        with open(model_save_path, 'wb') as f:
            pickle.dump(knn_clf, f)
    return knn_clf
--------------------------------------------------

--- SAMPLE #2 ---
DOCSTRING: Recognizes faces in given image using a trained KNN classifier
CODE:
def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):
    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:
        raise Exception("Invalid image path: {}".format(X_img_path))
    if knn_clf is None and model_path is None:
        raise Exception("Must supply knn classifier either thourgh knn_clf or model_path")
    if knn_clf is None:
        with open(model_path, 'rb') as f:
            knn_clf = pickle.load(f)
    X_img = face_recognition.load_image_file(X_img_path)
    X_face_locations = face_recognition.face_locations(X_img)
    if len(X_face_locations) == 0:
        return []
    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)
    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)
    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]
    return [(pred, loc) if rec else ("unknown", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]
--------------------------------------------------

--- SAMPLE #3 ---
DOCSTRING: Shows the face recognition results visually.
CODE:
def show_prediction_labels_on_image(img_path, predictions):
    pil_image = Image.open(img_path).convert("RGB")
    draw = ImageDraw.Draw(pil_image)
    for name, (top, right, bottom, left) in predictions:
        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))
        name = name.encode("UTF-8")
        text_width, text_height = draw.textsize(name)
        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))
    del draw
    pil_image.show()
--------------------------------------------------

--- SAMPLE #4 ---
DOCSTRING: Returns an array of bounding boxes of human faces in a image
CODE:
def face_locations(img, number_of_times_to_upsample=1, model="hog"):
    if model == "cnn":
        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, "cnn")]
    else:
        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]
--------------------------------------------------

--- SAMPLE #5 ---
DOCSTRING: Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image
CODE:
def face_landmarks(face_image, face_locations=None, model="large"):
    landmarks = _raw_face_landmarks(face_image, face_locations, model)
    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]
    if model == 'large':
        return [{
            "chin": points[0:17],
            "left_eyebrow": points[17:22],
            "right_eyebrow": points[22:27],
            "nose_bridge": points[27:31],
            "nose_tip": points[31:36],
            "left_eye": points[36:42],
            "right_eye": points[42:48],
            "top_lip": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],
            "bottom_lip": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]
        } for points in landmarks_as_tuples]
    elif model == 'small':
        return [{
            "nose_tip": [points[4]],
            "left_eye": points[2:4],
            "right_eye": points[0:2],
        } for points in landmarks_as_tuples]
    else:
        raise ValueError("Invalid landmarks model type. Supported models are ['small', 'large'].")
--------------------------------------------------

--- SAMPLE #6 ---
DOCSTRING: Return the Catalyst datatype from the size of integers.
CODE:
def _int_size_to_type(size):
    if size <= 8:
        return ByteType
    if size <= 16:
        return ShortType
    if size <= 32:
        return IntegerType
    if size <= 64:
        return LongType
--------------------------------------------------

--- SAMPLE #7 ---
DOCSTRING: Infer the DataType from obj
CODE:
def _infer_type(obj):
    if obj is None:
        return NullType()
    if hasattr(obj, '__UDT__'):
        return obj.__UDT__
    dataType = _type_mappings.get(type(obj))
    if dataType is DecimalType:
        return DecimalType(38, 18)
    elif dataType is not None:
        return dataType()
    if isinstance(obj, dict):
        for key, value in obj.items():
            if key is not None and value is not None:
                return MapType(_infer_type(key), _infer_type(value), True)
        return MapType(NullType(), NullType(), True)
    elif isinstance(obj, list):
        for v in obj:
            if v is not None:
                return ArrayType(_infer_type(obj[0]), True)
        return ArrayType(NullType(), True)
    elif isinstance(obj, array):
        if obj.typecode in _array_type_mappings:
            return ArrayType(_array_type_mappings[obj.typecode](), False)
        else:
            raise TypeError("not supported type: array(%s)" % obj.typecode)
    else:
        try:
            return _infer_schema(obj)
        except TypeError:
            raise TypeError("not supported type: %s" % type(obj))
--------------------------------------------------

--- SAMPLE #8 ---
DOCSTRING: Infer the schema from dict/namedtuple/object
CODE:
def _infer_schema(row, names=None):
    if isinstance(row, dict):
        items = sorted(row.items())
    elif isinstance(row, (tuple, list)):
        if hasattr(row, "__fields__"):  
            items = zip(row.__fields__, tuple(row))
        elif hasattr(row, "_fields"):  
            items = zip(row._fields, tuple(row))
        else:
            if names is None:
                names = ['_%d' % i for i in range(1, len(row) + 1)]
            elif len(names) < len(row):
                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))
            items = zip(names, row)
    elif hasattr(row, "__dict__"):  
        items = sorted(row.__dict__.items())
    else:
        raise TypeError("Can not infer schema for type: %s" % type(row))
    fields = [StructField(k, _infer_type(v), True) for k, v in items]
    return StructType(fields)
--------------------------------------------------

--- SAMPLE #9 ---
DOCSTRING: Return whether there is NullType in `dt` or not
CODE:
def _has_nulltype(dt):
    if isinstance(dt, StructType):
        return any(_has_nulltype(f.dataType) for f in dt.fields)
    elif isinstance(dt, ArrayType):
        return _has_nulltype((dt.elementType))
    elif isinstance(dt, MapType):
        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)
    else:
        return isinstance(dt, NullType)
--------------------------------------------------

--- SAMPLE #10 ---
DOCSTRING: Create a converter to drop the names of fields in obj
CODE:
def _create_converter(dataType):
    if not _need_converter(dataType):
        return lambda x: x
    if isinstance(dataType, ArrayType):
        conv = _create_converter(dataType.elementType)
        return lambda row: [conv(v) for v in row]
    elif isinstance(dataType, MapType):
        kconv = _create_converter(dataType.keyType)
        vconv = _create_converter(dataType.valueType)
        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())
    elif isinstance(dataType, NullType):
        return lambda x: None
    elif not isinstance(dataType, StructType):
        return lambda x: x
    names = [f.name for f in dataType.fields]
    converters = [_create_converter(f.dataType) for f in dataType.fields]
    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)
    def convert_struct(obj):
        if obj is None:
            return
        if isinstance(obj, (tuple, list)):
            if convert_fields:
                return tuple(conv(v) for v, conv in zip(obj, converters))
            else:
                return tuple(obj)
        if isinstance(obj, dict):
            d = obj
        elif hasattr(obj, "__dict__"):  
            d = obj.__dict__
        else:
            raise TypeError("Unexpected obj type: %s" % type(obj))
        if convert_fields:
            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])
        else:
            return tuple([d.get(name) for name in names])
    return convert_struct
--------------------------------------------------

--- SAMPLE #11 ---
DOCSTRING: Convert Spark data type to pyarrow type
CODE:
def to_arrow_type(dt):
    import pyarrow as pa
    if type(dt) == BooleanType:
        arrow_type = pa.bool_()
    elif type(dt) == ByteType:
        arrow_type = pa.int8()
    elif type(dt) == ShortType:
        arrow_type = pa.int16()
    elif type(dt) == IntegerType:
        arrow_type = pa.int32()
    elif type(dt) == LongType:
        arrow_type = pa.int64()
    elif type(dt) == FloatType:
        arrow_type = pa.float32()
    elif type(dt) == DoubleType:
        arrow_type = pa.float64()
    elif type(dt) == DecimalType:
        arrow_type = pa.decimal128(dt.precision, dt.scale)
    elif type(dt) == StringType:
        arrow_type = pa.string()
    elif type(dt) == BinaryType:
        arrow_type = pa.binary()
    elif type(dt) == DateType:
        arrow_type = pa.date32()
    elif type(dt) == TimestampType:
        arrow_type = pa.timestamp('us', tz='UTC')
    elif type(dt) == ArrayType:
        if type(dt.elementType) in [StructType, TimestampType]:
            raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
        arrow_type = pa.list_(to_arrow_type(dt.elementType))
    elif type(dt) == StructType:
        if any(type(field.dataType) == StructType for field in dt):
            raise TypeError("Nested StructType not supported in conversion to Arrow")
        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
                  for field in dt]
        arrow_type = pa.struct(fields)
    else:
        raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
    return arrow_type
--------------------------------------------------

--- SAMPLE #12 ---
DOCSTRING: Convert pyarrow type to Spark data type.
CODE:
def from_arrow_type(at):
    import pyarrow.types as types
    if types.is_boolean(at):
        spark_type = BooleanType()
    elif types.is_int8(at):
        spark_type = ByteType()
    elif types.is_int16(at):
        spark_type = ShortType()
    elif types.is_int32(at):
        spark_type = IntegerType()
    elif types.is_int64(at):
        spark_type = LongType()
    elif types.is_float32(at):
        spark_type = FloatType()
    elif types.is_float64(at):
        spark_type = DoubleType()
    elif types.is_decimal(at):
        spark_type = DecimalType(precision=at.precision, scale=at.scale)
    elif types.is_string(at):
        spark_type = StringType()
    elif types.is_binary(at):
        spark_type = BinaryType()
    elif types.is_date32(at):
        spark_type = DateType()
    elif types.is_timestamp(at):
        spark_type = TimestampType()
    elif types.is_list(at):
        if types.is_timestamp(at.value_type):
            raise TypeError("Unsupported type in conversion from Arrow: " + str(at))
        spark_type = ArrayType(from_arrow_type(at.value_type))
    elif types.is_struct(at):
        if any(types.is_struct(field.type) for field in at):
            raise TypeError("Nested StructType not supported in conversion from Arrow: " + str(at))
        return StructType(
            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
             for field in at])
    else:
        raise TypeError("Unsupported type in conversion from Arrow: " + str(at))
    return spark_type
--------------------------------------------------

--- SAMPLE #13 ---
DOCSTRING: Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.
CODE:
def _check_series_localize_timestamps(s, timezone):
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()
    from pandas.api.types import is_datetime64tz_dtype
    tz = timezone or _get_local_timezone()
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(tz).dt.tz_localize(None)
    else:
        return s
--------------------------------------------------

--- SAMPLE #14 ---
DOCSTRING: Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for Spark internal storage
CODE:
def _check_series_convert_timestamps_internal(s, timezone):
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()
    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype
    if is_datetime64_dtype(s.dtype):
        tz = timezone or _get_local_timezone()
        return s.dt.tz_localize(tz, ambiguous=False).dt.tz_convert('UTC')
    elif is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert('UTC')
    else:
        return s
--------------------------------------------------

--- SAMPLE #15 ---
DOCSTRING: Convert timestamp to timezone-naive in the specified timezone or local timezone
CODE:
def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()
    import pandas as pd
    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype
    from_tz = from_timezone or _get_local_timezone()
    to_tz = to_timezone or _get_local_timezone()
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(to_tz).dt.tz_localize(None)
    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:
        return s.apply(
            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)
            if ts is not pd.NaT else pd.NaT)
    else:
        return s
--------------------------------------------------

--- SAMPLE #16 ---
DOCSTRING: Construct a StructType by adding new elements to it to define the schema. The method accepts either:
CODE:
def add(self, field, data_type=None, nullable=True, metadata=None):
        if isinstance(field, StructField):
            self.fields.append(field)
            self.names.append(field.name)
        else:
            if isinstance(field, str) and data_type is None:
                raise ValueError("Must specify DataType if passing name of struct_field to create.")
            if isinstance(data_type, str):
                data_type_f = _parse_datatype_json_value(data_type)
            else:
                data_type_f = data_type
            self.fields.append(StructField(field, data_type_f, nullable, metadata))
            self.names.append(field)
        self._needConversion = [f.needConversion() for f in self]
        self._needSerializeAnyField = any(self._needConversion)
        return self
--------------------------------------------------

--- SAMPLE #17 ---
DOCSTRING: Return as an dict
CODE:
def asDict(self, recursive=False):
        if not hasattr(self, "__fields__"):
            raise TypeError("Cannot convert a Row class into dict")
        if recursive:
            def conv(obj):
                if isinstance(obj, Row):
                    return obj.asDict(True)
                elif isinstance(obj, list):
                    return [conv(o) for o in obj]
                elif isinstance(obj, dict):
                    return dict((k, conv(v)) for k, v in obj.items())
                else:
                    return obj
            return dict(zip(self.__fields__, (conv(o) for o in self)))
        else:
            return dict(zip(self.__fields__, self))
--------------------------------------------------

--- SAMPLE #18 ---
DOCSTRING: Evaluates the model on a test dataset.
CODE:
def evaluate(self, dataset):
        if not isinstance(dataset, DataFrame):
            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))
        java_lr_summary = self._call_java("evaluate", dataset)
        return LinearRegressionSummary(java_lr_summary)
--------------------------------------------------

--- SAMPLE #19 ---
DOCSTRING: Evaluates the model on a test dataset.
CODE:
def evaluate(self, dataset):
        if not isinstance(dataset, DataFrame):
            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))
        java_glr_summary = self._call_java("evaluate", dataset)
        return GeneralizedLinearRegressionSummary(java_glr_summary)
--------------------------------------------------

--- SAMPLE #20 ---
DOCSTRING: Get all the directories
CODE:
def _get_local_dirs(sub):
    path = os.environ.get("SPARK_LOCAL_DIRS", "/tmp")
    dirs = path.split(",")
    if len(dirs) > 1:
        rnd = random.Random(os.getpid() + id(dirs))
        random.shuffle(dirs, rnd.random)
    return [os.path.join(d, "python", str(os.getpid()), sub) for d in dirs]
--------------------------------------------------

--- SAMPLE #21 ---
DOCSTRING: Combine the items by creator and combiner
CODE:
def mergeValues(self, iterator):
        creator, comb = self.agg.createCombiner, self.agg.mergeValue
        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch
        limit = self.memory_limit
        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else creator(v)
            c += 1
            if c >= batch:
                if get_used_memory() >= limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5
        if get_used_memory() >= limit:
            self._spill()
--------------------------------------------------

--- SAMPLE #22 ---
DOCSTRING: Merge (K,V) pair by mergeCombiner
CODE:
def mergeCombiners(self, iterator, limit=None):
        if limit is None:
            limit = self.memory_limit
        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size
        c, data, pdata, batch = 0, self.data, self.pdata, self.batch
        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else v
            if not limit:
                continue
            c += objsize(v)
            if c > batch:
                if get_used_memory() > limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5
        if limit and get_used_memory() >= limit:
            self._spill()
--------------------------------------------------

--- SAMPLE #23 ---
DOCSTRING: dump already partitioned data into disks.
CODE:
def _spill(self):
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)
        used_memory = get_used_memory()
        if not self.pdata:
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]
            for k, v in self.data.items():
                h = self._partition(k)
                self.serializer.dump_stream([(k, v)], streams[h])
            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()
            self.data.clear()
            self.pdata.extend([{} for i in range(self.partitions)])
        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, "wb") as f:
                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)
        self.spills += 1
        gc.collect()  
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #24 ---
DOCSTRING: Return all partitioned items as iterator
CODE:
def _external_items(self):
        assert not self.data
        if any(self.pdata):
            self._spill()
        self.pdata = []
        try:
            for i in range(self.partitions):
                for v in self._merged_items(i):
                    yield v
                self.data.clear()
                for j in range(self.spills):
                    path = self._get_spill_dir(j)
                    os.remove(os.path.join(path, str(i)))
        finally:
            self._cleanup()
--------------------------------------------------

--- SAMPLE #25 ---
DOCSTRING: merge the partitioned items and return the as iterator
CODE:
def _recursive_merged_items(self, index):
        subdirs = [os.path.join(d, "parts", str(index)) for d in self.localdirs]
        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,
                           self.scale * self.partitions, self.partitions, self.batch)
        m.pdata = [{} for _ in range(self.partitions)]
        limit = self._next_limit()
        for j in range(self.spills):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb') as f:
                m.mergeCombiners(self.serializer.load_stream(f), 0)
            if get_used_memory() > limit:
                m._spill()
                limit = self._next_limit()
        return m._external_items()
--------------------------------------------------

--- SAMPLE #26 ---
DOCSTRING: Sort the elements in iterator, do external sort when the memory goes above the limit.
CODE:
def sorted(self, iterator, key=None, reverse=False):
        global MemoryBytesSpilled, DiskBytesSpilled
        batch, limit = 100, self._next_limit()
        chunks, current_chunk = [], []
        iterator = iter(iterator)
        while True:
            chunk = list(itertools.islice(iterator, batch))
            current_chunk.extend(chunk)
            if len(chunk) < batch:
                break
            used_memory = get_used_memory()
            if used_memory > limit:
                current_chunk.sort(key=key, reverse=reverse)
                path = self._get_path(len(chunks))
                with open(path, 'wb') as f:
                    self.serializer.dump_stream(current_chunk, f)
                def load(f):
                    for v in self.serializer.load_stream(f):
                        yield v
                    f.close()
                chunks.append(load(open(path, 'rb')))
                current_chunk = []
                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
                DiskBytesSpilled += os.path.getsize(path)
                os.unlink(path)  
            elif not chunks:
                batch = min(int(batch * 1.5), 10000)
        current_chunk.sort(key=key, reverse=reverse)
        if not chunks:
            return current_chunk
        if current_chunk:
            chunks.append(iter(current_chunk))
        return heapq.merge(chunks, key=key, reverse=reverse)
--------------------------------------------------

--- SAMPLE #27 ---
DOCSTRING: dump the values into disk
CODE:
def _spill(self):
        global MemoryBytesSpilled, DiskBytesSpilled
        if self._file is None:
            self._open_file()
        used_memory = get_used_memory()
        pos = self._file.tell()
        self._ser.dump_stream(self.values, self._file)
        self.values = []
        gc.collect()
        DiskBytesSpilled += self._file.tell() - pos
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #28 ---
DOCSTRING: dump already partitioned data into disks.
CODE:
def _spill(self):
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)
        used_memory = get_used_memory()
        if not self.pdata:
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]
            self._sorted = len(self.data) < self.SORT_KEY_LIMIT
            if self._sorted:
                self.serializer = self.flattened_serializer()
                for k in sorted(self.data.keys()):
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, self.data[k])], streams[h])
            else:
                for k, v in self.data.items():
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, v)], streams[h])
            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()
            self.data.clear()
            self.pdata.extend([{} for i in range(self.partitions)])
        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, "wb") as f:
                    if self._sorted:
                        sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))
                        self.serializer.dump_stream(sorted_items, f)
                    else:
                        self.serializer.dump_stream(self.pdata[i].items(), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)
        self.spills += 1
        gc.collect()  
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #29 ---
DOCSTRING: load a partition from disk, then sort and group by key
CODE:
def _merge_sorted_items(self, index):
        def load_partition(j):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb', 65536) as f:
                for v in self.serializer.load_stream(f):
                    yield v
        disk_items = [load_partition(j) for j in range(self.spills)]
        if self._sorted:
            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))
        else:
            ser = self.flattened_serializer()
            sorter = ExternalSorter(self.memory_limit, ser)
            sorted_items = sorter.sorted(itertools.chain(*disk_items),
                                         key=operator.itemgetter(0))
        return ((k, vs) for k, vs in GroupByKey(sorted_items))
--------------------------------------------------

--- SAMPLE #30 ---
DOCSTRING: Called by a worker process after the fork().
CODE:
def worker(sock, authenticated):
    signal.signal(SIGHUP, SIG_DFL)
    signal.signal(SIGCHLD, SIG_DFL)
    signal.signal(SIGTERM, SIG_DFL)
    signal.signal(SIGINT, signal.default_int_handler)
    infile = os.fdopen(os.dup(sock.fileno()), "rb", 65536)
    outfile = os.fdopen(os.dup(sock.fileno()), "wb", 65536)
    if not authenticated:
        client_secret = UTF8Deserializer().loads(infile)
        if os.environ["PYTHON_WORKER_FACTORY_SECRET"] == client_secret:
            write_with_length("ok".encode("utf-8"), outfile)
            outfile.flush()
        else:
            write_with_length("err".encode("utf-8"), outfile)
            outfile.flush()
            sock.close()
            return 1
    exit_code = 0
    try:
        worker_main(infile, outfile)
    except SystemExit as exc:
        exit_code = compute_real_exit_code(exc.code)
    finally:
        try:
            outfile.flush()
        except Exception:
            pass
    return exit_code
--------------------------------------------------

--- SAMPLE #31 ---
DOCSTRING: This function returns consistent hash code for builtin types, especially for None and tuple with None.
CODE:
def portable_hash(x):
    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:
        raise Exception("Randomness of hash of string should be disabled via PYTHONHASHSEED")
    if x is None:
        return 0
    if isinstance(x, tuple):
        h = 0x345678
        for i in x:
            h ^= portable_hash(i)
            h *= 1000003
            h &= sys.maxsize
        h ^= len(x)
        if h == -1:
            h = -2
        return int(h)
    return hash(x)
--------------------------------------------------

--- SAMPLE #32 ---
DOCSTRING: Parse a memory string in the format supported by Java (e.g. 1g, 200m) and return the value in MiB
CODE:
def _parse_memory(s):
    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}
    if s[-1].lower() not in units:
        raise ValueError("invalid format: " + s)
    return int(float(s[:-1]) * units[s[-1].lower()])
--------------------------------------------------

--- SAMPLE #33 ---
DOCSTRING: Return a sampled subset of this RDD.
CODE:
def sample(self, withReplacement, fraction, seed=None):
        assert fraction >= 0.0, "Negative fraction value: %s" % fraction
        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)
--------------------------------------------------

--- SAMPLE #34 ---
DOCSTRING: Randomly splits this RDD with the provided weights.
CODE:
def randomSplit(self, weights, seed=None):
        s = float(sum(weights))
        cweights = [0.0]
        for w in weights:
            cweights.append(cweights[-1] + w / s)
        if seed is None:
            seed = random.randint(0, 2 ** 32 - 1)
        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)
                for lb, ub in zip(cweights, cweights[1:])]
--------------------------------------------------

--- SAMPLE #35 ---
DOCSTRING: Return a fixed-size sampled subset of this RDD.
CODE:
def takeSample(self, withReplacement, num, seed=None):
        numStDev = 10.0
        if num < 0:
            raise ValueError("Sample size cannot be negative.")
        elif num == 0:
            return []
        initialCount = self.count()
        if initialCount == 0:
            return []
        rand = random.Random(seed)
        if (not withReplacement) and num >= initialCount:
            samples = self.collect()
            rand.shuffle(samples)
            return samples
        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))
        if num > maxSampleSize:
            raise ValueError(
                "Sample size cannot be greater than %d." % maxSampleSize)
        fraction = RDD._computeFractionForSampleSize(
            num, initialCount, withReplacement)
        samples = self.sample(withReplacement, fraction, seed).collect()
        while len(samples) < num:
            seed = rand.randint(0, sys.maxsize)
            samples = self.sample(withReplacement, fraction, seed).collect()
        rand.shuffle(samples)
        return samples[0:num]
--------------------------------------------------

--- SAMPLE #36 ---
DOCSTRING: Returns a sampling rate that guarantees a sample of size >= sampleSizeLowerBound 99.99% of the time.
CODE:
def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):
        fraction = float(sampleSizeLowerBound) / total
        if withReplacement:
            numStDev = 5
            if (sampleSizeLowerBound < 12):
                numStDev = 9
            return fraction + numStDev * sqrt(fraction / total)
        else:
            delta = 0.00005
            gamma = - log(delta) / total
            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))
--------------------------------------------------

--- SAMPLE #37 ---
DOCSTRING: Return the union of this RDD and another one.
CODE:
def union(self, other):
        if self._jrdd_deserializer == other._jrdd_deserializer:
            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,
                      self._jrdd_deserializer)
        else:
            self_copy = self._reserialize()
            other_copy = other._reserialize()
            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,
                      self.ctx.serializer)
        if (self.partitioner == other.partitioner and
                self.getNumPartitions() == rdd.getNumPartitions()):
            rdd.partitioner = self.partitioner
        return rdd
--------------------------------------------------

--- SAMPLE #38 ---
DOCSTRING: Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys.
CODE:
def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,
                                           ascending=True, keyfunc=lambda x: x):
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        memory = _parse_memory(self.ctx._conf.get("spark.python.worker.memory", "512m"))
        serializer = self._jrdd_deserializer
        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))
        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)
--------------------------------------------------

--- SAMPLE #39 ---
DOCSTRING: Sorts this RDD, which is assumed to consist of (key, value) pairs.
CODE:
def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        memory = self._memory_limit()
        serializer = self._jrdd_deserializer
        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))
        if numPartitions == 1:
            if self.getNumPartitions() > 1:
                self = self.coalesce(1)
            return self.mapPartitions(sortPartition, True)
        rddSize = self.count()
        if not rddSize:
            return self  
        maxSampleSize = numPartitions * 20.0  
        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)
        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()
        samples = sorted(samples, key=keyfunc)
        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]
                  for i in range(0, numPartitions - 1)]
        def rangePartitioner(k):
            p = bisect.bisect_left(bounds, keyfunc(k))
            if ascending:
                return p
            else:
                return numPartitions - 1 - p
        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)
--------------------------------------------------

--- SAMPLE #40 ---
DOCSTRING: Return an RDD created by piping elements to a forked external process.
CODE:
def pipe(self, command, env=None, checkCode=False):
        if env is None:
            env = dict()
        def func(iterator):
            pipe = Popen(
                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)
            def pipe_objs(out):
                for obj in iterator:
                    s = unicode(obj).rstrip('\n') + '\n'
                    out.write(s.encode('utf-8'))
                out.close()
            Thread(target=pipe_objs, args=[pipe.stdin]).start()
            def check_return_code():
                pipe.wait()
                if checkCode and pipe.returncode:
                    raise Exception("Pipe function `%s' exited "
                                    "with error code %d" % (command, pipe.returncode))
                else:
                    for i in range(0):
                        yield i
            return (x.rstrip(b'\n').decode('utf-8') for x in
                    chain(iter(pipe.stdout.readline, b''), check_return_code()))
        return self.mapPartitions(func)
--------------------------------------------------

--- SAMPLE #41 ---
DOCSTRING: Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally.
CODE:
def reduce(self, f):
        f = fail_on_stopiteration(f)
        def func(iterator):
            iterator = iter(iterator)
            try:
                initial = next(iterator)
            except StopIteration:
                return
            yield reduce(f, iterator, initial)
        vals = self.mapPartitions(func).collect()
        if vals:
            return reduce(f, vals)
        raise ValueError("Can not reduce() empty RDD")
--------------------------------------------------

--- SAMPLE #42 ---
DOCSTRING: Reduces the elements of this RDD in a multi-level tree pattern.
CODE:
def treeReduce(self, f, depth=2):
        if depth < 1:
            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)
        zeroValue = None, True  
        def op(x, y):
            if x[1]:
                return y
            elif y[1]:
                return x
            else:
                return f(x[0], y[0]), False
        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)
        if reduced[1]:
            raise ValueError("Cannot reduce empty RDD.")
        return reduced[0]
--------------------------------------------------

--- SAMPLE #43 ---
DOCSTRING: Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral "zero value."
CODE:
def fold(self, zeroValue, op):
        op = fail_on_stopiteration(op)
        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = op(acc, obj)
            yield acc
        vals = self.mapPartitions(func).collect()
        return reduce(op, vals, zeroValue)
--------------------------------------------------

--- SAMPLE #44 ---
DOCSTRING: Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral "zero value."
CODE:
def aggregate(self, zeroValue, seqOp, combOp):
        seqOp = fail_on_stopiteration(seqOp)
        combOp = fail_on_stopiteration(combOp)
        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        vals = self.mapPartitions(func).collect()
        return reduce(combOp, vals, zeroValue)
--------------------------------------------------

--- SAMPLE #45 ---
DOCSTRING: Aggregates the elements of this RDD in a multi-level tree pattern.
CODE:
def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):
        if depth < 1:
            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)
        if self.getNumPartitions() == 0:
            return zeroValue
        def aggregatePartition(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        partiallyAggregated = self.mapPartitions(aggregatePartition)
        numPartitions = partiallyAggregated.getNumPartitions()
        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)
        while numPartitions > scale + numPartitions / scale:
            numPartitions /= scale
            curNumPartitions = int(numPartitions)
            def mapPartition(i, iterator):
                for obj in iterator:
                    yield (i % curNumPartitions, obj)
            partiallyAggregated = partiallyAggregated \
                .mapPartitionsWithIndex(mapPartition) \
                .reduceByKey(combOp, curNumPartitions) \
                .values()
        return partiallyAggregated.reduce(combOp)
--------------------------------------------------

--- SAMPLE #46 ---
DOCSTRING: Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.
CODE:
def countByValue(self):
        def countPartition(iterator):
            counts = defaultdict(int)
            for obj in iterator:
                counts[obj] += 1
            yield counts
        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] += v
            return m1
        return self.mapPartitions(countPartition).reduce(mergeMaps)
--------------------------------------------------

--- SAMPLE #47 ---
DOCSTRING: Get the top N elements from an RDD.
CODE:
def top(self, num, key=None):
        def topIterator(iterator):
            yield heapq.nlargest(num, iterator, key=key)
        def merge(a, b):
            return heapq.nlargest(num, a + b, key=key)
        return self.mapPartitions(topIterator).reduce(merge)
--------------------------------------------------

--- SAMPLE #48 ---
DOCSTRING: Take the first num elements of the RDD.
CODE:
def take(self, num):
        items = []
        totalParts = self.getNumPartitions()
        partsScanned = 0
        while len(items) < num and partsScanned < totalParts:
            numPartsToTry = 1
            if partsScanned > 0:
                if len(items) == 0:
                    numPartsToTry = partsScanned * 4
                else:
                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned
                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)
            left = num - len(items)
            def takeUpToNumLeft(iterator):
                iterator = iter(iterator)
                taken = 0
                while taken < left:
                    try:
                        yield next(iterator)
                    except StopIteration:
                        return
                    taken += 1
            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
            res = self.context.runJob(self, takeUpToNumLeft, p)
            items += res
            partsScanned += numPartsToTry
        return items[:num]
--------------------------------------------------

--- SAMPLE #49 ---
DOCSTRING: Save this RDD as a text file, using string representations of elements.
CODE:
def saveAsTextFile(self, path, compressionCodecClass=None):
        def func(split, iterator):
            for x in iterator:
                if not isinstance(x, (unicode, bytes)):
                    x = unicode(x)
                if isinstance(x, unicode):
                    x = x.encode("utf-8")
                yield x
        keyed = self.mapPartitionsWithIndex(func)
        keyed._bypass_serializer = True
        if compressionCodecClass:
            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)
        else:
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)
--------------------------------------------------

--- SAMPLE #50 ---
DOCSTRING: Merge the values for each key using an associative and commutative reduce function, but return the results immediately to the master as a dictionary.
CODE:
def reduceByKeyLocally(self, func):
        func = fail_on_stopiteration(func)
        def reducePartition(iterator):
            m = {}
            for k, v in iterator:
                m[k] = func(m[k], v) if k in m else v
            yield m
        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] = func(m1[k], v) if k in m1 else v
            return m1
        return self.mapPartitions(reducePartition).reduce(mergeMaps)
--------------------------------------------------

--- SAMPLE #51 ---
DOCSTRING: Return a copy of the RDD partitioned using the specified partitioner.
CODE:
def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        partitioner = Partitioner(numPartitions, partitionFunc)
        if self.partitioner == partitioner:
            return self
        outputSerializer = self.ctx._unbatched_serializer
        limit = (_parse_memory(self.ctx._conf.get(
            "spark.python.worker.memory", "512m")) / 2)
        def add_shuffle_key(split, iterator):
            buckets = defaultdict(list)
            c, batch = 0, min(10 * numPartitions, 1000)
            for k, v in iterator:
                buckets[partitionFunc(k) % numPartitions].append((k, v))
                c += 1
                if (c % 1000 == 0 and get_used_memory() > limit
                        or c > batch):
                    n, size = len(buckets), 0
                    for split in list(buckets.keys()):
                        yield pack_long(split)
                        d = outputSerializer.dumps(buckets[split])
                        del buckets[split]
                        yield d
                        size += len(d)
                    avg = int(size / n) >> 20
                    if avg < 1:
                        batch *= 1.5
                    elif avg > 10:
                        batch = max(int(batch / 1.5), 1)
                    c = 0
            for split, items in buckets.items():
                yield pack_long(split)
                yield outputSerializer.dumps(items)
        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)
        keyed._bypass_serializer = True
        with SCCallSiteSync(self.context) as css:
            pairRDD = self.ctx._jvm.PairwiseRDD(
                keyed._jrdd.rdd()).asJavaPairRDD()
            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,
                                                           id(partitionFunc))
        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))
        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))
        rdd.partitioner = partitioner
        return rdd
--------------------------------------------------

--- SAMPLE #52 ---
DOCSTRING: Generic function to combine the elements for each key using a custom set of aggregation functions.
CODE:
def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None, partitionFunc=portable_hash):
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        serializer = self.ctx.serializer
        memory = self._memory_limit()
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)
        def combineLocally(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()
        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)
        def _mergeCombiners(iterator):
            merger = ExternalMerger(agg, memory, serializer)
            merger.mergeCombiners(iterator)
            return merger.items()
        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)
--------------------------------------------------

--- SAMPLE #53 ---
DOCSTRING: Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.
CODE:
def groupByKey(self, numPartitions=None, partitionFunc=portable_hash):
        def createCombiner(x):
            return [x]
        def mergeValue(xs, x):
            xs.append(x)
            return xs
        def mergeCombiners(a, b):
            a.extend(b)
            return a
        memory = self._memory_limit()
        serializer = self._jrdd_deserializer
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)
        def combine(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()
        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)
        def groupByKey(it):
            merger = ExternalGroupBy(agg, memory, serializer)
            merger.mergeCombiners(it)
            return merger.items()
        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)
--------------------------------------------------

--- SAMPLE #54 ---
DOCSTRING: Return a subset of this RDD sampled by key (via stratified sampling). Create a sample of this RDD using variable sampling rates for different keys as specified by fractions, a key to sampling rate map.
CODE:
def sampleByKey(self, withReplacement, fractions, seed=None):
        for fraction in fractions.values():
            assert fraction >= 0.0, "Negative fraction value: %s" % fraction
        return self.mapPartitionsWithIndex(
            RDDStratifiedSampler(withReplacement, fractions, seed).func, True)
--------------------------------------------------

--- SAMPLE #55 ---
DOCSTRING: Return each (key, value) pair in C{self} that has no pair with matching key in C{other}.
CODE:
def subtractByKey(self, other, numPartitions=None):
        def filter_func(pair):
            key, (val1, val2) = pair
            return val1 and not val2
        return self.cogroup(other, numPartitions).filter(filter_func).flatMapValues(lambda x: x[0])
--------------------------------------------------

--- SAMPLE #56 ---
DOCSTRING: Return a new RDD that is reduced into `numPartitions` partitions.
CODE:
def coalesce(self, numPartitions, shuffle=False):
        if shuffle:
            batchSize = min(10, self.ctx._batchSize or 1024)
            ser = BatchedSerializer(PickleSerializer(), batchSize)
            selfCopy = self._reserialize(ser)
            jrdd_deserializer = selfCopy._jrdd_deserializer
            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)
        else:
            jrdd_deserializer = self._jrdd_deserializer
            jrdd = self._jrdd.coalesce(numPartitions, shuffle)
        return RDD(jrdd, self.ctx, jrdd_deserializer)
--------------------------------------------------

--- SAMPLE #57 ---
DOCSTRING: Zips this RDD with its element indices.
CODE:
def zipWithIndex(self):
        starts = [0]
        if self.getNumPartitions() > 1:
            nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
            for i in range(len(nums) - 1):
                starts.append(starts[-1] + nums[i])
        def func(k, it):
            for i, v in enumerate(it, starts[k]):
                yield v, i
        return self.mapPartitionsWithIndex(func)
--------------------------------------------------

--- SAMPLE #58 ---
DOCSTRING: Zips this RDD with generated unique Long ids.
CODE:
def zipWithUniqueId(self):
        n = self.getNumPartitions()
        def func(k, it):
            for i, v in enumerate(it):
                yield v, i * n + k
        return self.mapPartitionsWithIndex(func)
--------------------------------------------------

--- SAMPLE #59 ---
DOCSTRING: Return the list of values in the RDD for key `key`. This operation is done efficiently if the RDD has a known partitioner by only searching the partition that the key maps to.
CODE:
def lookup(self, key):
        values = self.filter(lambda kv: kv[0] == key).values()
        if self.partitioner is not None:
            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])
        return values.collect()
--------------------------------------------------

--- SAMPLE #60 ---
DOCSTRING: .. note:: Experimental
CODE:
def sumApprox(self, timeout, confidence=0.95):
        jrdd = self.mapPartitions(lambda it: [float(sum(it))])._to_java_object_rdd()
        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())
        r = jdrdd.sumApprox(timeout, confidence).getFinalValue()
        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())
--------------------------------------------------

--- SAMPLE #61 ---
DOCSTRING: .. note:: Experimental
CODE:
def countApproxDistinct(self, relativeSD=0.05):
        if relativeSD < 0.000017:
            raise ValueError("relativeSD should be greater than 0.000017")
        hashRDD = self.map(lambda x: portable_hash(x) & 0xFFFFFFFF)
        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)
--------------------------------------------------

--- SAMPLE #62 ---
DOCSTRING: Create a method for given binary operator
CODE:
def _bin_op(name, doc="binary operator"):
    def _(self, other):
        jc = other._jc if isinstance(other, Column) else other
        njc = getattr(self._jc, name)(jc)
        return Column(njc)
    _.__doc__ = doc
    return _
--------------------------------------------------

--- SAMPLE #63 ---
DOCSTRING: Create a method for binary operator (this object is on right side)
CODE:
def _reverse_op(name, doc="binary operator"):
    def _(self, other):
        jother = _create_column_from_literal(other)
        jc = getattr(jother, name)(self._jc)
        return Column(jc)
    _.__doc__ = doc
    return _
--------------------------------------------------

--- SAMPLE #64 ---
DOCSTRING: Return a :class:`Column` which is a substring of the column.
CODE:
def substr(self, startPos, length):
        if type(startPos) != type(length):
            raise TypeError(
                "startPos and length must be the same type. "
                "Got {startPos_t} and {length_t}, respectively."
                .format(
                    startPos_t=type(startPos),
                    length_t=type(length),
                ))
        if isinstance(startPos, int):
            jc = self._jc.substr(startPos, length)
        elif isinstance(startPos, Column):
            jc = self._jc.substr(startPos._jc, length._jc)
        else:
            raise TypeError("Unexpected type: %s" % type(startPos))
        return Column(jc)
--------------------------------------------------

--- SAMPLE #65 ---
DOCSTRING: A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.
CODE:
def isin(self, *cols):
        if len(cols) == 1 and isinstance(cols[0], (list, set)):
            cols = cols[0]
        cols = [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols]
        sc = SparkContext._active_spark_context
        jc = getattr(self._jc, "isin")(_to_seq(sc, cols))
        return Column(jc)
--------------------------------------------------

--- SAMPLE #66 ---
DOCSTRING: Returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).
CODE:
def alias(self, *alias, **kwargs):
        metadata = kwargs.pop('metadata', None)
        assert not kwargs, 'Unexpected kwargs where passed: %s' % kwargs
        sc = SparkContext._active_spark_context
        if len(alias) == 1:
            if metadata:
                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(
                    json.dumps(metadata))
                return Column(getattr(self._jc, "as")(alias[0], jmeta))
            else:
                return Column(getattr(self._jc, "as")(alias[0]))
        else:
            if metadata:
                raise ValueError('metadata can only be provided for a single column')
            return Column(getattr(self._jc, "as")(_to_seq(sc, list(alias))))
--------------------------------------------------

--- SAMPLE #67 ---
DOCSTRING: Convert the column into type ``dataType``.
CODE:
def cast(self, dataType):
        if isinstance(dataType, basestring):
            jc = self._jc.cast(dataType)
        elif isinstance(dataType, DataType):
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.getOrCreate()
            jdt = spark._jsparkSession.parseDataType(dataType.json())
            jc = self._jc.cast(jdt)
        else:
            raise TypeError("unexpected type: %s" % type(dataType))
        return Column(jc)
--------------------------------------------------

--- SAMPLE #68 ---
DOCSTRING: Evaluates a list of conditions and returns one of multiple possible result expressions. If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.
CODE:
def when(self, condition, value):
        if not isinstance(condition, Column):
            raise TypeError("condition should be a Column")
        v = value._jc if isinstance(value, Column) else value
        jc = self._jc.when(condition._jc, v)
        return Column(jc)
--------------------------------------------------

--- SAMPLE #69 ---
DOCSTRING: Define a windowing column.
CODE:
def over(self, window):
        from pyspark.sql.window import WindowSpec
        if not isinstance(window, WindowSpec):
            raise TypeError("window should be WindowSpec")
        jc = self._jc.over(window._jspec)
        return Column(jc)
--------------------------------------------------

--- SAMPLE #70 ---
DOCSTRING: Transforms the input document (list of terms) to term frequency vectors, or transform the RDD of document to RDD of term frequency vectors.
CODE:
def transform(self, document):
        if isinstance(document, RDD):
            return document.map(self.transform)
        freq = {}
        for term in document:
            i = self.indexOf(term)
            freq[i] = 1.0 if self.binary else freq.get(i, 0) + 1.0
        return Vectors.sparse(self.numFeatures, freq.items())
--------------------------------------------------

--- SAMPLE #71 ---
DOCSTRING: Computes the inverse document frequency.
CODE:
def fit(self, dataset):
        if not isinstance(dataset, RDD):
            raise TypeError("dataset should be an RDD of term frequency vectors")
        jmodel = callMLlibFunc("fitIDF", self.minDocFreq, dataset.map(_convert_to_vector))
        return IDFModel(jmodel)
--------------------------------------------------

--- SAMPLE #72 ---
DOCSTRING: Find synonyms of a word
CODE:
def findSynonyms(self, word, num):
        if not isinstance(word, basestring):
            word = _convert_to_vector(word)
        words, similarity = self.call("findSynonyms", word, num)
        return zip(words, similarity)
--------------------------------------------------

--- SAMPLE #73 ---
DOCSTRING: Train a decision tree model for classification.
CODE:
def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,
                        impurity="gini", maxDepth=5, maxBins=32, minInstancesPerNode=1,
                        minInfoGain=0.0):
        return cls._train(data, "classification", numClasses, categoricalFeaturesInfo,
                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)
--------------------------------------------------

--- SAMPLE #74 ---
DOCSTRING: Train a random forest model for binary or multiclass classification.
CODE:
def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, numTrees,
                        featureSubsetStrategy="auto", impurity="gini", maxDepth=4, maxBins=32,
                        seed=None):
        return cls._train(data, "classification", numClasses,
                          categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity,
                          maxDepth, maxBins, seed)
--------------------------------------------------

--- SAMPLE #75 ---
DOCSTRING: Train a random forest model for regression.
CODE:
def trainRegressor(cls, data, categoricalFeaturesInfo, numTrees, featureSubsetStrategy="auto",
                       impurity="variance", maxDepth=4, maxBins=32, seed=None):
        return cls._train(data, "regression", 0, categoricalFeaturesInfo, numTrees,
                          featureSubsetStrategy, impurity, maxDepth, maxBins, seed)
--------------------------------------------------

--- SAMPLE #76 ---
DOCSTRING: Set an environment variable to be passed to executors.
CODE:
def setExecutorEnv(self, key=None, value=None, pairs=None):
        if (key is not None and pairs is not None) or (key is None and pairs is None):
            raise Exception("Either pass one key-value pair or a list of pairs")
        elif key is not None:
            self.set("spark.executorEnv." + key, value)
        elif pairs is not None:
            for (k, v) in pairs:
                self.set("spark.executorEnv." + k, v)
        return self
--------------------------------------------------

--- SAMPLE #77 ---
DOCSTRING: Get the configured value for some key, or return a default otherwise.
CODE:
def get(self, key, defaultValue=None):
        if defaultValue is None:   
            if self._jconf is not None:
                if not self._jconf.contains(key):
                    return None
                return self._jconf.get(key)
            else:
                if key not in self._conf:
                    return None
                return self._conf[key]
        else:
            if self._jconf is not None:
                return self._jconf.get(key, defaultValue)
            else:
                return self._conf.get(key, defaultValue)
--------------------------------------------------

--- SAMPLE #78 ---
DOCSTRING: Returns a printable version of the configuration, as a list of key=value pairs, one per line.
CODE:
def toDebugString(self):
        if self._jconf is not None:
            return self._jconf.toDebugString()
        else:
            return '\n'.join('%s=%s' % (k, v) for k, v in self._conf.items())
--------------------------------------------------

--- SAMPLE #79 ---
DOCSTRING: Returns a list of tables/views in the specified database.
CODE:
def listTables(self, dbName=None):
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listTables(dbName).toLocalIterator()
        tables = []
        while iter.hasNext():
            jtable = iter.next()
            tables.append(Table(
                name=jtable.name(),
                database=jtable.database(),
                description=jtable.description(),
                tableType=jtable.tableType(),
                isTemporary=jtable.isTemporary()))
        return tables
--------------------------------------------------

--- SAMPLE #80 ---
DOCSTRING: Returns a list of functions registered in the specified database.
CODE:
def listFunctions(self, dbName=None):
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listFunctions(dbName).toLocalIterator()
        functions = []
        while iter.hasNext():
            jfunction = iter.next()
            functions.append(Function(
                name=jfunction.name(),
                description=jfunction.description(),
                className=jfunction.className(),
                isTemporary=jfunction.isTemporary()))
        return functions
--------------------------------------------------

--- SAMPLE #81 ---
DOCSTRING: Returns a list of columns for the given table/view in the specified database.
CODE:
def listColumns(self, tableName, dbName=None):
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listColumns(dbName, tableName).toLocalIterator()
        columns = []
        while iter.hasNext():
            jcolumn = iter.next()
            columns.append(Column(
                name=jcolumn.name(),
                description=jcolumn.description(),
                dataType=jcolumn.dataType(),
                nullable=jcolumn.nullable(),
                isPartition=jcolumn.isPartition(),
                isBucket=jcolumn.isBucket()))
        return columns
--------------------------------------------------

--- SAMPLE #82 ---
DOCSTRING: Creates a table based on the dataset in a data source.
CODE:
def createExternalTable(self, tableName, path=None, source=None, schema=None, **options):
        warnings.warn(
            "createExternalTable is deprecated since Spark 2.2, please use createTable instead.",
            DeprecationWarning)
        return self.createTable(tableName, path, source, schema, **options)
--------------------------------------------------

--- SAMPLE #83 ---
DOCSTRING: Creates a table based on the dataset in a data source.
CODE:
def createTable(self, tableName, path=None, source=None, schema=None, **options):
        if path is not None:
            options["path"] = path
        if source is None:
            source = self._sparkSession._wrapped._conf.defaultDataSourceName()
        if schema is None:
            df = self._jcatalog.createTable(tableName, source, options)
        else:
            if not isinstance(schema, StructType):
                raise TypeError("schema should be StructType")
            scala_datatype = self._jsparkSession.parseDataType(schema.json())
            df = self._jcatalog.createTable(tableName, source, scala_datatype, options)
        return DataFrame(df, self._sparkSession._wrapped)
--------------------------------------------------

--- SAMPLE #84 ---
DOCSTRING: .. note:: Experimental
CODE:
def barrier(self):
        if self._port is None or self._secret is None:
            raise Exception("Not supported to call barrier() before initialize " +
                            "BarrierTaskContext.")
        else:
            _load_from_socket(self._port, self._secret)
--------------------------------------------------

--- SAMPLE #85 ---
DOCSTRING: .. note:: Experimental
CODE:
def getTaskInfos(self):
        if self._port is None or self._secret is None:
            raise Exception("Not supported to call getTaskInfos() before initialize " +
                            "BarrierTaskContext.")
        else:
            addresses = self._localProperties.get("addresses", "")
            return [BarrierTaskInfo(h.strip()) for h in addresses.split(",")]
--------------------------------------------------

--- SAMPLE #86 ---
DOCSTRING: A decorator that annotates a function to append the version of Spark the function was added.
CODE:
def since(version):
    import re
    indent_p = re.compile(r'\n( +)')
    def deco(f):
        indents = indent_p.findall(f.__doc__)
        indent = ' ' * (min(len(m) for m in indents) if indents else 0)
        f.__doc__ = f.__doc__.rstrip() + "\n\n%s.. versionadded:: %s" % (indent, version)
        return f
    return deco
--------------------------------------------------

--- SAMPLE #87 ---
DOCSTRING: Returns a function with same code, globals, defaults, closure, and name (or provide a new name).
CODE:
def copy_func(f, name=None, sinceversion=None, doc=None):
    fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__,
                            f.__closure__)
    fn.__dict__.update(f.__dict__)
    if doc is not None:
        fn.__doc__ = doc
    if sinceversion is not None:
        fn = since(sinceversion)(fn)
    return fn
--------------------------------------------------

--- SAMPLE #88 ---
DOCSTRING: A decorator that forces keyword arguments in the wrapped method and saves actual input keyword arguments in `_input_kwargs`.
CODE:
def keyword_only(func):
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        if len(args) > 0:
            raise TypeError("Method %s forces keyword arguments." % func.__name__)
        self._input_kwargs = kwargs
        return func(self, **kwargs)
    return wrapper
--------------------------------------------------

--- SAMPLE #89 ---
DOCSTRING: Generates the header part for shared variables
CODE:
def _gen_param_header(name, doc, defaultValueStr, typeConverter):
    template = '''class Has$Name(Params):
    """
    Mixin for param $name: $doc
    """
    $name = Param(Params._dummy(), "$name", "$doc", typeConverter=$typeConverter)
    def __init__(self):
        super(Has$Name, self).__init__()'''
    if defaultValueStr is not None:
        template += '''
        self._setDefault($name=$defaultValueStr)'''
    Name = name[0].upper() + name[1:]
    if typeConverter is None:
        typeConverter = str(None)
    return template \
        .replace("$name", name) \
        .replace("$Name", Name) \
        .replace("$doc", doc) \
        .replace("$defaultValueStr", str(defaultValueStr)) \
        .replace("$typeConverter", typeConverter)
--------------------------------------------------

--- SAMPLE #90 ---
DOCSTRING: Generates Python code for a shared param class.
CODE:
def _gen_param_code(name, doc, defaultValueStr):
    template = '''
    def set$Name(self, value):
        """
        Sets the value of :py:attr:`$name`.
        """
        return self._set($name=value)
    def get$Name(self):
        """
        Gets the value of $name or its default value.
        """
        return self.getOrDefault(self.$name)'''
    Name = name[0].upper() + name[1:]
    return template \
        .replace("$name", name) \
        .replace("$Name", Name) \
        .replace("$doc", doc) \
        .replace("$defaultValueStr", str(defaultValueStr))
--------------------------------------------------

--- SAMPLE #91 ---
DOCSTRING: Train a k-means clustering model.
CODE:
def train(cls, rdd, k, maxIterations=100, runs=1, initializationMode="k-means||",
              seed=None, initializationSteps=2, epsilon=1e-4, initialModel=None):
        if runs != 1:
            warnings.warn("The param `runs` has no effect since Spark 2.0.0.")
        clusterInitialModel = []
        if initialModel is not None:
            if not isinstance(initialModel, KMeansModel):
                raise Exception("initialModel is of "+str(type(initialModel))+". It needs "
                                "to be of <type 'KMeansModel'>")
            clusterInitialModel = [_convert_to_vector(c) for c in initialModel.clusterCenters]
        model = callMLlibFunc("trainKMeansModel", rdd.map(_convert_to_vector), k, maxIterations,
                              runs, initializationMode, seed, initializationSteps, epsilon,
                              clusterInitialModel)
        centers = callJavaFunc(rdd.context, model.clusterCenters)
        return KMeansModel([c.toArray() for c in centers])
--------------------------------------------------

--- SAMPLE #92 ---
DOCSTRING: Train a Gaussian Mixture clustering model.
CODE:
def train(cls, rdd, k, convergenceTol=1e-3, maxIterations=100, seed=None, initialModel=None):
        initialModelWeights = None
        initialModelMu = None
        initialModelSigma = None
        if initialModel is not None:
            if initialModel.k != k:
                raise Exception("Mismatched cluster count, initialModel.k = %s, however k = %s"
                                % (initialModel.k, k))
            initialModelWeights = list(initialModel.weights)
            initialModelMu = [initialModel.gaussians[i].mu for i in range(initialModel.k)]
            initialModelSigma = [initialModel.gaussians[i].sigma for i in range(initialModel.k)]
        java_model = callMLlibFunc("trainGaussianMixtureModel", rdd.map(_convert_to_vector),
                                   k, convergenceTol, maxIterations, seed,
                                   initialModelWeights, initialModelMu, initialModelSigma)
        return GaussianMixtureModel(java_model)
--------------------------------------------------

--- SAMPLE #93 ---
DOCSTRING: Update the centroids, according to data
CODE:
def update(self, data, decayFactor, timeUnit):
        if not isinstance(data, RDD):
            raise TypeError("Data should be of an RDD, got %s." % type(data))
        data = data.map(_convert_to_vector)
        decayFactor = float(decayFactor)
        if timeUnit not in ["batches", "points"]:
            raise ValueError(
                "timeUnit should be 'batches' or 'points', got %s." % timeUnit)
        vectorCenters = [_convert_to_vector(center) for center in self.centers]
        updatedModel = callMLlibFunc(
            "updateStreamingKMeansModel", vectorCenters, self._clusterWeights,
            data, decayFactor, timeUnit)
        self.centers = array(updatedModel[0])
        self._clusterWeights = list(updatedModel[1])
        return self
--------------------------------------------------

--- SAMPLE #94 ---
DOCSTRING: Set the initial centres to be random samples from a gaussian population with constant weights.
CODE:
def setRandomCenters(self, dim, weight, seed):
        rng = random.RandomState(seed)
        clusterCenters = rng.randn(self._k, dim)
        clusterWeights = tile(weight, self._k)
        self._model = StreamingKMeansModel(clusterCenters, clusterWeights)
        return self
--------------------------------------------------

--- SAMPLE #95 ---
DOCSTRING: Load the LDAModel from disk.
CODE:
def load(cls, sc, path):
        if not isinstance(sc, SparkContext):
            raise TypeError("sc should be a SparkContext, got type %s" % type(sc))
        if not isinstance(path, basestring):
            raise TypeError("path should be a basestring, got type %s" % type(path))
        model = callMLlibFunc("loadLDAModel", sc, path)
        return LDAModel(model)
--------------------------------------------------

--- SAMPLE #96 ---
DOCSTRING: Train a LDA model.
CODE:
def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,
              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer="em"):
        model = callMLlibFunc("trainLDAModel", rdd, k, maxIterations,
                              docConcentration, topicConcentration, seed,
                              checkpointInterval, optimizer)
        return LDAModel(model)
--------------------------------------------------

--- SAMPLE #97 ---
DOCSTRING: Convert Python object into Java
CODE:
def _py2java(sc, obj):
    if isinstance(obj, RDD):
        obj = _to_java_object_rdd(obj)
    elif isinstance(obj, DataFrame):
        obj = obj._jdf
    elif isinstance(obj, SparkContext):
        obj = obj._jsc
    elif isinstance(obj, list):
        obj = [_py2java(sc, x) for x in obj]
    elif isinstance(obj, JavaObject):
        pass
    elif isinstance(obj, (int, long, float, bool, bytes, unicode)):
        pass
    else:
        data = bytearray(PickleSerializer().dumps(obj))
        obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)
    return obj
--------------------------------------------------

--- SAMPLE #98 ---
DOCSTRING: A decorator that makes a class inherit documentation from its parents.
CODE:
def inherit_doc(cls):
    for name, func in vars(cls).items():
        if name.startswith("_"):
            continue
        if not func.__doc__:
            for parent in cls.__bases__:
                parent_func = getattr(parent, name, None)
                if parent_func and getattr(parent_func, "__doc__", None):
                    func.__doc__ = parent_func.__doc__
                    break
    return cls
--------------------------------------------------

--- SAMPLE #99 ---
DOCSTRING: Return a new DStream by applying combineByKey to each RDD.
CODE:
def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None):
        if numPartitions is None:
            numPartitions = self._sc.defaultParallelism
        def func(rdd):
            return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)
        return self.transform(func)
--------------------------------------------------

--- SAMPLE #100 ---
DOCSTRING: Apply a function to each RDD in this DStream.
CODE:
def foreachRDD(self, func):
        if func.__code__.co_argcount == 1:
            old_func = func
            func = lambda t, rdd: old_func(rdd)
        jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer)
        api = self._ssc._jvm.PythonDStream
        api.callForeachRDD(self._jdstream, jfunc)
--------------------------------------------------



================================================================================

--- FILE: data_preview_raw.txt | PATH: Project\Datasets\Human_readable_sample\data_preview_raw.txt ---
------------------------------------------------------------------------------------------------------

=== DATASET PREVIEW ===
Source File: C:\Users\mpiet\Documents\GitHub\Project_MLSA_MMA\Project\Datasets\python\final\jsonl\train\python_train_0.jsonl.gz

--- SAMPLE #1 ---
DOCSTRING: Trains a k-nearest neighbors classifier for face recognition.

    :param train_dir: directory that contains a sub-directory for each known person, with its name.

     (View in source code to see train_dir example tree structure)

     Structure:
        <train_dir>/
        â”œâ”€â”€ <person1>/
        â”‚   â”œâ”€â”€ <somename1>.jpeg
        â”‚   â”œâ”€â”€ <somename2>.jpeg
        â”‚   â”œâ”€â”€ ...
        â”œâ”€â”€ <person2>/
        â”‚   â”œâ”€â”€ <somename1>.jpeg
        â”‚   â””â”€â”€ <somename2>.jpeg
        â””â”€â”€ ...

    :param model_save_path: (optional) path to save model on disk
    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified
    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree
    :param verbose: verbosity of training
    :return: returns knn classifier that was trained on the given data.
CODE:
def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):
    """
    Trains a k-nearest neighbors classifier for face recognition.

    :param train_dir: directory that contains a sub-directory for each known person, with its name.

     (View in source code to see train_dir example tree structure)

     Structure:
        <train_dir>/
        â”œâ”€â”€ <person1>/
        â”‚   â”œâ”€â”€ <somename1>.jpeg
        â”‚   â”œâ”€â”€ <somename2>.jpeg
        â”‚   â”œâ”€â”€ ...
        â”œâ”€â”€ <person2>/
        â”‚   â”œâ”€â”€ <somename1>.jpeg
        â”‚   â””â”€â”€ <somename2>.jpeg
        â””â”€â”€ ...

    :param model_save_path: (optional) path to save model on disk
    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified
    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree
    :param verbose: verbosity of training
    :return: returns knn classifier that was trained on the given data.
    """
    X = []
    y = []

    # Loop through each person in the training set
    for class_dir in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_dir)):
            continue

        # Loop through each training image for the current person
        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):
            image = face_recognition.load_image_file(img_path)
            face_bounding_boxes = face_recognition.face_locations(image)

            if len(face_bounding_boxes) != 1:
                # If there are no people (or too many people) in a training image, skip the image.
                if verbose:
                    print("Image {} not suitable for training: {}".format(img_path, "Didn't find a face" if len(face_bounding_boxes) < 1 else "Found more than one face"))
            else:
                # Add face encoding for current image to the training set
                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])
                y.append(class_dir)

    # Determine how many neighbors to use for weighting in the KNN classifier
    if n_neighbors is None:
        n_neighbors = int(round(math.sqrt(len(X))))
        if verbose:
            print("Chose n_neighbors automatically:", n_neighbors)

    # Create and train the KNN classifier
    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')
    knn_clf.fit(X, y)

    # Save the trained KNN classifier
    if model_save_path is not None:
        with open(model_save_path, 'wb') as f:
            pickle.dump(knn_clf, f)

    return knn_clf
--------------------------------------------------

--- SAMPLE #2 ---
DOCSTRING: Recognizes faces in given image using a trained KNN classifier

    :param X_img_path: path to image to be recognized
    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.
    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.
    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance
           of mis-classifying an unknown person as a known one.
    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
        For faces of unrecognized persons, the name 'unknown' will be returned.
CODE:
def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):
    """
    Recognizes faces in given image using a trained KNN classifier

    :param X_img_path: path to image to be recognized
    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.
    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.
    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance
           of mis-classifying an unknown person as a known one.
    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
        For faces of unrecognized persons, the name 'unknown' will be returned.
    """
    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:
        raise Exception("Invalid image path: {}".format(X_img_path))

    if knn_clf is None and model_path is None:
        raise Exception("Must supply knn classifier either thourgh knn_clf or model_path")

    # Load a trained KNN model (if one was passed in)
    if knn_clf is None:
        with open(model_path, 'rb') as f:
            knn_clf = pickle.load(f)

    # Load image file and find face locations
    X_img = face_recognition.load_image_file(X_img_path)
    X_face_locations = face_recognition.face_locations(X_img)

    # If no faces are found in the image, return an empty result.
    if len(X_face_locations) == 0:
        return []

    # Find encodings for faces in the test iamge
    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)

    # Use the KNN model to find the best matches for the test face
    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)
    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]

    # Predict classes and remove classifications that aren't within the threshold
    return [(pred, loc) if rec else ("unknown", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]
--------------------------------------------------

--- SAMPLE #3 ---
DOCSTRING: Shows the face recognition results visually.

    :param img_path: path to image to be recognized
    :param predictions: results of the predict function
    :return:
CODE:
def show_prediction_labels_on_image(img_path, predictions):
    """
    Shows the face recognition results visually.

    :param img_path: path to image to be recognized
    :param predictions: results of the predict function
    :return:
    """
    pil_image = Image.open(img_path).convert("RGB")
    draw = ImageDraw.Draw(pil_image)

    for name, (top, right, bottom, left) in predictions:
        # Draw a box around the face using the Pillow module
        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))

        # There's a bug in Pillow where it blows up with non-UTF-8 text
        # when using the default bitmap font
        name = name.encode("UTF-8")

        # Draw a label with a name below the face
        text_width, text_height = draw.textsize(name)
        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))

    # Remove the drawing library from memory as per the Pillow docs
    del draw

    # Display the resulting image
    pil_image.show()
--------------------------------------------------

--- SAMPLE #4 ---
DOCSTRING: Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order

    :param rect: a dlib 'rect' object
    :return: a plain tuple representation of the rect in (top, right, bottom, left) order
CODE:
def _rect_to_css(rect):
    """
    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order

    :param rect: a dlib 'rect' object
    :return: a plain tuple representation of the rect in (top, right, bottom, left) order
    """
    return rect.top(), rect.right(), rect.bottom(), rect.left()
--------------------------------------------------

--- SAMPLE #5 ---
DOCSTRING: Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.

    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order
    :param image_shape: numpy shape of the image array
    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order
CODE:
def _trim_css_to_bounds(css, image_shape):
    """
    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.

    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order
    :param image_shape: numpy shape of the image array
    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order
    """
    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)
--------------------------------------------------

--- SAMPLE #6 ---
DOCSTRING: Given a list of face encodings, compare them to a known face encoding and get a euclidean distance
    for each comparison face. The distance tells you how similar the faces are.

    :param faces: List of face encodings to compare
    :param face_to_compare: A face encoding to compare against
    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array
CODE:
def face_distance(face_encodings, face_to_compare):
    """
    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance
    for each comparison face. The distance tells you how similar the faces are.

    :param faces: List of face encodings to compare
    :param face_to_compare: A face encoding to compare against
    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array
    """
    if len(face_encodings) == 0:
        return np.empty((0))

    return np.linalg.norm(face_encodings - face_to_compare, axis=1)
--------------------------------------------------

--- SAMPLE #7 ---
DOCSTRING: Loads an image file (.jpg, .png, etc) into a numpy array

    :param file: image file name or file object to load
    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.
    :return: image contents as numpy array
CODE:
def load_image_file(file, mode='RGB'):
    """
    Loads an image file (.jpg, .png, etc) into a numpy array

    :param file: image file name or file object to load
    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.
    :return: image contents as numpy array
    """
    im = PIL.Image.open(file)
    if mode:
        im = im.convert(mode)
    return np.array(im)
--------------------------------------------------

--- SAMPLE #8 ---
DOCSTRING: Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
    :return: A list of dlib 'rect' objects of found face locations
CODE:
def _raw_face_locations(img, number_of_times_to_upsample=1, model="hog"):
    """
    Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
    :return: A list of dlib 'rect' objects of found face locations
    """
    if model == "cnn":
        return cnn_face_detector(img, number_of_times_to_upsample)
    else:
        return face_detector(img, number_of_times_to_upsample)
--------------------------------------------------

--- SAMPLE #9 ---
DOCSTRING: Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
CODE:
def face_locations(img, number_of_times_to_upsample=1, model="hog"):
    """
    Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
    """
    if model == "cnn":
        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, "cnn")]
    else:
        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]
--------------------------------------------------

--- SAMPLE #10 ---
DOCSTRING: Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector
    If you are using a GPU, this can give you much faster results since the GPU
    can process batches of images at once. If you aren't using a GPU, you don't need this function.

    :param img: A list of images (each as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param batch_size: How many images to include in each GPU processing batch.
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
CODE:
def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):
    """
    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector
    If you are using a GPU, this can give you much faster results since the GPU
    can process batches of images at once. If you aren't using a GPU, you don't need this function.

    :param img: A list of images (each as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param batch_size: How many images to include in each GPU processing batch.
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
    """
    def convert_cnn_detections_to_css(detections):
        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]

    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)

    return list(map(convert_cnn_detections_to_css, raw_detections_batched))
--------------------------------------------------

--- SAMPLE #11 ---
DOCSTRING: Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image

    :param face_image: image to search
    :param face_locations: Optionally provide a list of face locations to check.
    :param model: Optional - which model to use. "large" (default) or "small" which only returns 5 points but is faster.
    :return: A list of dicts of face feature locations (eyes, nose, etc)
CODE:
def face_landmarks(face_image, face_locations=None, model="large"):
    """
    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image

    :param face_image: image to search
    :param face_locations: Optionally provide a list of face locations to check.
    :param model: Optional - which model to use. "large" (default) or "small" which only returns 5 points but is faster.
    :return: A list of dicts of face feature locations (eyes, nose, etc)
    """
    landmarks = _raw_face_landmarks(face_image, face_locations, model)
    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]

    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png
    if model == 'large':
        return [{
            "chin": points[0:17],
            "left_eyebrow": points[17:22],
            "right_eyebrow": points[22:27],
            "nose_bridge": points[27:31],
            "nose_tip": points[31:36],
            "left_eye": points[36:42],
            "right_eye": points[42:48],
            "top_lip": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],
            "bottom_lip": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]
        } for points in landmarks_as_tuples]
    elif model == 'small':
        return [{
            "nose_tip": [points[4]],
            "left_eye": points[2:4],
            "right_eye": points[0:2],
        } for points in landmarks_as_tuples]
    else:
        raise ValueError("Invalid landmarks model type. Supported models are ['small', 'large'].")
--------------------------------------------------

--- SAMPLE #12 ---
DOCSTRING: Given an image, return the 128-dimension face encoding for each face in the image.

    :param face_image: The image that contains one or more faces
    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.
    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)
    :return: A list of 128-dimensional face encodings (one for each face in the image)
CODE:
def face_encodings(face_image, known_face_locations=None, num_jitters=1):
    """
    Given an image, return the 128-dimension face encoding for each face in the image.

    :param face_image: The image that contains one or more faces
    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.
    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)
    :return: A list of 128-dimensional face encodings (one for each face in the image)
    """
    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model="small")
    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]
--------------------------------------------------

--- SAMPLE #13 ---
DOCSTRING: Parses the given data type string to a :class:`DataType`. The data type string format equals
    to :class:`DataType.simpleString`, except that top level struct type can omit
    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead
    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name
    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted
    string and case-insensitive strings.

    >>> _parse_datatype_string("int ")
    IntegerType
    >>> _parse_datatype_string("INT ")
    IntegerType
    >>> _parse_datatype_string("a: byte, b: decimal(  16 , 8   ) ")
    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))
    >>> _parse_datatype_string("a DOUBLE, b STRING")
    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))
    >>> _parse_datatype_string("a: array< short>")
    StructType(List(StructField(a,ArrayType(ShortType,true),true)))
    >>> _parse_datatype_string(" map<string , string > ")
    MapType(StringType,StringType,true)

    >>> # Error cases
    >>> _parse_datatype_string("blabla") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("a: int,") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("array<int") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("map<int, boolean>>") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
CODE:
def _parse_datatype_string(s):
    """
    Parses the given data type string to a :class:`DataType`. The data type string format equals
    to :class:`DataType.simpleString`, except that top level struct type can omit
    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead
    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name
    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted
    string and case-insensitive strings.

    >>> _parse_datatype_string("int ")
    IntegerType
    >>> _parse_datatype_string("INT ")
    IntegerType
    >>> _parse_datatype_string("a: byte, b: decimal(  16 , 8   ) ")
    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))
    >>> _parse_datatype_string("a DOUBLE, b STRING")
    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))
    >>> _parse_datatype_string("a: array< short>")
    StructType(List(StructField(a,ArrayType(ShortType,true),true)))
    >>> _parse_datatype_string(" map<string , string > ")
    MapType(StringType,StringType,true)

    >>> # Error cases
    >>> _parse_datatype_string("blabla") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("a: int,") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("array<int") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string("map<int, boolean>>") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    """
    sc = SparkContext._active_spark_context

    def from_ddl_schema(type_str):
        return _parse_datatype_json_string(
            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())

    def from_ddl_datatype(type_str):
        return _parse_datatype_json_string(
            sc._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.parseDataType(type_str).json())

    try:
        # DDL format, "fieldname datatype, fieldname datatype".
        return from_ddl_schema(s)
    except Exception as e:
        try:
            # For backwards compatibility, "integer", "struct<fieldname: datatype>" and etc.
            return from_ddl_datatype(s)
        except:
            try:
                # For backwards compatibility, "fieldname: datatype, fieldname: datatype" case.
                return from_ddl_datatype("struct<%s>" % s.strip())
            except:
                raise e
--------------------------------------------------

--- SAMPLE #14 ---
DOCSTRING: Return the Catalyst datatype from the size of integers.
CODE:
def _int_size_to_type(size):
    """
    Return the Catalyst datatype from the size of integers.
    """
    if size <= 8:
        return ByteType
    if size <= 16:
        return ShortType
    if size <= 32:
        return IntegerType
    if size <= 64:
        return LongType
--------------------------------------------------

--- SAMPLE #15 ---
DOCSTRING: Infer the DataType from obj
CODE:
def _infer_type(obj):
    """Infer the DataType from obj
    """
    if obj is None:
        return NullType()

    if hasattr(obj, '__UDT__'):
        return obj.__UDT__

    dataType = _type_mappings.get(type(obj))
    if dataType is DecimalType:
        # the precision and scale of `obj` may be different from row to row.
        return DecimalType(38, 18)
    elif dataType is not None:
        return dataType()

    if isinstance(obj, dict):
        for key, value in obj.items():
            if key is not None and value is not None:
                return MapType(_infer_type(key), _infer_type(value), True)
        return MapType(NullType(), NullType(), True)
    elif isinstance(obj, list):
        for v in obj:
            if v is not None:
                return ArrayType(_infer_type(obj[0]), True)
        return ArrayType(NullType(), True)
    elif isinstance(obj, array):
        if obj.typecode in _array_type_mappings:
            return ArrayType(_array_type_mappings[obj.typecode](), False)
        else:
            raise TypeError("not supported type: array(%s)" % obj.typecode)
    else:
        try:
            return _infer_schema(obj)
        except TypeError:
            raise TypeError("not supported type: %s" % type(obj))
--------------------------------------------------

--- SAMPLE #16 ---
DOCSTRING: Infer the schema from dict/namedtuple/object
CODE:
def _infer_schema(row, names=None):
    """Infer the schema from dict/namedtuple/object"""
    if isinstance(row, dict):
        items = sorted(row.items())

    elif isinstance(row, (tuple, list)):
        if hasattr(row, "__fields__"):  # Row
            items = zip(row.__fields__, tuple(row))
        elif hasattr(row, "_fields"):  # namedtuple
            items = zip(row._fields, tuple(row))
        else:
            if names is None:
                names = ['_%d' % i for i in range(1, len(row) + 1)]
            elif len(names) < len(row):
                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))
            items = zip(names, row)

    elif hasattr(row, "__dict__"):  # object
        items = sorted(row.__dict__.items())

    else:
        raise TypeError("Can not infer schema for type: %s" % type(row))

    fields = [StructField(k, _infer_type(v), True) for k, v in items]
    return StructType(fields)
--------------------------------------------------

--- SAMPLE #17 ---
DOCSTRING: Return whether there is NullType in `dt` or not
CODE:
def _has_nulltype(dt):
    """ Return whether there is NullType in `dt` or not """
    if isinstance(dt, StructType):
        return any(_has_nulltype(f.dataType) for f in dt.fields)
    elif isinstance(dt, ArrayType):
        return _has_nulltype((dt.elementType))
    elif isinstance(dt, MapType):
        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)
    else:
        return isinstance(dt, NullType)
--------------------------------------------------

--- SAMPLE #18 ---
DOCSTRING: Create a converter to drop the names of fields in obj
CODE:
def _create_converter(dataType):
    """Create a converter to drop the names of fields in obj """
    if not _need_converter(dataType):
        return lambda x: x

    if isinstance(dataType, ArrayType):
        conv = _create_converter(dataType.elementType)
        return lambda row: [conv(v) for v in row]

    elif isinstance(dataType, MapType):
        kconv = _create_converter(dataType.keyType)
        vconv = _create_converter(dataType.valueType)
        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())

    elif isinstance(dataType, NullType):
        return lambda x: None

    elif not isinstance(dataType, StructType):
        return lambda x: x

    # dataType must be StructType
    names = [f.name for f in dataType.fields]
    converters = [_create_converter(f.dataType) for f in dataType.fields]
    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)

    def convert_struct(obj):
        if obj is None:
            return

        if isinstance(obj, (tuple, list)):
            if convert_fields:
                return tuple(conv(v) for v, conv in zip(obj, converters))
            else:
                return tuple(obj)

        if isinstance(obj, dict):
            d = obj
        elif hasattr(obj, "__dict__"):  # object
            d = obj.__dict__
        else:
            raise TypeError("Unexpected obj type: %s" % type(obj))

        if convert_fields:
            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])
        else:
            return tuple([d.get(name) for name in names])

    return convert_struct
--------------------------------------------------

--- SAMPLE #19 ---
DOCSTRING: Make a verifier that checks the type of obj against dataType and raises a TypeError if they do
    not match.

    This verifier also checks the value of obj against datatype and raises a ValueError if it's not
    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is
    not checked, so it will become infinity when cast to Java float if it overflows.

    >>> _make_type_verifier(StructType([]))(None)
    >>> _make_type_verifier(StringType())("")
    >>> _make_type_verifier(LongType())(0)
    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))
    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TypeError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})
    >>> _make_type_verifier(StructType([]))(())
    >>> _make_type_verifier(StructType([]))([])
    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> # Check if numeric values are within the allowed range.
    >>> _make_type_verifier(ByteType())(12)
    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(
    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> schema = StructType().add("a", IntegerType()).add("b", StringType(), False)
    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
CODE:
def _make_type_verifier(dataType, nullable=True, name=None):
    """
    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do
    not match.

    This verifier also checks the value of obj against datatype and raises a ValueError if it's not
    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is
    not checked, so it will become infinity when cast to Java float if it overflows.

    >>> _make_type_verifier(StructType([]))(None)
    >>> _make_type_verifier(StringType())("")
    >>> _make_type_verifier(LongType())(0)
    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))
    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TypeError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})
    >>> _make_type_verifier(StructType([]))(())
    >>> _make_type_verifier(StructType([]))([])
    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> # Check if numeric values are within the allowed range.
    >>> _make_type_verifier(ByteType())(12)
    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(
    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> schema = StructType().add("a", IntegerType()).add("b", StringType(), False)
    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    """

    if name is None:
        new_msg = lambda msg: msg
        new_name = lambda n: "field %s" % n
    else:
        new_msg = lambda msg: "%s: %s" % (name, msg)
        new_name = lambda n: "field %s in %s" % (n, name)

    def verify_nullability(obj):
        if obj is None:
            if nullable:
                return True
            else:
                raise ValueError(new_msg("This field is not nullable, but got None"))
        else:
            return False

    _type = type(dataType)

    def assert_acceptable_types(obj):
        assert _type in _acceptable_types, \
            new_msg("unknown datatype: %s for object %r" % (dataType, obj))

    def verify_acceptable_types(obj):
        # subclass of them can not be fromInternal in JVM
        if type(obj) not in _acceptable_types[_type]:
            raise TypeError(new_msg("%s can not accept object %r in type %s"
                                    % (dataType, obj, type(obj))))

    if isinstance(dataType, StringType):
        # StringType can work with any types
        verify_value = lambda _: _

    elif isinstance(dataType, UserDefinedType):
        verifier = _make_type_verifier(dataType.sqlType(), name=name)

        def verify_udf(obj):
            if not (hasattr(obj, '__UDT__') and obj.__UDT__ == dataType):
                raise ValueError(new_msg("%r is not an instance of type %r" % (obj, dataType)))
            verifier(dataType.toInternal(obj))

        verify_value = verify_udf

    elif isinstance(dataType, ByteType):
        def verify_byte(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -128 or obj > 127:
                raise ValueError(new_msg("object of ByteType out of range, got: %s" % obj))

        verify_value = verify_byte

    elif isinstance(dataType, ShortType):
        def verify_short(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -32768 or obj > 32767:
                raise ValueError(new_msg("object of ShortType out of range, got: %s" % obj))

        verify_value = verify_short

    elif isinstance(dataType, IntegerType):
        def verify_integer(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -2147483648 or obj > 2147483647:
                raise ValueError(
                    new_msg("object of IntegerType out of range, got: %s" % obj))

        verify_value = verify_integer

    elif isinstance(dataType, ArrayType):
        element_verifier = _make_type_verifier(
            dataType.elementType, dataType.containsNull, name="element in array %s" % name)

        def verify_array(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            for i in obj:
                element_verifier(i)

        verify_value = verify_array

    elif isinstance(dataType, MapType):
        key_verifier = _make_type_verifier(dataType.keyType, False, name="key of map %s" % name)
        value_verifier = _make_type_verifier(
            dataType.valueType, dataType.valueContainsNull, name="value of map %s" % name)

        def verify_map(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            for k, v in obj.items():
                key_verifier(k)
                value_verifier(v)

        verify_value = verify_map

    elif isinstance(dataType, StructType):
        verifiers = []
        for f in dataType.fields:
            verifier = _make_type_verifier(f.dataType, f.nullable, name=new_name(f.name))
            verifiers.append((f.name, verifier))

        def verify_struct(obj):
            assert_acceptable_types(obj)

            if isinstance(obj, dict):
                for f, verifier in verifiers:
                    verifier(obj.get(f))
            elif isinstance(obj, Row) and getattr(obj, "__from_dict__", False):
                # the order in obj could be different than dataType.fields
                for f, verifier in verifiers:
                    verifier(obj[f])
            elif isinstance(obj, (tuple, list)):
                if len(obj) != len(verifiers):
                    raise ValueError(
                        new_msg("Length of object (%d) does not match with "
                                "length of fields (%d)" % (len(obj), len(verifiers))))
                for v, (_, verifier) in zip(obj, verifiers):
                    verifier(v)
            elif hasattr(obj, "__dict__"):
                d = obj.__dict__
                for f, verifier in verifiers:
                    verifier(d.get(f))
            else:
                raise TypeError(new_msg("StructType can not accept object %r in type %s"
                                        % (obj, type(obj))))
        verify_value = verify_struct

    else:
        def verify_default(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)

        verify_value = verify_default

    def verify(obj):
        if not verify_nullability(obj):
            verify_value(obj)

    return verify
--------------------------------------------------

--- SAMPLE #20 ---
DOCSTRING: Convert Spark data type to pyarrow type
CODE:
def to_arrow_type(dt):
    """ Convert Spark data type to pyarrow type
    """
    import pyarrow as pa
    if type(dt) == BooleanType:
        arrow_type = pa.bool_()
    elif type(dt) == ByteType:
        arrow_type = pa.int8()
    elif type(dt) == ShortType:
        arrow_type = pa.int16()
    elif type(dt) == IntegerType:
        arrow_type = pa.int32()
    elif type(dt) == LongType:
        arrow_type = pa.int64()
    elif type(dt) == FloatType:
        arrow_type = pa.float32()
    elif type(dt) == DoubleType:
        arrow_type = pa.float64()
    elif type(dt) == DecimalType:
        arrow_type = pa.decimal128(dt.precision, dt.scale)
    elif type(dt) == StringType:
        arrow_type = pa.string()
    elif type(dt) == BinaryType:
        arrow_type = pa.binary()
    elif type(dt) == DateType:
        arrow_type = pa.date32()
    elif type(dt) == TimestampType:
        # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read
        arrow_type = pa.timestamp('us', tz='UTC')
    elif type(dt) == ArrayType:
        if type(dt.elementType) in [StructType, TimestampType]:
            raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
        arrow_type = pa.list_(to_arrow_type(dt.elementType))
    elif type(dt) == StructType:
        if any(type(field.dataType) == StructType for field in dt):
            raise TypeError("Nested StructType not supported in conversion to Arrow")
        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
                  for field in dt]
        arrow_type = pa.struct(fields)
    else:
        raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
    return arrow_type
--------------------------------------------------

--- SAMPLE #21 ---
DOCSTRING: Convert a schema from Spark to Arrow
CODE:
def to_arrow_schema(schema):
    """ Convert a schema from Spark to Arrow
    """
    import pyarrow as pa
    fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
              for field in schema]
    return pa.schema(fields)
--------------------------------------------------

--- SAMPLE #22 ---
DOCSTRING: Convert pyarrow type to Spark data type.
CODE:
def from_arrow_type(at):
    """ Convert pyarrow type to Spark data type.
    """
    import pyarrow.types as types
    if types.is_boolean(at):
        spark_type = BooleanType()
    elif types.is_int8(at):
        spark_type = ByteType()
    elif types.is_int16(at):
        spark_type = ShortType()
    elif types.is_int32(at):
        spark_type = IntegerType()
    elif types.is_int64(at):
        spark_type = LongType()
    elif types.is_float32(at):
        spark_type = FloatType()
    elif types.is_float64(at):
        spark_type = DoubleType()
    elif types.is_decimal(at):
        spark_type = DecimalType(precision=at.precision, scale=at.scale)
    elif types.is_string(at):
        spark_type = StringType()
    elif types.is_binary(at):
        spark_type = BinaryType()
    elif types.is_date32(at):
        spark_type = DateType()
    elif types.is_timestamp(at):
        spark_type = TimestampType()
    elif types.is_list(at):
        if types.is_timestamp(at.value_type):
            raise TypeError("Unsupported type in conversion from Arrow: " + str(at))
        spark_type = ArrayType(from_arrow_type(at.value_type))
    elif types.is_struct(at):
        if any(types.is_struct(field.type) for field in at):
            raise TypeError("Nested StructType not supported in conversion from Arrow: " + str(at))
        return StructType(
            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
             for field in at])
    else:
        raise TypeError("Unsupported type in conversion from Arrow: " + str(at))
    return spark_type
--------------------------------------------------

--- SAMPLE #23 ---
DOCSTRING: Convert schema from Arrow to Spark.
CODE:
def from_arrow_schema(arrow_schema):
    """ Convert schema from Arrow to Spark.
    """
    return StructType(
        [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
         for field in arrow_schema])
--------------------------------------------------

--- SAMPLE #24 ---
DOCSTRING: Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.

    If the input series is not a timestamp series, then the same series is returned. If the input
    series is a timestamp series, then a converted series is returned.

    :param s: pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series that have been converted to tz-naive
CODE:
def _check_series_localize_timestamps(s, timezone):
    """
    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.

    If the input series is not a timestamp series, then the same series is returned. If the input
    series is a timestamp series, then a converted series is returned.

    :param s: pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series that have been converted to tz-naive
    """
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    from pandas.api.types import is_datetime64tz_dtype
    tz = timezone or _get_local_timezone()
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(tz).dt.tz_localize(None)
    else:
        return s
--------------------------------------------------

--- SAMPLE #25 ---
DOCSTRING: Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone

    :param pdf: pandas.DataFrame
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive
CODE:
def _check_dataframe_localize_timestamps(pdf, timezone):
    """
    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone

    :param pdf: pandas.DataFrame
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive
    """
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    for column, series in pdf.iteritems():
        pdf[column] = _check_series_localize_timestamps(series, timezone)
    return pdf
--------------------------------------------------

--- SAMPLE #26 ---
DOCSTRING: Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for
    Spark internal storage

    :param s: a pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone
CODE:
def _check_series_convert_timestamps_internal(s, timezone):
    """
    Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for
    Spark internal storage

    :param s: a pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone
    """
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64_dtype(s.dtype):
        # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive
        # timestamp is during the hour when the clock is adjusted backward during due to
        # daylight saving time (dst).
        # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to
        # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize
        # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either
        # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).
        #
        # Here we explicit choose to use standard time. This matches the default behavior of
        # pytz.
        #
        # Here are some code to help understand this behavior:
        # >>> import datetime
        # >>> import pandas as pd
        # >>> import pytz
        # >>>
        # >>> t = datetime.datetime(2015, 11, 1, 1, 30)
        # >>> ts = pd.Series([t])
        # >>> tz = pytz.timezone('America/New_York')
        # >>>
        # >>> ts.dt.tz_localize(tz, ambiguous=True)
        # 0   2015-11-01 01:30:00-04:00
        # dtype: datetime64[ns, America/New_York]
        # >>>
        # >>> ts.dt.tz_localize(tz, ambiguous=False)
        # 0   2015-11-01 01:30:00-05:00
        # dtype: datetime64[ns, America/New_York]
        # >>>
        # >>> str(tz.localize(t))
        # '2015-11-01 01:30:00-05:00'
        tz = timezone or _get_local_timezone()
        return s.dt.tz_localize(tz, ambiguous=False).dt.tz_convert('UTC')
    elif is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert('UTC')
    else:
        return s
--------------------------------------------------

--- SAMPLE #27 ---
DOCSTRING: Convert timestamp to timezone-naive in the specified timezone or local timezone

    :param s: a pandas.Series
    :param from_timezone: the timezone to convert from. if None then use local timezone
    :param to_timezone: the timezone to convert to. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been converted to tz-naive
CODE:
def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):
    """
    Convert timestamp to timezone-naive in the specified timezone or local timezone

    :param s: a pandas.Series
    :param from_timezone: the timezone to convert from. if None then use local timezone
    :param to_timezone: the timezone to convert to. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been converted to tz-naive
    """
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    import pandas as pd
    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype
    from_tz = from_timezone or _get_local_timezone()
    to_tz = to_timezone or _get_local_timezone()
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(to_tz).dt.tz_localize(None)
    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:
        # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.
        return s.apply(
            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)
            if ts is not pd.NaT else pd.NaT)
    else:
        return s
--------------------------------------------------

--- SAMPLE #28 ---
DOCSTRING: Construct a StructType by adding new elements to it to define the schema. The method accepts
        either:

            a) A single parameter which is a StructField object.
            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),
               metadata(optional). The data_type parameter may be either a String or a
               DataType object.

        >>> struct1 = StructType().add("f1", StringType(), True).add("f2", StringType(), True, None)
        >>> struct2 = StructType([StructField("f1", StringType(), True), \\
        ...     StructField("f2", StringType(), True, None)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(StructField("f1", StringType(), True))
        >>> struct2 = StructType([StructField("f1", StringType(), True)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add("f1", "string", True)
        >>> struct2 = StructType([StructField("f1", StringType(), True)])
        >>> struct1 == struct2
        True

        :param field: Either the name of the field or a StructField object
        :param data_type: If present, the DataType of the StructField to create
        :param nullable: Whether the field to add should be nullable (default True)
        :param metadata: Any additional metadata (default None)
        :return: a new updated StructType
CODE:
def add(self, field, data_type=None, nullable=True, metadata=None):
        """
        Construct a StructType by adding new elements to it to define the schema. The method accepts
        either:

            a) A single parameter which is a StructField object.
            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),
               metadata(optional). The data_type parameter may be either a String or a
               DataType object.

        >>> struct1 = StructType().add("f1", StringType(), True).add("f2", StringType(), True, None)
        >>> struct2 = StructType([StructField("f1", StringType(), True), \\
        ...     StructField("f2", StringType(), True, None)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(StructField("f1", StringType(), True))
        >>> struct2 = StructType([StructField("f1", StringType(), True)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add("f1", "string", True)
        >>> struct2 = StructType([StructField("f1", StringType(), True)])
        >>> struct1 == struct2
        True

        :param field: Either the name of the field or a StructField object
        :param data_type: If present, the DataType of the StructField to create
        :param nullable: Whether the field to add should be nullable (default True)
        :param metadata: Any additional metadata (default None)
        :return: a new updated StructType
        """
        if isinstance(field, StructField):
            self.fields.append(field)
            self.names.append(field.name)
        else:
            if isinstance(field, str) and data_type is None:
                raise ValueError("Must specify DataType if passing name of struct_field to create.")

            if isinstance(data_type, str):
                data_type_f = _parse_datatype_json_value(data_type)
            else:
                data_type_f = data_type
            self.fields.append(StructField(field, data_type_f, nullable, metadata))
            self.names.append(field)
        # Precalculated list of fields that need conversion with fromInternal/toInternal functions
        self._needConversion = [f.needConversion() for f in self]
        self._needSerializeAnyField = any(self._needConversion)
        return self
--------------------------------------------------

--- SAMPLE #29 ---
DOCSTRING: Cache the sqlType() into class, because it's heavy used in `toInternal`.
CODE:
def _cachedSqlType(cls):
        """
        Cache the sqlType() into class, because it's heavy used in `toInternal`.
        """
        if not hasattr(cls, "_cached_sql_type"):
            cls._cached_sql_type = cls.sqlType()
        return cls._cached_sql_type
--------------------------------------------------

--- SAMPLE #30 ---
DOCSTRING: Return as an dict

        :param recursive: turns the nested Row as dict (default: False).

        >>> Row(name="Alice", age=11).asDict() == {'name': 'Alice', 'age': 11}
        True
        >>> row = Row(key=1, value=Row(name='a', age=2))
        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}
        True
        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}
        True
CODE:
def asDict(self, recursive=False):
        """
        Return as an dict

        :param recursive: turns the nested Row as dict (default: False).

        >>> Row(name="Alice", age=11).asDict() == {'name': 'Alice', 'age': 11}
        True
        >>> row = Row(key=1, value=Row(name='a', age=2))
        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}
        True
        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}
        True
        """
        if not hasattr(self, "__fields__"):
            raise TypeError("Cannot convert a Row class into dict")

        if recursive:
            def conv(obj):
                if isinstance(obj, Row):
                    return obj.asDict(True)
                elif isinstance(obj, list):
                    return [conv(o) for o in obj]
                elif isinstance(obj, dict):
                    return dict((k, conv(v)) for k, v in obj.items())
                else:
                    return obj
            return dict(zip(self.__fields__, (conv(o) for o in self)))
        else:
            return dict(zip(self.__fields__, self))
--------------------------------------------------

--- SAMPLE #31 ---
DOCSTRING: Gets summary (e.g. residuals, mse, r-squared ) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
CODE:
def summary(self):
        """
        Gets summary (e.g. residuals, mse, r-squared ) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
        """
        if self.hasSummary:
            return LinearRegressionTrainingSummary(super(LinearRegressionModel, self).summary)
        else:
            raise RuntimeError("No training summary available for this %s" %
                               self.__class__.__name__)
--------------------------------------------------

--- SAMPLE #32 ---
DOCSTRING: Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
CODE:
def evaluate(self, dataset):
        """
        Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
        """
        if not isinstance(dataset, DataFrame):
            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))
        java_lr_summary = self._call_java("evaluate", dataset)
        return LinearRegressionSummary(java_lr_summary)
--------------------------------------------------

--- SAMPLE #33 ---
DOCSTRING: Gets summary (e.g. residuals, deviance, pValues) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
CODE:
def summary(self):
        """
        Gets summary (e.g. residuals, deviance, pValues) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
        """
        if self.hasSummary:
            return GeneralizedLinearRegressionTrainingSummary(
                super(GeneralizedLinearRegressionModel, self).summary)
        else:
            raise RuntimeError("No training summary available for this %s" %
                               self.__class__.__name__)
--------------------------------------------------

--- SAMPLE #34 ---
DOCSTRING: Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
CODE:
def evaluate(self, dataset):
        """
        Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
        """
        if not isinstance(dataset, DataFrame):
            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))
        java_glr_summary = self._call_java("evaluate", dataset)
        return GeneralizedLinearRegressionSummary(java_glr_summary)
--------------------------------------------------

--- SAMPLE #35 ---
DOCSTRING: Get all the directories
CODE:
def _get_local_dirs(sub):
    """ Get all the directories """
    path = os.environ.get("SPARK_LOCAL_DIRS", "/tmp")
    dirs = path.split(",")
    if len(dirs) > 1:
        # different order in different processes and instances
        rnd = random.Random(os.getpid() + id(dirs))
        random.shuffle(dirs, rnd.random)
    return [os.path.join(d, "python", str(os.getpid()), sub) for d in dirs]
--------------------------------------------------

--- SAMPLE #36 ---
DOCSTRING: Choose one directory for spill by number n
CODE:
def _get_spill_dir(self, n):
        """ Choose one directory for spill by number n """
        return os.path.join(self.localdirs[n % len(self.localdirs)], str(n))
--------------------------------------------------

--- SAMPLE #37 ---
DOCSTRING: Combine the items by creator and combiner
CODE:
def mergeValues(self, iterator):
        """ Combine the items by creator and combiner """
        # speedup attribute lookup
        creator, comb = self.agg.createCombiner, self.agg.mergeValue
        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch
        limit = self.memory_limit

        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else creator(v)

            c += 1
            if c >= batch:
                if get_used_memory() >= limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5

        if get_used_memory() >= limit:
            self._spill()
--------------------------------------------------

--- SAMPLE #38 ---
DOCSTRING: Merge (K,V) pair by mergeCombiner
CODE:
def mergeCombiners(self, iterator, limit=None):
        """ Merge (K,V) pair by mergeCombiner """
        if limit is None:
            limit = self.memory_limit
        # speedup attribute lookup
        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size
        c, data, pdata, batch = 0, self.data, self.pdata, self.batch
        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else v
            if not limit:
                continue

            c += objsize(v)
            if c > batch:
                if get_used_memory() > limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5

        if limit and get_used_memory() >= limit:
            self._spill()
--------------------------------------------------

--- SAMPLE #39 ---
DOCSTRING: dump already partitioned data into disks.

        It will dump the data in batch for better performance.
CODE:
def _spill(self):
        """
        dump already partitioned data into disks.

        It will dump the data in batch for better performance.
        """
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)

        used_memory = get_used_memory()
        if not self.pdata:
            # The data has not been partitioned, it will iterator the
            # dataset once, write them into different files, has no
            # additional memory. It only called when the memory goes
            # above limit at the first time.

            # open all the files for writing
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]

            for k, v in self.data.items():
                h = self._partition(k)
                # put one item in batch, make it compatible with load_stream
                # it will increase the memory if dump them in batch
                self.serializer.dump_stream([(k, v)], streams[h])

            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()

            self.data.clear()
            self.pdata.extend([{} for i in range(self.partitions)])

        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, "wb") as f:
                    # dump items in batch
                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)

        self.spills += 1
        gc.collect()  # release the memory as much as possible
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #40 ---
DOCSTRING: Return all merged items as iterator
CODE:
def items(self):
        """ Return all merged items as iterator """
        if not self.pdata and not self.spills:
            return iter(self.data.items())
        return self._external_items()
--------------------------------------------------

--- SAMPLE #41 ---
DOCSTRING: Return all partitioned items as iterator
CODE:
def _external_items(self):
        """ Return all partitioned items as iterator """
        assert not self.data
        if any(self.pdata):
            self._spill()
        # disable partitioning and spilling when merge combiners from disk
        self.pdata = []

        try:
            for i in range(self.partitions):
                for v in self._merged_items(i):
                    yield v
                self.data.clear()

                # remove the merged partition
                for j in range(self.spills):
                    path = self._get_spill_dir(j)
                    os.remove(os.path.join(path, str(i)))
        finally:
            self._cleanup()
--------------------------------------------------

--- SAMPLE #42 ---
DOCSTRING: merge the partitioned items and return the as iterator

        If one partition can not be fit in memory, then them will be
        partitioned and merged recursively.
CODE:
def _recursive_merged_items(self, index):
        """
        merge the partitioned items and return the as iterator

        If one partition can not be fit in memory, then them will be
        partitioned and merged recursively.
        """
        subdirs = [os.path.join(d, "parts", str(index)) for d in self.localdirs]
        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,
                           self.scale * self.partitions, self.partitions, self.batch)
        m.pdata = [{} for _ in range(self.partitions)]
        limit = self._next_limit()

        for j in range(self.spills):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb') as f:
                m.mergeCombiners(self.serializer.load_stream(f), 0)

            if get_used_memory() > limit:
                m._spill()
                limit = self._next_limit()

        return m._external_items()
--------------------------------------------------

--- SAMPLE #43 ---
DOCSTRING: Choose one directory for spill by number n
CODE:
def _get_path(self, n):
        """ Choose one directory for spill by number n """
        d = self.local_dirs[n % len(self.local_dirs)]
        if not os.path.exists(d):
            os.makedirs(d)
        return os.path.join(d, str(n))
--------------------------------------------------

--- SAMPLE #44 ---
DOCSTRING: Sort the elements in iterator, do external sort when the memory
        goes above the limit.
CODE:
def sorted(self, iterator, key=None, reverse=False):
        """
        Sort the elements in iterator, do external sort when the memory
        goes above the limit.
        """
        global MemoryBytesSpilled, DiskBytesSpilled
        batch, limit = 100, self._next_limit()
        chunks, current_chunk = [], []
        iterator = iter(iterator)
        while True:
            # pick elements in batch
            chunk = list(itertools.islice(iterator, batch))
            current_chunk.extend(chunk)
            if len(chunk) < batch:
                break

            used_memory = get_used_memory()
            if used_memory > limit:
                # sort them inplace will save memory
                current_chunk.sort(key=key, reverse=reverse)
                path = self._get_path(len(chunks))
                with open(path, 'wb') as f:
                    self.serializer.dump_stream(current_chunk, f)

                def load(f):
                    for v in self.serializer.load_stream(f):
                        yield v
                    # close the file explicit once we consume all the items
                    # to avoid ResourceWarning in Python3
                    f.close()
                chunks.append(load(open(path, 'rb')))
                current_chunk = []
                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
                DiskBytesSpilled += os.path.getsize(path)
                os.unlink(path)  # data will be deleted after close

            elif not chunks:
                batch = min(int(batch * 1.5), 10000)

        current_chunk.sort(key=key, reverse=reverse)
        if not chunks:
            return current_chunk

        if current_chunk:
            chunks.append(iter(current_chunk))

        return heapq.merge(chunks, key=key, reverse=reverse)
--------------------------------------------------

--- SAMPLE #45 ---
DOCSTRING: dump the values into disk
CODE:
def _spill(self):
        """ dump the values into disk """
        global MemoryBytesSpilled, DiskBytesSpilled
        if self._file is None:
            self._open_file()

        used_memory = get_used_memory()
        pos = self._file.tell()
        self._ser.dump_stream(self.values, self._file)
        self.values = []
        gc.collect()
        DiskBytesSpilled += self._file.tell() - pos
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #46 ---
DOCSTRING: dump already partitioned data into disks.
CODE:
def _spill(self):
        """
        dump already partitioned data into disks.
        """
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)

        used_memory = get_used_memory()
        if not self.pdata:
            # The data has not been partitioned, it will iterator the
            # data once, write them into different files, has no
            # additional memory. It only called when the memory goes
            # above limit at the first time.

            # open all the files for writing
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]

            # If the number of keys is small, then the overhead of sort is small
            # sort them before dumping into disks
            self._sorted = len(self.data) < self.SORT_KEY_LIMIT
            if self._sorted:
                self.serializer = self.flattened_serializer()
                for k in sorted(self.data.keys()):
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, self.data[k])], streams[h])
            else:
                for k, v in self.data.items():
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, v)], streams[h])

            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()

            self.data.clear()
            # self.pdata is cached in `mergeValues` and `mergeCombiners`
            self.pdata.extend([{} for i in range(self.partitions)])

        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, "wb") as f:
                    # dump items in batch
                    if self._sorted:
                        # sort by key only (stable)
                        sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))
                        self.serializer.dump_stream(sorted_items, f)
                    else:
                        self.serializer.dump_stream(self.pdata[i].items(), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)

        self.spills += 1
        gc.collect()  # release the memory as much as possible
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
--------------------------------------------------

--- SAMPLE #47 ---
DOCSTRING: load a partition from disk, then sort and group by key
CODE:
def _merge_sorted_items(self, index):
        """ load a partition from disk, then sort and group by key """
        def load_partition(j):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb', 65536) as f:
                for v in self.serializer.load_stream(f):
                    yield v

        disk_items = [load_partition(j) for j in range(self.spills)]

        if self._sorted:
            # all the partitions are already sorted
            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))

        else:
            # Flatten the combined values, so it will not consume huge
            # memory during merging sort.
            ser = self.flattened_serializer()
            sorter = ExternalSorter(self.memory_limit, ser)
            sorted_items = sorter.sorted(itertools.chain(*disk_items),
                                         key=operator.itemgetter(0))
        return ((k, vs) for k, vs in GroupByKey(sorted_items))
--------------------------------------------------

--- SAMPLE #48 ---
DOCSTRING: Called by a worker process after the fork().
CODE:
def worker(sock, authenticated):
    """
    Called by a worker process after the fork().
    """
    signal.signal(SIGHUP, SIG_DFL)
    signal.signal(SIGCHLD, SIG_DFL)
    signal.signal(SIGTERM, SIG_DFL)
    # restore the handler for SIGINT,
    # it's useful for debugging (show the stacktrace before exit)
    signal.signal(SIGINT, signal.default_int_handler)

    # Read the socket using fdopen instead of socket.makefile() because the latter
    # seems to be very slow; note that we need to dup() the file descriptor because
    # otherwise writes also cause a seek that makes us miss data on the read side.
    infile = os.fdopen(os.dup(sock.fileno()), "rb", 65536)
    outfile = os.fdopen(os.dup(sock.fileno()), "wb", 65536)

    if not authenticated:
        client_secret = UTF8Deserializer().loads(infile)
        if os.environ["PYTHON_WORKER_FACTORY_SECRET"] == client_secret:
            write_with_length("ok".encode("utf-8"), outfile)
            outfile.flush()
        else:
            write_with_length("err".encode("utf-8"), outfile)
            outfile.flush()
            sock.close()
            return 1

    exit_code = 0
    try:
        worker_main(infile, outfile)
    except SystemExit as exc:
        exit_code = compute_real_exit_code(exc.code)
    finally:
        try:
            outfile.flush()
        except Exception:
            pass
    return exit_code
--------------------------------------------------

--- SAMPLE #49 ---
DOCSTRING: This function returns consistent hash code for builtin types, especially
    for None and tuple with None.

    The algorithm is similar to that one used by CPython 2.7

    >>> portable_hash(None)
    0
    >>> portable_hash((None, 1)) & 0xffffffff
    219750521
CODE:
def portable_hash(x):
    """
    This function returns consistent hash code for builtin types, especially
    for None and tuple with None.

    The algorithm is similar to that one used by CPython 2.7

    >>> portable_hash(None)
    0
    >>> portable_hash((None, 1)) & 0xffffffff
    219750521
    """

    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:
        raise Exception("Randomness of hash of string should be disabled via PYTHONHASHSEED")

    if x is None:
        return 0
    if isinstance(x, tuple):
        h = 0x345678
        for i in x:
            h ^= portable_hash(i)
            h *= 1000003
            h &= sys.maxsize
        h ^= len(x)
        if h == -1:
            h = -2
        return int(h)
    return hash(x)
--------------------------------------------------

--- SAMPLE #50 ---
DOCSTRING: Parse a memory string in the format supported by Java (e.g. 1g, 200m) and
    return the value in MiB

    >>> _parse_memory("256m")
    256
    >>> _parse_memory("2g")
    2048
CODE:
def _parse_memory(s):
    """
    Parse a memory string in the format supported by Java (e.g. 1g, 200m) and
    return the value in MiB

    >>> _parse_memory("256m")
    256
    >>> _parse_memory("2g")
    2048
    """
    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}
    if s[-1].lower() not in units:
        raise ValueError("invalid format: " + s)
    return int(float(s[:-1]) * units[s[-1].lower()])
--------------------------------------------------

--- SAMPLE #51 ---
DOCSTRING: Ignore the 'u' prefix of string in doc tests, to make it works
    in both python 2 and 3
CODE:
def ignore_unicode_prefix(f):
    """
    Ignore the 'u' prefix of string in doc tests, to make it works
    in both python 2 and 3
    """
    if sys.version >= '3':
        # the representation of unicode string in Python 3 does not have prefix 'u',
        # so remove the prefix 'u' for doc tests
        literal_re = re.compile(r"(\W|^)[uU](['])", re.UNICODE)
        f.__doc__ = literal_re.sub(r'\1\2', f.__doc__)
    return f
--------------------------------------------------

--- SAMPLE #52 ---
DOCSTRING: Persist this RDD with the default storage level (C{MEMORY_ONLY}).
CODE:
def cache(self):
        """
        Persist this RDD with the default storage level (C{MEMORY_ONLY}).
        """
        self.is_cached = True
        self.persist(StorageLevel.MEMORY_ONLY)
        return self
--------------------------------------------------

--- SAMPLE #53 ---
DOCSTRING: Set this RDD's storage level to persist its values across operations
        after the first time it is computed. This can only be used to assign
        a new storage level if the RDD does not have a storage level set yet.
        If no storage level is specified defaults to (C{MEMORY_ONLY}).

        >>> rdd = sc.parallelize(["b", "a", "c"])
        >>> rdd.persist().is_cached
        True
CODE:
def persist(self, storageLevel=StorageLevel.MEMORY_ONLY):
        """
        Set this RDD's storage level to persist its values across operations
        after the first time it is computed. This can only be used to assign
        a new storage level if the RDD does not have a storage level set yet.
        If no storage level is specified defaults to (C{MEMORY_ONLY}).

        >>> rdd = sc.parallelize(["b", "a", "c"])
        >>> rdd.persist().is_cached
        True
        """
        self.is_cached = True
        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
        self._jrdd.persist(javaStorageLevel)
        return self
--------------------------------------------------

--- SAMPLE #54 ---
DOCSTRING: Mark the RDD as non-persistent, and remove all blocks for it from
        memory and disk.

        .. versionchanged:: 3.0.0
           Added optional argument `blocking` to specify whether to block until all
           blocks are deleted.
CODE:
def unpersist(self, blocking=False):
        """
        Mark the RDD as non-persistent, and remove all blocks for it from
        memory and disk.

        .. versionchanged:: 3.0.0
           Added optional argument `blocking` to specify whether to block until all
           blocks are deleted.
        """
        self.is_cached = False
        self._jrdd.unpersist(blocking)
        return self
--------------------------------------------------

--- SAMPLE #55 ---
DOCSTRING: Gets the name of the file to which this RDD was checkpointed

        Not defined if RDD is checkpointed locally.
CODE:
def getCheckpointFile(self):
        """
        Gets the name of the file to which this RDD was checkpointed

        Not defined if RDD is checkpointed locally.
        """
        checkpointFile = self._jrdd.rdd().getCheckpointFile()
        if checkpointFile.isDefined():
            return checkpointFile.get()
--------------------------------------------------

--- SAMPLE #56 ---
DOCSTRING: Return a new RDD by applying a function to each element of this RDD.

        >>> rdd = sc.parallelize(["b", "a", "c"])
        >>> sorted(rdd.map(lambda x: (x, 1)).collect())
        [('a', 1), ('b', 1), ('c', 1)]
CODE:
def map(self, f, preservesPartitioning=False):
        """
        Return a new RDD by applying a function to each element of this RDD.

        >>> rdd = sc.parallelize(["b", "a", "c"])
        >>> sorted(rdd.map(lambda x: (x, 1)).collect())
        [('a', 1), ('b', 1), ('c', 1)]
        """
        def func(_, iterator):
            return map(fail_on_stopiteration(f), iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)
--------------------------------------------------

--- SAMPLE #57 ---
DOCSTRING: Return a new RDD by first applying a function to all elements of this
        RDD, and then flattening the results.

        >>> rdd = sc.parallelize([2, 3, 4])
        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())
        [1, 1, 1, 2, 2, 3]
        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())
        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]
CODE:
def flatMap(self, f, preservesPartitioning=False):
        """
        Return a new RDD by first applying a function to all elements of this
        RDD, and then flattening the results.

        >>> rdd = sc.parallelize([2, 3, 4])
        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())
        [1, 1, 1, 2, 2, 3]
        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())
        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]
        """
        def func(s, iterator):
            return chain.from_iterable(map(fail_on_stopiteration(f), iterator))
        return self.mapPartitionsWithIndex(func, preservesPartitioning)
--------------------------------------------------

--- SAMPLE #58 ---
DOCSTRING: Return a new RDD by applying a function to each partition of this RDD.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
        >>> def f(iterator): yield sum(iterator)
        >>> rdd.mapPartitions(f).collect()
        [3, 7]
CODE:
def mapPartitions(self, f, preservesPartitioning=False):
        """
        Return a new RDD by applying a function to each partition of this RDD.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
        >>> def f(iterator): yield sum(iterator)
        >>> rdd.mapPartitions(f).collect()
        [3, 7]
        """
        def func(s, iterator):
            return f(iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)
--------------------------------------------------

--- SAMPLE #59 ---
DOCSTRING: Deprecated: use mapPartitionsWithIndex instead.

        Return a new RDD by applying a function to each partition of this RDD,
        while tracking the index of the original partition.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
        >>> def f(splitIndex, iterator): yield splitIndex
        >>> rdd.mapPartitionsWithSplit(f).sum()
        6
CODE:
def mapPartitionsWithSplit(self, f, preservesPartitioning=False):
        """
        Deprecated: use mapPartitionsWithIndex instead.

        Return a new RDD by applying a function to each partition of this RDD,
        while tracking the index of the original partition.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
        >>> def f(splitIndex, iterator): yield splitIndex
        >>> rdd.mapPartitionsWithSplit(f).sum()
        6
        """
        warnings.warn("mapPartitionsWithSplit is deprecated; "
                      "use mapPartitionsWithIndex instead", DeprecationWarning, stacklevel=2)
        return self.mapPartitionsWithIndex(f, preservesPartitioning)
--------------------------------------------------

--- SAMPLE #60 ---
DOCSTRING: Return a new RDD containing the distinct elements in this RDD.

        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())
        [1, 2, 3]
CODE:
def distinct(self, numPartitions=None):
        """
        Return a new RDD containing the distinct elements in this RDD.

        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())
        [1, 2, 3]
        """
        return self.map(lambda x: (x, None)) \
                   .reduceByKey(lambda x, _: x, numPartitions) \
                   .map(lambda x: x[0])
--------------------------------------------------

--- SAMPLE #61 ---
DOCSTRING: Return a sampled subset of this RDD.

        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)
        :param fraction: expected size of the sample as a fraction of this RDD's size
            without replacement: probability that each element is chosen; fraction must be [0, 1]
            with replacement: expected number of times each element is chosen; fraction must be >= 0
        :param seed: seed for the random number generator

        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
            count of the given :class:`DataFrame`.

        >>> rdd = sc.parallelize(range(100), 4)
        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14
        True
CODE:
def sample(self, withReplacement, fraction, seed=None):
        """
        Return a sampled subset of this RDD.

        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)
        :param fraction: expected size of the sample as a fraction of this RDD's size
            without replacement: probability that each element is chosen; fraction must be [0, 1]
            with replacement: expected number of times each element is chosen; fraction must be >= 0
        :param seed: seed for the random number generator

        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
            count of the given :class:`DataFrame`.

        >>> rdd = sc.parallelize(range(100), 4)
        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14
        True
        """
        assert fraction >= 0.0, "Negative fraction value: %s" % fraction
        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)
--------------------------------------------------

--- SAMPLE #62 ---
DOCSTRING: Randomly splits this RDD with the provided weights.

        :param weights: weights for splits, will be normalized if they don't sum to 1
        :param seed: random seed
        :return: split RDDs in a list

        >>> rdd = sc.parallelize(range(500), 1)
        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)
        >>> len(rdd1.collect() + rdd2.collect())
        500
        >>> 150 < rdd1.count() < 250
        True
        >>> 250 < rdd2.count() < 350
        True
CODE:
def randomSplit(self, weights, seed=None):
        """
        Randomly splits this RDD with the provided weights.

        :param weights: weights for splits, will be normalized if they don't sum to 1
        :param seed: random seed
        :return: split RDDs in a list

        >>> rdd = sc.parallelize(range(500), 1)
        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)
        >>> len(rdd1.collect() + rdd2.collect())
        500
        >>> 150 < rdd1.count() < 250
        True
        >>> 250 < rdd2.count() < 350
        True
        """
        s = float(sum(weights))
        cweights = [0.0]
        for w in weights:
            cweights.append(cweights[-1] + w / s)
        if seed is None:
            seed = random.randint(0, 2 ** 32 - 1)
        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)
                for lb, ub in zip(cweights, cweights[1:])]
--------------------------------------------------

--- SAMPLE #63 ---
DOCSTRING: Return a fixed-size sampled subset of this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> rdd = sc.parallelize(range(0, 10))
        >>> len(rdd.takeSample(True, 20, 1))
        20
        >>> len(rdd.takeSample(False, 5, 2))
        5
        >>> len(rdd.takeSample(False, 15, 3))
        10
CODE:
def takeSample(self, withReplacement, num, seed=None):
        """
        Return a fixed-size sampled subset of this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> rdd = sc.parallelize(range(0, 10))
        >>> len(rdd.takeSample(True, 20, 1))
        20
        >>> len(rdd.takeSample(False, 5, 2))
        5
        >>> len(rdd.takeSample(False, 15, 3))
        10
        """
        numStDev = 10.0

        if num < 0:
            raise ValueError("Sample size cannot be negative.")
        elif num == 0:
            return []

        initialCount = self.count()
        if initialCount == 0:
            return []

        rand = random.Random(seed)

        if (not withReplacement) and num >= initialCount:
            # shuffle current RDD and return
            samples = self.collect()
            rand.shuffle(samples)
            return samples

        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))
        if num > maxSampleSize:
            raise ValueError(
                "Sample size cannot be greater than %d." % maxSampleSize)

        fraction = RDD._computeFractionForSampleSize(
            num, initialCount, withReplacement)
        samples = self.sample(withReplacement, fraction, seed).collect()

        # If the first sample didn't turn out large enough, keep trying to take samples;
        # this shouldn't happen often because we use a big multiplier for their initial size.
        # See: scala/spark/RDD.scala
        while len(samples) < num:
            # TODO: add log warning for when more than one iteration was run
            seed = rand.randint(0, sys.maxsize)
            samples = self.sample(withReplacement, fraction, seed).collect()

        rand.shuffle(samples)

        return samples[0:num]
--------------------------------------------------

--- SAMPLE #64 ---
DOCSTRING: Returns a sampling rate that guarantees a sample of
        size >= sampleSizeLowerBound 99.99% of the time.

        How the sampling rate is determined:
        Let p = num / total, where num is the sample size and total is the
        total number of data points in the RDD. We're trying to compute
        q > p such that
          - when sampling with replacement, we're drawing each data point
            with prob_i ~ Pois(q), where we want to guarantee
            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to
            total), i.e. the failure rate of not having a sufficiently large
            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient
            to guarantee 0.9999 success rate for num > 12, but we need a
            slightly larger q (9 empirically determined).
          - when sampling without replacement, we're drawing each data point
            with prob_i ~ Binomial(total, fraction) and our choice of q
            guarantees 1-delta, or 0.9999 success rate, where success rate is
            defined the same as in sampling with replacement.
CODE:
def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):
        """
        Returns a sampling rate that guarantees a sample of
        size >= sampleSizeLowerBound 99.99% of the time.

        How the sampling rate is determined:
        Let p = num / total, where num is the sample size and total is the
        total number of data points in the RDD. We're trying to compute
        q > p such that
          - when sampling with replacement, we're drawing each data point
            with prob_i ~ Pois(q), where we want to guarantee
            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to
            total), i.e. the failure rate of not having a sufficiently large
            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient
            to guarantee 0.9999 success rate for num > 12, but we need a
            slightly larger q (9 empirically determined).
          - when sampling without replacement, we're drawing each data point
            with prob_i ~ Binomial(total, fraction) and our choice of q
            guarantees 1-delta, or 0.9999 success rate, where success rate is
            defined the same as in sampling with replacement.
        """
        fraction = float(sampleSizeLowerBound) / total
        if withReplacement:
            numStDev = 5
            if (sampleSizeLowerBound < 12):
                numStDev = 9
            return fraction + numStDev * sqrt(fraction / total)
        else:
            delta = 0.00005
            gamma = - log(delta) / total
            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))
--------------------------------------------------

--- SAMPLE #65 ---
DOCSTRING: Return the union of this RDD and another one.

        >>> rdd = sc.parallelize([1, 1, 2, 3])
        >>> rdd.union(rdd).collect()
        [1, 1, 2, 3, 1, 1, 2, 3]
CODE:
def union(self, other):
        """
        Return the union of this RDD and another one.

        >>> rdd = sc.parallelize([1, 1, 2, 3])
        >>> rdd.union(rdd).collect()
        [1, 1, 2, 3, 1, 1, 2, 3]
        """
        if self._jrdd_deserializer == other._jrdd_deserializer:
            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,
                      self._jrdd_deserializer)
        else:
            # These RDDs contain data in different serialized formats, so we
            # must normalize them to the default serializer.
            self_copy = self._reserialize()
            other_copy = other._reserialize()
            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,
                      self.ctx.serializer)
        if (self.partitioner == other.partitioner and
                self.getNumPartitions() == rdd.getNumPartitions()):
            rdd.partitioner = self.partitioner
        return rdd
--------------------------------------------------

--- SAMPLE #66 ---
DOCSTRING: Return the intersection of this RDD and another one. The output will
        not contain any duplicate elements, even if the input RDDs did.

        .. note:: This method performs a shuffle internally.

        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
        >>> rdd1.intersection(rdd2).collect()
        [1, 2, 3]
CODE:
def intersection(self, other):
        """
        Return the intersection of this RDD and another one. The output will
        not contain any duplicate elements, even if the input RDDs did.

        .. note:: This method performs a shuffle internally.

        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
        >>> rdd1.intersection(rdd2).collect()
        [1, 2, 3]
        """
        return self.map(lambda v: (v, None)) \
            .cogroup(other.map(lambda v: (v, None))) \
            .filter(lambda k_vs: all(k_vs[1])) \
            .keys()
--------------------------------------------------

--- SAMPLE #67 ---
DOCSTRING: Repartition the RDD according to the given partitioner and, within each resulting partition,
        sort records by their keys.

        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])
        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)
        >>> rdd2.glom().collect()
        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]
CODE:
def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,
                                           ascending=True, keyfunc=lambda x: x):
        """
        Repartition the RDD according to the given partitioner and, within each resulting partition,
        sort records by their keys.

        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])
        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)
        >>> rdd2.glom().collect()
        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]
        """
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        memory = _parse_memory(self.ctx._conf.get("spark.python.worker.memory", "512m"))
        serializer = self._jrdd_deserializer

        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))

        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)
--------------------------------------------------

--- SAMPLE #68 ---
DOCSTRING: Sorts this RDD, which is assumed to consist of (key, value) pairs.

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortByKey().first()
        ('1', 3)
        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]
        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])
        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]
CODE:
def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):
        """
        Sorts this RDD, which is assumed to consist of (key, value) pairs.

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortByKey().first()
        ('1', 3)
        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]
        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])
        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]
        """
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        memory = self._memory_limit()
        serializer = self._jrdd_deserializer

        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))

        if numPartitions == 1:
            if self.getNumPartitions() > 1:
                self = self.coalesce(1)
            return self.mapPartitions(sortPartition, True)

        # first compute the boundary of each part via sampling: we want to partition
        # the key-space into bins such that the bins have roughly the same
        # number of (key, value) pairs falling into them
        rddSize = self.count()
        if not rddSize:
            return self  # empty RDD
        maxSampleSize = numPartitions * 20.0  # constant from Spark's RangePartitioner
        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)
        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()
        samples = sorted(samples, key=keyfunc)

        # we have numPartitions many parts but one of the them has
        # an implicit boundary
        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]
                  for i in range(0, numPartitions - 1)]

        def rangePartitioner(k):
            p = bisect.bisect_left(bounds, keyfunc(k))
            if ascending:
                return p
            else:
                return numPartitions - 1 - p

        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)
--------------------------------------------------

--- SAMPLE #69 ---
DOCSTRING: Sorts this RDD by the given keyfunc

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()
        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
CODE:
def sortBy(self, keyfunc, ascending=True, numPartitions=None):
        """
        Sorts this RDD by the given keyfunc

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()
        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        """
        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()
--------------------------------------------------

--- SAMPLE #70 ---
DOCSTRING: Return the Cartesian product of this RDD and another one, that is, the
        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and
        C{b} is in C{other}.

        >>> rdd = sc.parallelize([1, 2])
        >>> sorted(rdd.cartesian(rdd).collect())
        [(1, 1), (1, 2), (2, 1), (2, 2)]
CODE:
def cartesian(self, other):
        """
        Return the Cartesian product of this RDD and another one, that is, the
        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and
        C{b} is in C{other}.

        >>> rdd = sc.parallelize([1, 2])
        >>> sorted(rdd.cartesian(rdd).collect())
        [(1, 1), (1, 2), (2, 1), (2, 2)]
        """
        # Due to batching, we can't use the Java cartesian method.
        deserializer = CartesianDeserializer(self._jrdd_deserializer,
                                             other._jrdd_deserializer)
        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)
--------------------------------------------------

--- SAMPLE #71 ---
DOCSTRING: Return an RDD of grouped items.

        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])
        >>> result = rdd.groupBy(lambda x: x % 2).collect()
        >>> sorted([(x, sorted(y)) for (x, y) in result])
        [(0, [2, 8]), (1, [1, 1, 3, 5])]
CODE:
def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash):
        """
        Return an RDD of grouped items.

        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])
        >>> result = rdd.groupBy(lambda x: x % 2).collect()
        >>> sorted([(x, sorted(y)) for (x, y) in result])
        [(0, [2, 8]), (1, [1, 1, 3, 5])]
        """
        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)
--------------------------------------------------

--- SAMPLE #72 ---
DOCSTRING: Return an RDD created by piping elements to a forked external process.

        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()
        [u'1', u'2', u'', u'3']

        :param checkCode: whether or not to check the return value of the shell command.
CODE:
def pipe(self, command, env=None, checkCode=False):
        """
        Return an RDD created by piping elements to a forked external process.

        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()
        [u'1', u'2', u'', u'3']

        :param checkCode: whether or not to check the return value of the shell command.
        """
        if env is None:
            env = dict()

        def func(iterator):
            pipe = Popen(
                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)

            def pipe_objs(out):
                for obj in iterator:
                    s = unicode(obj).rstrip('\n') + '\n'
                    out.write(s.encode('utf-8'))
                out.close()
            Thread(target=pipe_objs, args=[pipe.stdin]).start()

            def check_return_code():
                pipe.wait()
                if checkCode and pipe.returncode:
                    raise Exception("Pipe function `%s' exited "
                                    "with error code %d" % (command, pipe.returncode))
                else:
                    for i in range(0):
                        yield i
            return (x.rstrip(b'\n').decode('utf-8') for x in
                    chain(iter(pipe.stdout.readline, b''), check_return_code()))
        return self.mapPartitions(func)
--------------------------------------------------

--- SAMPLE #73 ---
DOCSTRING: Applies a function to all elements of this RDD.

        >>> def f(x): print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)
CODE:
def foreach(self, f):
        """
        Applies a function to all elements of this RDD.

        >>> def f(x): print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)
        """
        f = fail_on_stopiteration(f)

        def processPartition(iterator):
            for x in iterator:
                f(x)
            return iter([])
        self.mapPartitions(processPartition).count()
--------------------------------------------------

--- SAMPLE #74 ---
DOCSTRING: Applies a function to each partition of this RDD.

        >>> def f(iterator):
        ...     for x in iterator:
        ...          print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)
CODE:
def foreachPartition(self, f):
        """
        Applies a function to each partition of this RDD.

        >>> def f(iterator):
        ...     for x in iterator:
        ...          print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)
        """
        def func(it):
            r = f(it)
            try:
                return iter(r)
            except TypeError:
                return iter([])
        self.mapPartitions(func).count()
--------------------------------------------------

--- SAMPLE #75 ---
DOCSTRING: Return a list that contains all of the elements in this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.
CODE:
def collect(self):
        """
        Return a list that contains all of the elements in this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.
        """
        with SCCallSiteSync(self.context) as css:
            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
        return list(_load_from_socket(sock_info, self._jrdd_deserializer))
--------------------------------------------------

--- SAMPLE #76 ---
DOCSTRING: Reduces the elements of this RDD using the specified commutative and
        associative binary operator. Currently reduces partitions locally.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)
        15
        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)
        10
        >>> sc.parallelize([]).reduce(add)
        Traceback (most recent call last):
            ...
        ValueError: Can not reduce() empty RDD
CODE:
def reduce(self, f):
        """
        Reduces the elements of this RDD using the specified commutative and
        associative binary operator. Currently reduces partitions locally.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)
        15
        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)
        10
        >>> sc.parallelize([]).reduce(add)
        Traceback (most recent call last):
            ...
        ValueError: Can not reduce() empty RDD
        """
        f = fail_on_stopiteration(f)

        def func(iterator):
            iterator = iter(iterator)
            try:
                initial = next(iterator)
            except StopIteration:
                return
            yield reduce(f, iterator, initial)

        vals = self.mapPartitions(func).collect()
        if vals:
            return reduce(f, vals)
        raise ValueError("Can not reduce() empty RDD")
--------------------------------------------------

--- SAMPLE #77 ---
DOCSTRING: Reduces the elements of this RDD in a multi-level tree pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeReduce(add)
        -5
        >>> rdd.treeReduce(add, 1)
        -5
        >>> rdd.treeReduce(add, 2)
        -5
        >>> rdd.treeReduce(add, 5)
        -5
        >>> rdd.treeReduce(add, 10)
        -5
CODE:
def treeReduce(self, f, depth=2):
        """
        Reduces the elements of this RDD in a multi-level tree pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeReduce(add)
        -5
        >>> rdd.treeReduce(add, 1)
        -5
        >>> rdd.treeReduce(add, 2)
        -5
        >>> rdd.treeReduce(add, 5)
        -5
        >>> rdd.treeReduce(add, 10)
        -5
        """
        if depth < 1:
            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)

        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.

        def op(x, y):
            if x[1]:
                return y
            elif y[1]:
                return x
            else:
                return f(x[0], y[0]), False

        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)
        if reduced[1]:
            raise ValueError("Cannot reduce empty RDD.")
        return reduced[0]
--------------------------------------------------

--- SAMPLE #78 ---
DOCSTRING: Aggregate the elements of each partition, and then the results for all
        the partitions, using a given associative function and a neutral "zero value."

        The function C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        This behaves somewhat differently from fold operations implemented
        for non-distributed collections in functional languages like Scala.
        This fold operation may be applied to partitions individually, and then
        fold those results into the final result, rather than apply the fold
        to each element sequentially in some defined ordering. For functions
        that are not commutative, the result may differ from that of a fold
        applied to a non-distributed collection.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
        15
CODE:
def fold(self, zeroValue, op):
        """
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given associative function and a neutral "zero value."

        The function C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        This behaves somewhat differently from fold operations implemented
        for non-distributed collections in functional languages like Scala.
        This fold operation may be applied to partitions individually, and then
        fold those results into the final result, rather than apply the fold
        to each element sequentially in some defined ordering. For functions
        that are not commutative, the result may differ from that of a fold
        applied to a non-distributed collection.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
        15
        """
        op = fail_on_stopiteration(op)

        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = op(acc, obj)
            yield acc
        # collecting result of mapPartitions here ensures that the copy of
        # zeroValue provided to each partition is unique from the one provided
        # to the final reduce call
        vals = self.mapPartitions(func).collect()
        return reduce(op, vals, zeroValue)
--------------------------------------------------

--- SAMPLE #79 ---
DOCSTRING: Aggregate the elements of each partition, and then the results for all
        the partitions, using a given combine functions and a neutral "zero
        value."

        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        The first function (seqOp) can return a different result type, U, than
        the type of this RDD. Thus, we need one operation for merging a T into
        an U and one operation for merging two U

        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
        (10, 4)
        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
        (0, 0)
CODE:
def aggregate(self, zeroValue, seqOp, combOp):
        """
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given combine functions and a neutral "zero
        value."

        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        The first function (seqOp) can return a different result type, U, than
        the type of this RDD. Thus, we need one operation for merging a T into
        an U and one operation for merging two U

        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
        (10, 4)
        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
        (0, 0)
        """
        seqOp = fail_on_stopiteration(seqOp)
        combOp = fail_on_stopiteration(combOp)

        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        # collecting result of mapPartitions here ensures that the copy of
        # zeroValue provided to each partition is unique from the one provided
        # to the final reduce call
        vals = self.mapPartitions(func).collect()
        return reduce(combOp, vals, zeroValue)
--------------------------------------------------

--- SAMPLE #80 ---
DOCSTRING: Aggregates the elements of this RDD in a multi-level tree
        pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeAggregate(0, add, add)
        -5
        >>> rdd.treeAggregate(0, add, add, 1)
        -5
        >>> rdd.treeAggregate(0, add, add, 2)
        -5
        >>> rdd.treeAggregate(0, add, add, 5)
        -5
        >>> rdd.treeAggregate(0, add, add, 10)
        -5
CODE:
def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):
        """
        Aggregates the elements of this RDD in a multi-level tree
        pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeAggregate(0, add, add)
        -5
        >>> rdd.treeAggregate(0, add, add, 1)
        -5
        >>> rdd.treeAggregate(0, add, add, 2)
        -5
        >>> rdd.treeAggregate(0, add, add, 5)
        -5
        >>> rdd.treeAggregate(0, add, add, 10)
        -5
        """
        if depth < 1:
            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)

        if self.getNumPartitions() == 0:
            return zeroValue

        def aggregatePartition(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc

        partiallyAggregated = self.mapPartitions(aggregatePartition)
        numPartitions = partiallyAggregated.getNumPartitions()
        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)
        # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree
        # aggregation.
        while numPartitions > scale + numPartitions / scale:
            numPartitions /= scale
            curNumPartitions = int(numPartitions)

            def mapPartition(i, iterator):
                for obj in iterator:
                    yield (i % curNumPartitions, obj)

            partiallyAggregated = partiallyAggregated \
                .mapPartitionsWithIndex(mapPartition) \
                .reduceByKey(combOp, curNumPartitions) \
                .values()

        return partiallyAggregated.reduce(combOp)
--------------------------------------------------

--- SAMPLE #81 ---
DOCSTRING: Find the maximum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])
        >>> rdd.max()
        43.0
        >>> rdd.max(key=str)
        5.0
CODE:
def max(self, key=None):
        """
        Find the maximum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])
        >>> rdd.max()
        43.0
        >>> rdd.max(key=str)
        5.0
        """
        if key is None:
            return self.reduce(max)
        return self.reduce(lambda a, b: max(a, b, key=key))
--------------------------------------------------

--- SAMPLE #82 ---
DOCSTRING: Find the minimum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])
        >>> rdd.min()
        2.0
        >>> rdd.min(key=str)
        10.0
CODE:
def min(self, key=None):
        """
        Find the minimum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])
        >>> rdd.min()
        2.0
        >>> rdd.min(key=str)
        10.0
        """
        if key is None:
            return self.reduce(min)
        return self.reduce(lambda a, b: min(a, b, key=key))
--------------------------------------------------

--- SAMPLE #83 ---
DOCSTRING: Add up the elements in this RDD.

        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()
        6.0
CODE:
def sum(self):
        """
        Add up the elements in this RDD.

        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()
        6.0
        """
        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)
--------------------------------------------------

--- SAMPLE #84 ---
DOCSTRING: Return a L{StatCounter} object that captures the mean, variance
        and count of the RDD's elements in one operation.
CODE:
def stats(self):
        """
        Return a L{StatCounter} object that captures the mean, variance
        and count of the RDD's elements in one operation.
        """
        def redFunc(left_counter, right_counter):
            return left_counter.mergeStats(right_counter)

        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)
--------------------------------------------------

--- SAMPLE #85 ---
DOCSTRING: Compute a histogram using the provided buckets. The buckets
        are all open to the right except for the last which is closed.
        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],
        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1
        and 50 we would have a histogram of 1,0,1.

        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),
        this can be switched from an O(log n) inseration to O(1) per
        element (where n is the number of buckets).

        Buckets must be sorted, not contain any duplicates, and have
        at least two elements.

        If `buckets` is a number, it will generate buckets which are
        evenly spaced between the minimum and maximum of the RDD. For
        example, if the min value is 0 and the max is 100, given `buckets`
        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must
        be at least 1. An exception is raised if the RDD contains infinity.
        If the elements in the RDD do not vary (max == min), a single bucket
        will be used.

        The return value is a tuple of buckets and histogram.

        >>> rdd = sc.parallelize(range(51))
        >>> rdd.histogram(2)
        ([0, 25, 50], [25, 26])
        >>> rdd.histogram([0, 5, 25, 50])
        ([0, 5, 25, 50], [5, 20, 26])
        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets
        ([0, 15, 30, 45, 60], [15, 15, 15, 6])
        >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])
        >>> rdd.histogram(("a", "b", "c"))
        (('a', 'b', 'c'), [2, 2])
CODE:
def histogram(self, buckets):
        """
        Compute a histogram using the provided buckets. The buckets
        are all open to the right except for the last which is closed.
        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],
        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1
        and 50 we would have a histogram of 1,0,1.

        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),
        this can be switched from an O(log n) inseration to O(1) per
        element (where n is the number of buckets).

        Buckets must be sorted, not contain any duplicates, and have
        at least two elements.

        If `buckets` is a number, it will generate buckets which are
        evenly spaced between the minimum and maximum of the RDD. For
        example, if the min value is 0 and the max is 100, given `buckets`
        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must
        be at least 1. An exception is raised if the RDD contains infinity.
        If the elements in the RDD do not vary (max == min), a single bucket
        will be used.

        The return value is a tuple of buckets and histogram.

        >>> rdd = sc.parallelize(range(51))
        >>> rdd.histogram(2)
        ([0, 25, 50], [25, 26])
        >>> rdd.histogram([0, 5, 25, 50])
        ([0, 5, 25, 50], [5, 20, 26])
        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets
        ([0, 15, 30, 45, 60], [15, 15, 15, 6])
        >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])
        >>> rdd.histogram(("a", "b", "c"))
        (('a', 'b', 'c'), [2, 2])
        """

        if isinstance(buckets, int):
            if buckets < 1:
                raise ValueError("number of buckets must be >= 1")

            # filter out non-comparable elements
            def comparable(x):
                if x is None:
                    return False
                if type(x) is float and isnan(x):
                    return False
                return True

            filtered = self.filter(comparable)

            # faster than stats()
            def minmax(a, b):
                return min(a[0], b[0]), max(a[1], b[1])
            try:
                minv, maxv = filtered.map(lambda x: (x, x)).reduce(minmax)
            except TypeError as e:
                if " empty " in str(e):
                    raise ValueError("can not generate buckets from empty RDD")
                raise

            if minv == maxv or buckets == 1:
                return [minv, maxv], [filtered.count()]

            try:
                inc = (maxv - minv) / buckets
            except TypeError:
                raise TypeError("Can not generate buckets with non-number in RDD")

            if isinf(inc):
                raise ValueError("Can not generate buckets with infinite value")

            # keep them as integer if possible
            inc = int(inc)
            if inc * buckets != maxv - minv:
                inc = (maxv - minv) * 1.0 / buckets

            buckets = [i * inc + minv for i in range(buckets)]
            buckets.append(maxv)  # fix accumulated error
            even = True

        elif isinstance(buckets, (list, tuple)):
            if len(buckets) < 2:
                raise ValueError("buckets should have more than one value")

            if any(i is None or isinstance(i, float) and isnan(i) for i in buckets):
                raise ValueError("can not have None or NaN in buckets")

            if sorted(buckets) != list(buckets):
                raise ValueError("buckets should be sorted")

            if len(set(buckets)) != len(buckets):
                raise ValueError("buckets should not contain duplicated values")

            minv = buckets[0]
            maxv = buckets[-1]
            even = False
            inc = None
            try:
                steps = [buckets[i + 1] - buckets[i] for i in range(len(buckets) - 1)]
            except TypeError:
                pass  # objects in buckets do not support '-'
            else:
                if max(steps) - min(steps) < 1e-10:  # handle precision errors
                    even = True
                    inc = (maxv - minv) / (len(buckets) - 1)

        else:
            raise TypeError("buckets should be a list or tuple or number(int or long)")

        def histogram(iterator):
            counters = [0] * len(buckets)
            for i in iterator:
                if i is None or (type(i) is float and isnan(i)) or i > maxv or i < minv:
                    continue
                t = (int((i - minv) / inc) if even
                     else bisect.bisect_right(buckets, i) - 1)
                counters[t] += 1
            # add last two together
            last = counters.pop()
            counters[-1] += last
            return [counters]

        def mergeCounters(a, b):
            return [i + j for i, j in zip(a, b)]

        return buckets, self.mapPartitions(histogram).reduce(mergeCounters)
--------------------------------------------------

--- SAMPLE #86 ---
DOCSTRING: Return the count of each unique value in this RDD as a dictionary of
        (value, count) pairs.

        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
        [(1, 2), (2, 3)]
CODE:
def countByValue(self):
        """
        Return the count of each unique value in this RDD as a dictionary of
        (value, count) pairs.

        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
        [(1, 2), (2, 3)]
        """
        def countPartition(iterator):
            counts = defaultdict(int)
            for obj in iterator:
                counts[obj] += 1
            yield counts

        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] += v
            return m1
        return self.mapPartitions(countPartition).reduce(mergeMaps)
--------------------------------------------------

--- SAMPLE #87 ---
DOCSTRING: Get the top N elements from an RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        .. note:: It returns the list sorted in descending order.

        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
        [12]
        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
        [6, 5]
        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
        [4, 3, 2]
CODE:
def top(self, num, key=None):
        """
        Get the top N elements from an RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        .. note:: It returns the list sorted in descending order.

        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
        [12]
        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
        [6, 5]
        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
        [4, 3, 2]
        """
        def topIterator(iterator):
            yield heapq.nlargest(num, iterator, key=key)

        def merge(a, b):
            return heapq.nlargest(num, a + b, key=key)

        return self.mapPartitions(topIterator).reduce(merge)
--------------------------------------------------

--- SAMPLE #88 ---
DOCSTRING: Get the N elements from an RDD ordered in ascending order or as
        specified by the optional key function.

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
        [1, 2, 3, 4, 5, 6]
        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
        [10, 9, 7, 6, 5, 4]
CODE:
def takeOrdered(self, num, key=None):
        """
        Get the N elements from an RDD ordered in ascending order or as
        specified by the optional key function.

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
        [1, 2, 3, 4, 5, 6]
        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
        [10, 9, 7, 6, 5, 4]
        """

        def merge(a, b):
            return heapq.nsmallest(num, a + b, key)

        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
--------------------------------------------------

--- SAMPLE #89 ---
DOCSTRING: Take the first num elements of the RDD.

        It works by first scanning one partition, and use the results from
        that partition to estimate the number of additional partitions needed
        to satisfy the limit.

        Translated from the Scala implementation in RDD#take().

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
        [2, 3]
        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)
        [2, 3, 4, 5, 6]
        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
        [91, 92, 93]
CODE:
def take(self, num):
        """
        Take the first num elements of the RDD.

        It works by first scanning one partition, and use the results from
        that partition to estimate the number of additional partitions needed
        to satisfy the limit.

        Translated from the Scala implementation in RDD#take().

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
        [2, 3]
        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)
        [2, 3, 4, 5, 6]
        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
        [91, 92, 93]
        """
        items = []
        totalParts = self.getNumPartitions()
        partsScanned = 0

        while len(items) < num and partsScanned < totalParts:
            # The number of partitions to try in this iteration.
            # It is ok for this number to be greater than totalParts because
            # we actually cap it at totalParts in runJob.
            numPartsToTry = 1
            if partsScanned > 0:
                # If we didn't find any rows after the previous iteration,
                # quadruple and retry.  Otherwise, interpolate the number of
                # partitions we need to try, but overestimate it by 50%.
                # We also cap the estimation in the end.
                if len(items) == 0:
                    numPartsToTry = partsScanned * 4
                else:
                    # the first parameter of max is >=1 whenever partsScanned >= 2
                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned
                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)

            left = num - len(items)

            def takeUpToNumLeft(iterator):
                iterator = iter(iterator)
                taken = 0
                while taken < left:
                    try:
                        yield next(iterator)
                    except StopIteration:
                        return
                    taken += 1

            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
            res = self.context.runJob(self, takeUpToNumLeft, p)

            items += res
            partsScanned += numPartsToTry

        return items[:num]
--------------------------------------------------

--- SAMPLE #90 ---
DOCSTRING: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are
        converted for output using either user specified converters or, by default,
        L{org.apache.spark.api.python.JavaToWritableConverter}.

        :param conf: Hadoop job configuration, passed in as a dict
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
CODE:
def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):
        """
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are
        converted for output using either user specified converters or, by default,
        L{org.apache.spark.api.python.JavaToWritableConverter}.

        :param conf: Hadoop job configuration, passed in as a dict
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        """
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,
                                                    keyConverter, valueConverter, True)
--------------------------------------------------

--- SAMPLE #91 ---
DOCSTRING: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types
        will be inferred if not specified. Keys and values are converted for output using either
        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The
        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext
        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.

        :param path: path to Hadoop file
        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat
               (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")
        :param keyClass: fully qualified classname of key Writable class
               (e.g. "org.apache.hadoop.io.IntWritable", None by default)
        :param valueClass: fully qualified classname of value Writable class
               (e.g. "org.apache.hadoop.io.Text", None by default)
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        :param conf: Hadoop job configuration, passed in as a dict (None by default)
CODE:
def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,
                               keyConverter=None, valueConverter=None, conf=None):
        """
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types
        will be inferred if not specified. Keys and values are converted for output using either
        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The
        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext
        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.

        :param path: path to Hadoop file
        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat
               (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")
        :param keyClass: fully qualified classname of key Writable class
               (e.g. "org.apache.hadoop.io.IntWritable", None by default)
        :param valueClass: fully qualified classname of value Writable class
               (e.g. "org.apache.hadoop.io.Text", None by default)
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        :param conf: Hadoop job configuration, passed in as a dict (None by default)
        """
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,
                                                       outputFormatClass,
                                                       keyClass, valueClass,
                                                       keyConverter, valueConverter, jconf)
--------------------------------------------------

--- SAMPLE #92 ---
DOCSTRING: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the
        RDD's key and value types. The mechanism is as follows:

            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.
            2. Keys and values of this Java RDD are converted to Writables and written out.

        :param path: path to sequence file
        :param compressionCodecClass: (None by default)
CODE:
def saveAsSequenceFile(self, path, compressionCodecClass=None):
        """
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the
        RDD's key and value types. The mechanism is as follows:

            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.
            2. Keys and values of this Java RDD are converted to Writables and written out.

        :param path: path to sequence file
        :param compressionCodecClass: (None by default)
        """
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,
                                                   path, compressionCodecClass)
--------------------------------------------------

--- SAMPLE #93 ---
DOCSTRING: Save this RDD as a SequenceFile of serialized objects. The serializer
        used is L{pyspark.serializers.PickleSerializer}, default batch size
        is 10.

        >>> tmpFile = NamedTemporaryFile(delete=True)
        >>> tmpFile.close()
        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)
        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())
        ['1', '2', 'rdd', 'spark']
CODE:
def saveAsPickleFile(self, path, batchSize=10):
        """
        Save this RDD as a SequenceFile of serialized objects. The serializer
        used is L{pyspark.serializers.PickleSerializer}, default batch size
        is 10.

        >>> tmpFile = NamedTemporaryFile(delete=True)
        >>> tmpFile.close()
        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)
        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())
        ['1', '2', 'rdd', 'spark']
        """
        if batchSize == 0:
            ser = AutoBatchedSerializer(PickleSerializer())
        else:
            ser = BatchedSerializer(PickleSerializer(), batchSize)
        self._reserialize(ser)._jrdd.saveAsObjectFile(path)
--------------------------------------------------

--- SAMPLE #94 ---
DOCSTRING: Save this RDD as a text file, using string representations of elements.

        @param path: path to text file
        @param compressionCodecClass: (None by default) string i.e.
            "org.apache.hadoop.io.compress.GzipCodec"

        >>> tempFile = NamedTemporaryFile(delete=True)
        >>> tempFile.close()
        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)
        >>> from fileinput import input
        >>> from glob import glob
        >>> ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))
        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'

        Empty lines are tolerated when saving to text files.

        >>> tempFile2 = NamedTemporaryFile(delete=True)
        >>> tempFile2.close()
        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)
        >>> ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))
        '\\n\\n\\nbar\\nfoo\\n'

        Using compressionCodecClass

        >>> tempFile3 = NamedTemporaryFile(delete=True)
        >>> tempFile3.close()
        >>> codec = "org.apache.hadoop.io.compress.GzipCodec"
        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)
        >>> from fileinput import input, hook_compressed
        >>> result = sorted(input(glob(tempFile3.name + "/part*.gz"), openhook=hook_compressed))
        >>> b''.join(result).decode('utf-8')
        u'bar\\nfoo\\n'
CODE:
def saveAsTextFile(self, path, compressionCodecClass=None):
        """
        Save this RDD as a text file, using string representations of elements.

        @param path: path to text file
        @param compressionCodecClass: (None by default) string i.e.
            "org.apache.hadoop.io.compress.GzipCodec"

        >>> tempFile = NamedTemporaryFile(delete=True)
        >>> tempFile.close()
        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)
        >>> from fileinput import input
        >>> from glob import glob
        >>> ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))
        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'

        Empty lines are tolerated when saving to text files.

        >>> tempFile2 = NamedTemporaryFile(delete=True)
        >>> tempFile2.close()
        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)
        >>> ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))
        '\\n\\n\\nbar\\nfoo\\n'

        Using compressionCodecClass

        >>> tempFile3 = NamedTemporaryFile(delete=True)
        >>> tempFile3.close()
        >>> codec = "org.apache.hadoop.io.compress.GzipCodec"
        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)
        >>> from fileinput import input, hook_compressed
        >>> result = sorted(input(glob(tempFile3.name + "/part*.gz"), openhook=hook_compressed))
        >>> b''.join(result).decode('utf-8')
        u'bar\\nfoo\\n'
        """
        def func(split, iterator):
            for x in iterator:
                if not isinstance(x, (unicode, bytes)):
                    x = unicode(x)
                if isinstance(x, unicode):
                    x = x.encode("utf-8")
                yield x
        keyed = self.mapPartitionsWithIndex(func)
        keyed._bypass_serializer = True
        if compressionCodecClass:
            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)
        else:
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)
--------------------------------------------------

--- SAMPLE #95 ---
DOCSTRING: Merge the values for each key using an associative and commutative reduce function.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a "combiner" in MapReduce.

        Output will be partitioned with C{numPartitions} partitions, or
        the default parallelism level if C{numPartitions} is not specified.
        Default partitioner is hash-partition.

        >>> from operator import add
        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> sorted(rdd.reduceByKey(add).collect())
        [('a', 2), ('b', 1)]
CODE:
def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash):
        """
        Merge the values for each key using an associative and commutative reduce function.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a "combiner" in MapReduce.

        Output will be partitioned with C{numPartitions} partitions, or
        the default parallelism level if C{numPartitions} is not specified.
        Default partitioner is hash-partition.

        >>> from operator import add
        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> sorted(rdd.reduceByKey(add).collect())
        [('a', 2), ('b', 1)]
        """
        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)
--------------------------------------------------

--- SAMPLE #96 ---
DOCSTRING: Merge the values for each key using an associative and commutative reduce function, but
        return the results immediately to the master as a dictionary.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a "combiner" in MapReduce.

        >>> from operator import add
        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> sorted(rdd.reduceByKeyLocally(add).items())
        [('a', 2), ('b', 1)]
CODE:
def reduceByKeyLocally(self, func):
        """
        Merge the values for each key using an associative and commutative reduce function, but
        return the results immediately to the master as a dictionary.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a "combiner" in MapReduce.

        >>> from operator import add
        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> sorted(rdd.reduceByKeyLocally(add).items())
        [('a', 2), ('b', 1)]
        """
        func = fail_on_stopiteration(func)

        def reducePartition(iterator):
            m = {}
            for k, v in iterator:
                m[k] = func(m[k], v) if k in m else v
            yield m

        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] = func(m1[k], v) if k in m1 else v
            return m1
        return self.mapPartitions(reducePartition).reduce(mergeMaps)
--------------------------------------------------

--- SAMPLE #97 ---
DOCSTRING: Return a copy of the RDD partitioned using the specified partitioner.

        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))
        >>> sets = pairs.partitionBy(2).glom().collect()
        >>> len(set(sets[0]).intersection(set(sets[1])))
        0
CODE:
def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        """
        Return a copy of the RDD partitioned using the specified partitioner.

        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))
        >>> sets = pairs.partitionBy(2).glom().collect()
        >>> len(set(sets[0]).intersection(set(sets[1])))
        0
        """
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        partitioner = Partitioner(numPartitions, partitionFunc)
        if self.partitioner == partitioner:
            return self

        # Transferring O(n) objects to Java is too expensive.
        # Instead, we'll form the hash buckets in Python,
        # transferring O(numPartitions) objects to Java.
        # Each object is a (splitNumber, [objects]) pair.
        # In order to avoid too huge objects, the objects are
        # grouped into chunks.
        outputSerializer = self.ctx._unbatched_serializer

        limit = (_parse_memory(self.ctx._conf.get(
            "spark.python.worker.memory", "512m")) / 2)

        def add_shuffle_key(split, iterator):

            buckets = defaultdict(list)
            c, batch = 0, min(10 * numPartitions, 1000)

            for k, v in iterator:
                buckets[partitionFunc(k) % numPartitions].append((k, v))
                c += 1

                # check used memory and avg size of chunk of objects
                if (c % 1000 == 0 and get_used_memory() > limit
                        or c > batch):
                    n, size = len(buckets), 0
                    for split in list(buckets.keys()):
                        yield pack_long(split)
                        d = outputSerializer.dumps(buckets[split])
                        del buckets[split]
                        yield d
                        size += len(d)

                    avg = int(size / n) >> 20
                    # let 1M < avg < 10M
                    if avg < 1:
                        batch *= 1.5
                    elif avg > 10:
                        batch = max(int(batch / 1.5), 1)
                    c = 0

            for split, items in buckets.items():
                yield pack_long(split)
                yield outputSerializer.dumps(items)

        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)
        keyed._bypass_serializer = True
        with SCCallSiteSync(self.context) as css:
            pairRDD = self.ctx._jvm.PairwiseRDD(
                keyed._jrdd.rdd()).asJavaPairRDD()
            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,
                                                           id(partitionFunc))
        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))
        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))
        rdd.partitioner = partitioner
        return rdd
--------------------------------------------------

--- SAMPLE #98 ---
DOCSTRING: Generic function to combine the elements for each key using a custom
        set of aggregation functions.

        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined
        type" C.

        Users provide three functions:

            - C{createCombiner}, which turns a V into a C (e.g., creates
              a one-element list)
            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of
              a list)
            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges
              the lists)

        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to
        modify and return their first argument instead of creating a new C.

        In addition, users can control the partitioning of the output RDD.

        .. note:: V and C can be different -- for example, one might group an RDD of type
            (Int, Int) into an RDD of type (Int, List[Int]).

        >>> x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)])
        >>> def to_list(a):
        ...     return [a]
        ...
        >>> def append(a, b):
        ...     a.append(b)
        ...     return a
        ...
        >>> def extend(a, b):
        ...     a.extend(b)
        ...     return a
        ...
        >>> sorted(x.combineByKey(to_list, append, extend).collect())
        [('a', [1, 2]), ('b', [1])]
CODE:
def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None, partitionFunc=portable_hash):
        """
        Generic function to combine the elements for each key using a custom
        set of aggregation functions.

        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined
        type" C.

        Users provide three functions:

            - C{createCombiner}, which turns a V into a C (e.g., creates
              a one-element list)
            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of
              a list)
            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges
              the lists)

        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to
        modify and return their first argument instead of creating a new C.

        In addition, users can control the partitioning of the output RDD.

        .. note:: V and C can be different -- for example, one might group an RDD of type
            (Int, Int) into an RDD of type (Int, List[Int]).

        >>> x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)])
        >>> def to_list(a):
        ...     return [a]
        ...
        >>> def append(a, b):
        ...     a.append(b)
        ...     return a
        ...
        >>> def extend(a, b):
        ...     a.extend(b)
        ...     return a
        ...
        >>> sorted(x.combineByKey(to_list, append, extend).collect())
        [('a', [1, 2]), ('b', [1])]
        """
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        serializer = self.ctx.serializer
        memory = self._memory_limit()
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)

        def combineLocally(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()

        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)

        def _mergeCombiners(iterator):
            merger = ExternalMerger(agg, memory, serializer)
            merger.mergeCombiners(iterator)
            return merger.items()

        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)
--------------------------------------------------

--- SAMPLE #99 ---
DOCSTRING: Aggregate the values of each key, using given combine functions and a neutral
        "zero value". This function can return a different result type, U, than the type
        of the values in this RDD, V. Thus, we need one operation for merging a V into
        a U and one operation for merging two U's, The former operation is used for merging
        values within a partition, and the latter is used for merging values between
        partitions. To avoid memory allocation, both of these functions are
        allowed to modify and return their first argument instead of creating a new U.
CODE:
def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,
                       partitionFunc=portable_hash):
        """
        Aggregate the values of each key, using given combine functions and a neutral
        "zero value". This function can return a different result type, U, than the type
        of the values in this RDD, V. Thus, we need one operation for merging a V into
        a U and one operation for merging two U's, The former operation is used for merging
        values within a partition, and the latter is used for merging values between
        partitions. To avoid memory allocation, both of these functions are
        allowed to modify and return their first argument instead of creating a new U.
        """
        def createZero():
            return copy.deepcopy(zeroValue)

        return self.combineByKey(
            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)
--------------------------------------------------

--- SAMPLE #100 ---
DOCSTRING: Merge the values for each key using an associative function "func"
        and a neutral "zeroValue" which may be added to the result an
        arbitrary number of times, and must not change the result
        (e.g., 0 for addition, or 1 for multiplication.).

        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> from operator import add
        >>> sorted(rdd.foldByKey(0, add).collect())
        [('a', 2), ('b', 1)]
CODE:
def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):
        """
        Merge the values for each key using an associative function "func"
        and a neutral "zeroValue" which may be added to the result an
        arbitrary number of times, and must not change the result
        (e.g., 0 for addition, or 1 for multiplication.).

        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
        >>> from operator import add
        >>> sorted(rdd.foldByKey(0, add).collect())
        [('a', 2), ('b', 1)]
        """
        def createZero():
            return copy.deepcopy(zeroValue)

        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,
                                 partitionFunc)
--------------------------------------------------



================================================================================

--- FILE: factory.py | PATH: Project\models\factory.py ---
----------------------------------------------------------

"""
================================================================================
ARCHITECTURAL FACTORY UNIT
================================================================================
ROLE: Centralized Model Synthesis and Hyperparameter Configuration.

DESIGN RATIONALE:
- Abstraction: Allows the C2Orchestrator to request an architecture by string 
  identifier without knowing internal implementation details.
- Phenotypic Calibration: Specifically tuned hyperparameters based on 
  architectural behavior (Sequential vs. Parallel).
- Hardware Mapping: Ensures immediate VRAM occupancy upon instantiation.
================================================================================
"""

import logging
import torch
from models.transformer import Seq2SeqTransformer
from models.seq2seq_bahdanau import Seq2SeqBahdanau
from models.seq2seq_dotproduct import Seq2SeqDotProduct

# Module-level logger for auditing architectural synthesis.
logger = logging.getLogger(__name__)

def get_model_architecture(model_type, device, vocab_size=20000):
    """
    FACTORY ACTUATOR v3.5 - Tailored Architectures
    ---------------------------------------------
    Calibrated parameters based on architectural phenotype:
    
    - LSTM: Maintains statistical conservatism (Dropout 0.1). 
      Recurrent nets are naturally regularized by their sequential nature.
      
    - TRANSFORMER: Employs aggressive regularization (Dropout 0.3) 
      to disrupt rote learning (memorization) of fixed strings, forcing the 
      attention heads to learn generalized syntactic patterns.
    """

    # --- [PHASE 1] LSTM CONFIGURATION (Sequential Backbones) ---
    # Common dimensions for Bahdanau (Additive) and DotProduct (Multiplicative).
    # Focus: Balance between latent capacity and training stability.
    lstm_params = {
        "emb_dim": 256,   # Dimension of the dense vector space for tokens.
        "hid_dim": 512,   # Latent space capacity (Hidden State size).
        "n_layers": 2,    # Depth of the recurrent stack.
        "dropout": 0.1    # Standard regularization protocol for RNNs.
    }

    # --- [PHASE 2] TRANSFORMER CONFIGURATION (Parallel Backbone) ---
    # Focus: Fighting Overfitting. Since Transformers have massive capacity, 
    # they tend to "shortcut" the learning process by memorizing the dataset.
    trans_params = {
        "emb_dim": 256,
        "hid_dim": 512,            # Maps to d_model in Transformer nomenclature.
        "nhead": 8,                # Number of parallel attention heads.
        "layers": 6,               # Total depth of Encoder and Decoder stacks.
        "dropout": 0.3,            # HIGH ENTROPY FIX: Forces model to find robust pathways.
        "dim_feedforward": 2048    # Expansion factor for the point-wise MLP.
    }

    # --- [PHASE 3] LOGICAL BRANCHING & INSTANTIATION ---

    # ARCHITECTURE A: LSTM + BAHDANAU ATTENTION
    # Uses a neural network layer to learn alignments (Additive Attention).
    if model_type == "lstm_bahdanau":
        logger.info(f"Factory: Generating LSTM + Bahdanau (Conservative Dropout: {lstm_params['dropout']})")
        return Seq2SeqBahdanau(
            vocab_size=vocab_size, 
            emb_dim=lstm_params["emb_dim"], 
            hid_dim=lstm_params["hid_dim"], 
            n_layers=lstm_params["n_layers"], 
            dropout=lstm_params["dropout"], 
            device=device
        ).to(device) # Immediate hardware mapping.

    # ARCHITECTURE B: LSTM + DOTPRODUCT ATTENTION
    # Scaled Dot-Product alignment mechanism (Geometric/Multiplicative Attention).
    elif model_type == "lstm_dotproduct":
        logger.info(f"Factory: Generating LSTM + DotProduct (Conservative Dropout: {lstm_params['dropout']})")
        return Seq2SeqDotProduct(
            vocab_size=vocab_size, 
            emb_dim=lstm_params["emb_dim"], 
            hid_dim=lstm_params["hid_dim"], 
            n_layers=lstm_params["n_layers"], 
            dropout=lstm_params["dropout"], 
            device=device
        ).to(device)

    # ARCHITECTURE C: TRANSFORMER (Attention Is All You Need)
    # The parallel titan. Requires strict regularization to converge on code logic.
    elif model_type == "transformer":
        logger.info(f"Factory: Generating Transformer (Aggressive Dropout: {trans_params['dropout']})")
        return Seq2SeqTransformer(
            vocab_size=vocab_size, 
            d_model=trans_params["hid_dim"],
            nhead=trans_params["nhead"],
            num_layers=trans_params["layers"], 
            dim_feedforward=trans_params["dim_feedforward"],
            dropout=trans_params["dropout"]
        ).to(device)

    # ERROR HANDLING: Prevents the pipeline from entering an undefined state.
    else:
        logger.error(f"Critical Error: Architecture '{model_type}' not supported by current Factory build.")
        raise ValueError(f"Unknown model type requested: {model_type}")

================================================================================

--- FILE: seq2seq_bahdanau.py | PATH: Project\models\seq2seq_bahdanau.py ---
----------------------------------------------------------------------------

"""
================================================================================
LSTM + BAHDANAU ATTENTION (Additive) 
================================================================================
ROLE: Sequential Sequence-to-Sequence Modeling with Neural Alignment.

DESIGN RATIONALE:
- Hybrid Contextualization: Unlike basic LSTMs, the Decoder does not rely solely 
  on the final hidden state. It re-scans the Encoder's history at every step.
- Additive Attention: Implements a learnable alignment function using a small 
  Multi-Layer Perceptron (MLP) with tanh activation.
- Robustness: Highly effective for long source sequences (code bodies) where 
  the 'Vanishing Gradient' would otherwise erode information.
================================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random

class BahdanauAttention(nn.Module):
    """
    NEURAL ALIGNMENT UNIT: Implements the 'Additive' Attention mechanism.
    
    This module computes a probability distribution over the encoder's hidden 
    states to determine which parts of the source code are relevant to the 
    current word being generated in the summary.
    """

    def __init__(self, hid_dim):
        """
        Args:
            hid_dim (int): The dimensionality of the hidden states.
        """
        super().__init__()

        # Scoring Layer: Projecting the combination of Decoder hidden state 
        # and Encoder outputs into a common latent space.
        self.attn = nn.Linear(hid_dim * 2, hid_dim)

        # Weighting Vector: A learnable parameter that reduces the projection 
        # to a single scalar (energy score) per source token.
        self.v = nn.Linear(hid_dim, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        """
        Args:
            hidden: Previous Decoder hidden state [batch, hid_dim].
            encoder_outputs: Full history from the Encoder [batch, src_len, hid_dim].
        """
        src_len = encoder_outputs.shape[1]

        # Expand Decoder hidden state to match Encoder sequence length.
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)

        # ENERGY CALCULATION: 
        # score = v * tanh(W * [decoder_h ; encoder_h])
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))

        # Reduce to scalar energy and remove the last dimension.
        attention = self.v(energy).squeeze(2)

        # SOFTMAX: Normalize energy into probabilities (sum to 1) across the sequence length.
        return F.softmax(attention, dim=1)

class Encoder(nn.Module):
    """
    FEATURE EXTRACTION UNIT: Consumes the source sequence and builds context.
    Uses a multi-layer LSTM to capture hierarchical dependencies in Python code.
    """

    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        """
        Args:
            input_dim: Vocab size.
            emb_dim: Token embedding size.
            hid_dim: LSTM internal capacity.
            n_layers: Depth of recurrent stacking.
            dropout: Regularization probability.
        """
        super().__init__()

        # Semantic projection layer.
        self.embedding = nn.Embedding(input_dim, emb_dim)

        # RECURRENT BACKBONE: Processes the sequence step-by-step.
        # batch_first=True ensures compatibility with modern PyTorch data flows.
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)

        self.dropout = nn.Dropout(dropout)

    def forward(self, src):
        """
        Transforms raw IDs into contextualized vectors.
        Returns:
            outputs: All hidden states [batch, seq_len, hid_dim].
            hidden/cell: Final state tensors [n_layers, batch, hid_dim].
        """
        embedded = self.dropout(self.embedding(src))

        # LSTM processing: outputs contains the features for EACH token.
        outputs, (hidden, cell) = self.rnn(embedded)

        return outputs, hidden, cell

class Decoder(nn.Module):
    """
    GENERATIVE REFINERY: Predicts the next token in the natural language summary.
    Fuses previous tokens, recurrent memory, and the 'Attention' context.
    """

    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()

        self.output_dim = output_dim
        self.attention = BahdanauAttention(hid_dim)
        self.embedding = nn.Embedding(output_dim, emb_dim)

        # The input to the RNN is a fusion of the current token and the context vector.
        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)

        # FINAL PROJECTION: Maps concatenated features to the vocabulary space.
        # hid_dim * 2 comes from (RNN output + Weighted Context).
        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell, encoder_outputs):
        """
        Single-step decoding logic.
        """
        # input: [batch] -> reshape to [batch, 1] for sequential processing.
        input = input.unsqueeze(1)
        embedded = self.dropout(self.embedding(input))

        # ATTENTION STEP: Find relevance of source tokens relative to current hidden state.
        # a: [batch, 1, src_len]
        a = self.attention(hidden[-1], encoder_outputs).unsqueeze(1)

        # CONTEXT VECTOR: Weighted sum of Encoder outputs.
        # weighted: [batch, 1, hid_dim]
        weighted = torch.bmm(a, encoder_outputs)

        # FUSION: Combine Embedding with Context to inform the RNN of the "Source Truth".
        rnn_input = torch.cat((embedded, weighted), dim=2)

        # RECURRENCE: Update internal memory.
        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))

        # OUTPUT LOGITS: Final prediction based on current, contextual, and recurrent information.
        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=2))

        return prediction.squeeze(1), hidden, cell

class Seq2SeqBahdanau(nn.Module):
    """
    SCAFFOLD: Integrates Encoder, Attention, and Decoder into a unified pipeline.
    """

    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, dropout, device):
        super().__init__()
        self.encoder = Encoder(vocab_size, emb_dim, hid_dim, n_layers, dropout)
        self.decoder = Decoder(vocab_size, emb_dim, hid_dim, n_layers, dropout)
        self.device = device
        self.output_dim = vocab_size

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        """
        Master Forward Pass.
        
        Logic:
        1. Encode the source (Python code).
        2. Iterate over target length (Docstring).
        3. Use 'Teacher Forcing' to stabilize early training stages.
        """
        batch_size, trg_len = trg.shape[0], trg.shape[1]
        outputs = torch.zeros(batch_size, trg_len, self.output_dim).to(self.device)

        encoder_outputs, hidden, cell = self.encoder(src)

        # Initial token is always <SOS> (Start of Sentence).
        input = trg[:, 0]

        for t in range(1, trg_len):
            # Pass through the attention-augmented decoder.
            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)
            outputs[:, t] = output

            # TEACHER FORCING: 
            # Decides if we use the ground truth or the model's own guess for the next step.
            teacher_force = random.random() < teacher_forcing_ratio
            input = trg[:, t] if teacher_force else output.argmax(1)

        return outputs

================================================================================

--- FILE: seq2seq_dotproduct.py | PATH: Project\models\seq2seq_dotproduct.py ---
--------------------------------------------------------------------------------

"""
================================================================================
LSTM + SCALED DOT-PRODUCT ATTENTION 
================================================================================
ROLE: Multiplicative Sequence-to-Sequence Modeling.

DESIGN RATIONALE:
- Efficiency: Replaces the MLP in Bahdanau with a direct Dot Product, reducing 
  trainable parameters and decreasing training latency.
- Mathematical Alignment: Follows the geometric approach where attention is 
  viewed as a similarity measure in a projected latent space.
- Scaling: Implements the 1/sqrt(d_k) factor to maintain stable gradients, 
  preventing the Softmax from saturating during the backward pass.
================================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import numpy as np

# Reusing the standard LSTM Encoder to maintain pipeline consistency and modularity.
from models.seq2seq_bahdanau import Encoder 

class DotProductAttention(nn.Module):
    """
    GEOMETRIC ALIGNMENT UNIT: Implements Multiplicative (Dot-Product) Attention.
    
    This mechanism treats the Decoder's hidden state as a 'Query' and the 
    Encoder's states as 'Keys', calculating their similarity through matrix 
    products rather than additive layers.
    """
    def __init__(self, hid_dim):
        """
        Args:
            hid_dim (int): Dimension of the Query/Key vectors.
        """
        super().__init__()
        # Linear Projections: Mapping raw hidden states into the specialized 
        # Attention Space (Standard practice in modern Transformer-style attention).
        self.w_q = nn.Linear(hid_dim, hid_dim)
        self.w_k = nn.Linear(hid_dim, hid_dim)
        
        # Scaling Factor: Used to normalize the Dot Product magnitude. 
        # Prevents vanishing gradients by keeping scores within the Softmax 'sweet spot'.
        self.scale = np.sqrt(hid_dim)

    def forward(self, query, keys):
        """
        Args:
            query: Current Decoder state [batch, hid_dim].
            keys: All Encoder outputs [batch, src_len, hid_dim].
        """
        # Step 1: Project raw states into the matching Attention Space.
        # q: [batch, 1, hid_dim] | k: [batch, hid_dim, src_len] (transposed for dot product)
        q = self.w_q(query).unsqueeze(1) 
        k = self.w_k(keys).transpose(1, 2) 
        
        # Step 2: Scaled Dot Product calculation (BMM = Batch Matrix Multiplication).
        # scores = (Q * K^T) / sqrt(d)
        scores = torch.bmm(q, k) / self.scale 
        
        # Step 3: Compute Attention Weights (alphas).
        # Softmax over the source sequence length.
        alphas = F.softmax(scores, dim=-1)
        
        # Step 4: Construct the Context Vector.
        # Sum of Encoder outputs weighted by their relevance to the current Query.
        context = torch.bmm(alphas, keys) # [batch, 1, hid_dim]
        
        return alphas.squeeze(1), context

class DecoderDot(nn.Module):
    """
    FAST GENERATIVE DECODER: Integrated with Multiplicative Attention.
    Optimized for high-speed docstring generation.
    """
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim
        self.attention = DotProductAttention(hid_dim)
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        # RNN unit: Fuses the current token embedding with the global context vector.
        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)
        
        # Final Projection: Translates latent features into vocabulary-sized logits.
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell, encoder_outputs):
        """
        Single-step autoregressive forward pass.
        """
        # input: [batch] -> embedded: [batch, 1, emb_dim]
        input = input.unsqueeze(1)
        embedded = self.dropout(self.embedding(input))
        
        # QUERY SELECTION: The top-layer hidden state of the LSTM acts as the Query 
        # to find the most relevant code sections in the Encoder history.
        _, context = self.attention(hidden[-1], encoder_outputs)
        
        # FUSION: Concatenating the semantic token with the spatial code context.
        rnn_input = torch.cat((embedded, context), dim=2)
        
        # RECURRENCE: Update the LSTM's internal state (memory).
        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))
        
        # LOGIT PREDICTION: Map the updated memory to the target language space.
        prediction = self.fc_out(output)
        
        return prediction.squeeze(1), hidden, cell

class Seq2SeqDotProduct(nn.Module):
    """
    SCAFFOLD: Integrates the Encoder and the Dot-Product optimized Decoder.
    """
    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, dropout, device):
        super().__init__()
        self.encoder = Encoder(vocab_size, emb_dim, hid_dim, n_layers, dropout)
        self.decoder = DecoderDot(vocab_size, emb_dim, hid_dim, n_layers, dropout)
        self.device = device
        self.output_dim = vocab_size

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        """
        Full Sequence Forward Pass.
        """
        batch_size, trg_len = trg.shape[0], trg.shape[1]
        outputs = torch.zeros(batch_size, trg_len, self.output_dim).to(self.device)
        
        # 1. Encode source code into context vectors.
        encoder_outputs, hidden, cell = self.encoder(src)
        
        # 2. Iterate through target sequence for word-by-word generation.
        input = trg[:, 0] # Start with <SOS>
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)
            outputs[:, t] = output
            
            # TEACHER FORCING: Balancing between ground truth and self-correction.
            teacher_force = random.random() < teacher_forcing_ratio
            input = trg[:, t] if teacher_force else output.argmax(1)
            
        return outputs

================================================================================

--- FILE: transformer.py | PATH: Project\models\transformer.py ---
------------------------------------------------------------------

"""
================================================================================
TRANSFORMER CORE 
================================================================================
ROLE: High-Parallelism Attention-Based Sequence Transformation.

DESIGN RATIONALE:
- Temporal Order: Since Transformers lack recurrent loops, we use Sinusoidal 
  Positional Encodings to preserve token hierarchy and sequence structure.
- Interface Compatibility: Implements internal shifting and dummy padding to 
  remain "Plug & Play" with the LSTM-centric training orchestrator.
- Numerical Integrity: Uses Scaled Dot-Product Attention scaling ($1/\sqrt{d_k}$) 
  to prevent Softmax saturation during high-dimensional tensor operations.
================================================================================
"""

import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    """
    SPATIAL ORIENTATION UNIT: Injects non-trainable sinusoidal signals.
    
    Standard Transformers are 'permutation invariant'. This module breaks that 
    symmetry by adding sine and cosine waves of varying frequencies, allowing 
    the Attention heads to distinguish between word positions.
    """
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Pre-compute the positional encoding matrix in log space for efficiency.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        # Apply sine to even indices and cosine to odd indices.
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # Register as a buffer: it stays with the model but is not updated by gradients.
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        """
        Fuses the token embeddings with the spatial signal.
        """
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)

class Seq2SeqTransformer(nn.Module):
    """
    NEURAL ENGINE: Vanilla Transformer architecture with Unified Interface Logic.
    
    Specifically revised for Version 2.6 to handle internal data shifting, 
    making it compatible with the 'scripts/train.py' logic without code changes.
    """
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # PROJECTION LAYERS: Mapping symbols to latent attention space.
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        
        # CORE TRANSFORMER: Built-in PyTorch implementation for optimized kernel execution.
        # batch_first=True is used to maintain consistency with the C2 dataset pipeline.
        self.transformer = nn.Transformer(
            d_model=d_model, nhead=nhead, 
            num_encoder_layers=num_layers, num_decoder_layers=num_layers, 
            dim_feedforward=dim_feedforward, dropout=dropout, 
            batch_first=True 
        )
        
        # OUTPUT HEAD: Linear mapping back to vocabulary dimensions.
        self.fc_out = nn.Linear(d_model, vocab_size)

    def _generate_causal_mask(self, sz, device):
        """
        CAUSALITY SHIELD: Creates an upper-triangular matrix to prevent 'Peeking'.
        Ensures position 'i' can only attend to positions '<= i'.
        """
        return torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()

    def forward(self, src, trg, **kwargs):
        """
        Unified Forward Pass.
        
        LOGIC FLOW:
        1. MODALITY DETECTION: If trg length > 1, the system is in Training/Validation.
           It then performs internal shifting (trg[:, :-1]) to prevent identity learning.
        2. MASKING: Generates padding masks and causal masks to isolate valid signals.
        3. SCALING: Scales embeddings by sqrt(d_model) to maintain gradient variance.
        4. RECONCILIATION: Concatenates a dummy prefix if in training to keep output shape
           aligned with the ground-truth target length.
        """
        # --- PHASE 1: MODALITY-BASED ALIGNMENT ---
        is_teacher_forcing = trg.size(1) > 1
        
        if is_teacher_forcing:
            # SHIFT LOGIC: Use tokens [0 to T-1] to predict [1 to T].
            trg_input = trg[:, :-1]
        else:
            # INFERENCE LOGIC: Use the provided sequence as-is (e.g., during Beam Search).
            trg_input = trg
        
        # --- PHASE 2: ATTENTION MASKING ---
        # Binary masks (True where pad index is 0) to avoid processing null data.
        src_key_padding_mask = (src == 0)
        tgt_key_padding_mask = (trg_input == 0)
        
        # Look-ahead mask for the autoregressive decoder.
        tgt_mask = self._generate_causal_mask(trg_input.size(1), src.device)
        
        # --- PHASE 3: LATENT TRANSFORMATION ---
        # Scaling is the 'Hidden Secret' of Transformer stability.
        src_emb = self.pos_encoder(self.embedding(src) * math.sqrt(self.d_model))
        tgt_emb = self.pos_encoder(self.embedding(trg_input) * math.sqrt(self.d_model))
        
        # CORE EXECUTION:
        # memory_key_padding_mask ensures encoder padding does not distract the decoder.
        output = self.transformer(
            src_emb, tgt_emb, tgt_mask=tgt_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=src_key_padding_mask 
        )
        
        # --- PHASE 4: VOCABULARY PROJECTION ---
        predictions = self.fc_out(output)
        
        # --- PHASE 5: INTERFACE RECONCILIATION ---
        if is_teacher_forcing:
            # REPAIR SHAPE: The internal shift removed 1 token. 
            # We add a null prefix so train.py can safely call output[:, 1:].
            batch_size = src.size(0)
            dummy_prefix = torch.zeros(batch_size, 1, self.vocab_size).to(src.device)
            return torch.cat([dummy_prefix, predictions], dim=1)
        
        # Pure predictions for autoregressive decoding (Inference mode).
        return predictions

================================================================================

--- FILE: ValLossChart.py | PATH: Project\scripts\ValLossChart.py ---
---------------------------------------------------------------------

import matplotlib.pyplot as plt
import re
import os

# --- CONFIGURATION ---
LOG_FILENAME = "20260201_210033_LSTM__S50000_B0.00.log"

# The relative path from the project root to the log folder
LOG_SUBDIR = os.path.join("logs", "Bahdanau")

OUTPUT_DIR_NAME = ""
# ---------------------

def get_absolute_paths():
    """
    Constructs absolute paths dynamically based on the script's location.
    Assumes the script is located in 'Project/scripts/' and logs are in 'Project/logs/'.
    """
    # Get the directory where this script is located (e.g., .../Project/scripts)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Go up one level to reach the project root (e.g., .../Project)
    project_root = os.path.dirname(script_dir)
    
    # Construct the full path to the log file
    full_log_path = os.path.join(project_root, LOG_SUBDIR, LOG_FILENAME)
    
    # Construct the full path for the output directory (inside scripts folder)
    output_dir = os.path.join(script_dir, OUTPUT_DIR_NAME)
    
    return full_log_path, output_dir

def parse_log_and_plot():
    log_path, output_dir = get_absolute_paths()
    
    epochs = []
    val_losses = []

    print(f"Reading file: {log_path} ...")

    # 1. Read file and extract data
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        # Regex to find lines like: "Epoch 01 | ... | Val: 7.3937"
        # Captures the Epoch number (Group 1) and Validation Loss (Group 2)
        pattern = re.compile(r"Epoch\s+(\d+).*?Val:\s+([\d\.]+)")

        for line in lines:
            match = pattern.search(line)
            if match:
                epoch_num = int(match.group(1))
                val_loss = float(match.group(2))
                
                epochs.append(epoch_num)
                val_losses.append(val_loss)

    except FileNotFoundError:
        print(f"ERROR: The file was not found at path: {log_path}")
        print("Check if the file name is correct and if the folder structure matches.")
        return

    if not epochs:
        print("WARNING: No data found. Please check the log file content format.")
        return

    # 2. Generate the plot
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, val_losses, marker='o', linestyle='-', color='#d62728', linewidth=2, label='Validation Loss')

    # Plot styling
    plt.title(f'Validation Loss Trend\n({LOG_FILENAME})', fontsize=14)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Validation Loss', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Ensure X-axis shows integer ticks for epochs
    plt.xticks(epochs) 
    plt.legend()

    # 3. Save the plot
    if not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir)
            print(f"Created output directory: {output_dir}")
        except OSError as e:
            print(f"Error creating directory: {e}")
            return

    # Construct output filename based on log filename
    output_filename = LOG_FILENAME.replace(".log", "_chart.png")
    output_path = os.path.join(output_dir, output_filename)
    
    plt.savefig(output_path)
    plt.close() # Close the figure to free memory

    print("-" * 30)
    print(f"Success! Processed {len(epochs)} epochs.")
    print(f"Chart saved at: {output_path}")
    print("-" * 30)

if __name__ == "__main__":
    parse_log_and_plot()

================================================================================

--- FILE: compare_models.py | PATH: Project\scripts\compare_models.py ---
-------------------------------------------------------------------------

import matplotlib.pyplot as plt
import re
import os

# --- CONFIGURATION ---
# Define the models and their specific log files to compare
# Paths are relative to the project root
COMPARISON_CONFIG = {
    "Bahdanau (LSTM)": os.path.join("logs", "Bahdanau", "20260201_215008_LSTM__S20000_B0.00.log"),
    "DotProduct (LSTM)": os.path.join("logs", "DotProduct", "20260201_215511_LSTM__S20000_B0.00.log"),
    "Transformer": os.path.join("logs", "Transformer", "20260202_175409_TRANS_S20000_B0.00.log")
}

OUTPUT_DIR_NAME = ""
OUTPUT_FILENAME = "comparison_20k_subset.png"
# ---------------------

def get_project_root():
    """Returns the absolute path to the Project root (assuming script is in scripts/)"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.dirname(script_dir)

def parse_log(file_path):
    """Parses a single log file to extract epochs and validation losses."""
    epochs = []
    val_losses = []
    
    if not os.path.exists(file_path):
        print(f"WARNING: File not found: {file_path}")
        return [], []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        pattern = re.compile(r"Epoch\s+(\d+).*?Val:\s+([\d\.]+)")

        for line in lines:
            match = pattern.search(line)
            if match:
                epochs.append(int(match.group(1)))
                val_losses.append(float(match.group(2)))
                
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        
    return epochs, val_losses

def compare_models():
    root_dir = get_project_root()
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    plt.figure(figsize=(12, 7))
    
    # Colors for distinct visibility
    colors = {"Bahdanau (LSTM)": "blue", "DotProduct (LSTM)": "orange", "Transformer": "green"}
    
    summary_data = []

    print(f"{'Model':<20} | {'Min Val Loss':<15} | {'Final Epoch':<12}")
    print("-" * 55)

    for model_name, relative_path in COMPARISON_CONFIG.items():
        full_path = os.path.join(root_dir, relative_path)
        epochs, losses = parse_log(full_path)
        
        if epochs:
            # Plotting the line
            plt.plot(epochs, losses, marker='o', markersize=4, linestyle='-', 
                     linewidth=2, label=model_name, color=colors.get(model_name, "black"))
            
            # Collecting stats for print summary
            min_loss = min(losses)
            final_epoch = epochs[-1]
            summary_data.append((model_name, min_loss))
            
            print(f"{model_name:<20} | {min_loss:.4f}          | {final_epoch}")
        else:
            print(f"{model_name:<20} | NO DATA FOUND")

    # Chart Styling
    plt.title('Model Comparison: Subset 20k (Validation Loss)', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Validation Loss (Lower is Better)', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(fontsize=11)
    
    # Save the plot
    output_dir = os.path.join(script_dir, OUTPUT_DIR_NAME)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    output_path = os.path.join(output_dir, OUTPUT_FILENAME)
    plt.savefig(output_path)
    print("-" * 55)
    print(f"Comparison chart saved at: {output_path}")

if __name__ == "__main__":
    compare_models()

================================================================================

--- FILE: log_manager.py | PATH: Project\scripts\log_manager.py ---
-------------------------------------------------------------------

"""
================================================================================
EXECUTION LOGGER & TELEMETRY UNIT
================================================================================
ROLE: Centralized Observability and Performance Tracking.

DESIGN RATIONALE:
- Data Integrity: Captures snapshots of the system state before and after runs.
- Analytics Ready: Exports structured JSON for downstream graphing and audit tools.
- Post-Mortem Diagnostics: Detailed timing of every pipeline stage (refinery, 
  tokenization, training) to identify computational bottlenecks.
================================================================================
"""

import os
import torch
import platform
import json
import time
from datetime import datetime

# --- [LOGGING AND TELEMETRY MANAGER] ---
class ExecutionLogger:
    """ 
    Black-box recorder for the ML Pipeline. 
    Manages the lifecycle of log files from initialization to final archival.
    """

    def __init__(self, root_dir, model_tag, subset_size=None):
        """ 
        Initializes the telemetry buffer and prepares filesystem hooks. 
        
        Args:
            root_dir (str): Base project directory.
            model_tag (str): Architectural identifier (e.g., 'transformer').
            subset_size (int): Data pressure coefficient used for this run.
        """
        self.start_time = datetime.now()

        # Metadata for session identification.
        self.model_tag = model_tag
        self.subset_size = subset_size if subset_size else "ALL"
        
        # Ensure log directory existence without interrupting the pipeline flow.
        self.log_dir = os.path.join(root_dir, "logs")
        os.makedirs(self.log_dir, exist_ok=True)
        
        # Temporary file creation to protect against incomplete data if the session crashes.
        self.temp_name = f"exec_{self.start_time.strftime('%Y%m%d_%H%M%S')}.tmp"
        self.file_path = os.path.join(self.log_dir, self.temp_name)
        
        # Structured Data Schema: Designed for direct serialization to JSON.
        self.data = {
            "metadata": {
                "model_type": model_tag,
                "subset": self.subset_size,
                "start_time": self.start_time.isoformat()
            },
            "durations": {}, # Stores execution time per phase (e.g., "data_cleaning")
            "infrastructure": {}, # Stores hardware specs
            "training_history": [], # Stores per-epoch metrics
            "evaluation": {} # Stores final BLEU/ROUGE scores
        }

    def log_sys_info(self):
        """ 
        Fingerprints the execution environment. 
        Captures OS, Python version, and GPU architecture to normalize performance benchmarks.
        """
        info = {
            "os": platform.system(),
            "python": platform.python_version(),
            "device": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "cpu"
        }

        # VRAM monitoring: Crucial for auditing OOM (Out of Memory) conditions.
        if torch.cuda.is_available():
            info["vram_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB"
            
        self.data["infrastructure"] = info
        self._write_to_file(f"--- INFRASTRUCTURE ---\n{json.dumps(info, indent=4)}\n")

    def log_phase(self, phase_name, duration_seconds):
        """ 
        Logs the temporal cost of specific pipeline stages. 
        Used to identify bottlenecks (e.g., slow data refinery vs. fast training).
        """
        dur_str = f"{duration_seconds:.2f}s"
        self.data["durations"][phase_name] = dur_str
        self._write_to_file(f"[TIMER] Phase '{phase_name}' completed in {dur_str}\n")

    def log_tokenizer_info(self, vocab_size, reused=True, duration=0):
        """ 
        Logs vocabulary constraints and tokenizer performance. 
        Ensures the model/tokenizer vocabulary match is tracked.
        """
        tok_info = {
            "vocab_size": vocab_size,
            "status": "REUSED" if reused else "NEWLY_TRAINED",
            "duration": f"{duration:.2f}s"
        }
        self.data["metadata"]["tokenizer"] = tok_info
        self._write_to_file(f"--- TOKENIZER ---\n{json.dumps(tok_info, indent=4)}\n")

    def log_epoch(self, epoch, train_loss, val_loss, lr, epoch_duration):
        """ 
        Captures granular epoch data. 
        Creates a time-series record of the learning process for curve analysis.
        """
        entry = {
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "lr": lr,
            "duration": f"{epoch_duration:.2f}s"
        }
        self.data["training_history"].append(entry)
        self._write_to_file(
            f"Epoch {epoch:02d} | Loss: {train_loss:.4f} | Val: {val_loss:.4f} | "
            f"LR: {lr:.6f} | Time: {epoch_duration:.2f}s\n"
        )

    def log_final_metrics(self, df_metrics, mode="fast"):
        """
        Aggregates terminal performance scores. 
        Accepts a Pandas DataFrame from the Evaluator and flattens it for the log.
        """
        metrics = df_metrics.to_dict(orient='records')[0] if df_metrics is not None and not df_metrics.empty else {}
        metrics["eval_mode"] = mode
        self.data["evaluation"] = metrics
        self._write_to_file(f"\n--- FINAL EVALUATION ({mode.upper()}) ---\n{json.dumps(metrics, indent=4)}\n")

    def finalize(self):
        """
        Renames temporary files into permanent chronological records. 
        Includes the BLEU score in the filename for immediate visual audit of logs.
        """
        end_time = datetime.now()
        total_duration = end_time - self.start_time
        self.data["durations"]["total_execution"] = str(total_duration)
        
        # BLEU Tagging Logic: Makes it easy to find the highest-performing checkpoints.
        bleu = self.data["evaluation"].get("bleu", 0)
        bleu_tag = f"B{bleu:.2f}" if bleu > 0.0001 else "NA"
        
        # Slug format: TIMESTAMP_ARCH_SUBSET_METRIC
        full_ts = self.start_time.strftime('%Y%m%d_%H%M%S')
        model_short = self.model_tag.upper()[:5]
        subset_tag = f"S{self.subset_size}"
        
        final_name = f"{full_ts}_{model_short}_{subset_tag}_{bleu_tag}"
        
        final_path_log = os.path.join(self.log_dir, f"{final_name}.log")
        final_path_json = os.path.join(self.log_dir, f"{final_name}.json")

        self._write_to_file(f"\n--- ALL PHASE DURATIONS ---\n{json.dumps(self.data['durations'], indent=4)}\n")
        self._write_to_file(f"\n--- FINISHED ---\nTotal Duration: {total_duration}\n")
        
        try:
            # Atomic Move: Prevents file corruption and ensures log persistence.
            if os.path.exists(self.file_path):
                os.rename(self.file_path, final_path_log)
            
            # Export structured data for visualization scripts.
            with open(final_path_json, "w", encoding="utf-8") as f:
                json.dump(self.data, f, indent=4)
                
            print(f"âœ… Log Saved: {os.path.basename(final_path_log)}")
        except Exception as e:
            print(f"âš ï¸ Critical error during log archiving: {e}")

    def _write_to_file(self, text):
        """ Low-level I/O method to commit text to the physical storage device. """
        with open(self.file_path, "a", encoding="utf-8") as f:
            f.write(text)

================================================================================

--- FILE: log_parser.py | PATH: Project\scripts\log_parser.py ---
-----------------------------------------------------------------

import re
import matplotlib.pyplot as plt
import os
import numpy as np
import json
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D
from scipy.interpolate import griddata

# --- [CONFIGURATION] ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
plt.rcParams.update({
    'font.size': 11, 
    'figure.titlesize': 16,
    'axes.titlesize': 13,
    'axes.labelsize': 11,
    'legend.fontsize': 10
})

class LogComparator:
    """CMU for individual log forensic analysis."""

    def __init__(self):
        self.epoch_pattern = re.compile(r"Epoch\s+(\d+)\s+\|\s+Loss:\s+([\d.]+)\s+\|\s+Val:\s+([\d.]+)", re.IGNORECASE)
        self.bleu_pattern = re.compile(r'["\']?bleu["\']?\s*[:=]\s*([\d.]+)', re.IGNORECASE)
        self.rouge_pattern = re.compile(r'["\']?rougeL["\']?\s*[:=]\s*([\d.]+)', re.IGNORECASE)
        self.internal_filename_pattern = re.compile(r'"file"\s*:\s*"([^"]+)"', re.IGNORECASE)

    def _get_short_name(self, filepath):
        name = os.path.splitext(os.path.basename(filepath))[0]
        return re.sub(r'^\d{8}_\d{6}_', '', name)

    def parse_file(self, filepath):
        data = {
            "name": self._get_short_name(filepath), 
            "epochs": [], 
            "train_loss": [], 
            "val_loss": [], 
            "bleu": None, 
            "rougeL": None
        }

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            name_match = self.internal_filename_pattern.search(content)

            if name_match: data["name"] = name_match.group(1).replace(".pt", "")

            for m in self.epoch_pattern.finditer(content):
                data["epochs"].append(int(m.group(1)))
                data["train_loss"].append(float(m.group(2)))
                data["val_loss"].append(float(m.group(3)))

            b_m = self.bleu_pattern.search(content)
            r_m = self.rouge_pattern.search(content)

            if b_m: data["bleu"] = float(b_m.group(1))
            if r_m: data["rougeL"] = float(r_m.group(1))

        except Exception as e:
            print(f"Parsing error: {e}"); return None
        
        return data

    def plot_best_model_trace(self, log_path):
        """Generates high-fidelity training trace for the best model."""

        data = self.parse_file(log_path)

        if not data: return

        plt.figure(figsize=(12, 7))

        plt.plot(data["epochs"], data["train_loss"], label='Training Loss', color='#3498db', linewidth=2.5, alpha=0.8)

        plt.plot(data["epochs"], data["val_loss"], label='Validation Loss', color='#e67e22', linewidth=2.5)
        
        min_loss = min(data["val_loss"])
        best_epoch = data["val_loss"].index(min_loss) + 1
        plt.axvline(x=best_epoch, color='#c0392b', linestyle='--', alpha=0.6, label=f'Optimal Stopping (Ep {best_epoch})')
        plt.scatter(best_epoch, min_loss, color='#c0392b', s=100, zorder=5)

        plt.title(f"Best Model Convergence Profile: {data['name']}", pad=20)
        plt.xlabel("Epochs"); plt.ylabel("Cross-Entropy Loss")
        plt.legend(frameon=True, facecolor='white', framealpha=0.9)
        plt.grid(True, linestyle=':', alpha=0.6)
        plt.tight_layout()

        save_path = os.path.join(SCRIPT_DIR, "best_model_trace.png")
        plt.savefig(save_path, dpi=300); 
        plt.show()

# --- [VISUALIZATION ENGINE] ---

def extract_subset_size(filename):
    match = re.search(r"sub(\d+)", filename, re.IGNORECASE)
    return int(match.group(1)) if match else 0

def plot_linguistic_metrics(df):
    """Section 1: Spaced 2x2 grid for Core NLP Metrics."""

    fig, axes = plt.subplots(2, 2, figsize=(16, 14))

    metrics = [('bleu', 'BLEU Score'), ('rougeL', 'ROUGE-L Score'), ('loss', 'Final Val Loss'), ('perplexity', 'Perplexity')]
    models = df['model'].unique(); subsets = sorted(df['subset_size'].unique())
    colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(subsets)))
    x = np.arange(len(models)); w = 0.8 / len(subsets)

    for idx, (col, title) in enumerate(metrics):
        ax = axes[idx//2, idx%2]

        for i, s in enumerate(subsets):
            sub_df = df[df['subset_size'] == s]
            vals = [sub_df[sub_df['model']==m][col].values[0] if not sub_df[sub_df['model']==m].empty else 0 for m in models]
            ax.bar(x + (i - len(subsets)/2)*w + w/2, vals, w, color=colors[i], label=f"Size: {s}" if idx==0 else "", edgecolor='white', alpha=0.9)

        ax.set_title(title, fontweight='bold', pad=10)
        ax.set_xticks(x); ax.set_xticklabels(models); ax.grid(axis='y', linestyle='--', alpha=0.4)

    fig.legend(*(axes[0,0].get_legend_handles_labels()), loc='upper center', ncol=len(subsets), bbox_to_anchor=(0.5, 0.97), frameon=False)

    plt.subplots_adjust(wspace=0.3, hspace=0.4, top=0.9)
    plt.savefig(os.path.join(SCRIPT_DIR, "linguistic_audit.png"), dpi=300); 
    plt.show()

def plot_temporal_efficiency(df):
    """Section 2: Detailed Spaced Temporal Costs."""

    fig, axes = plt.subplots(1, 3, figsize=(20, 7))

    configs = [('Mean Epoch (s)', 'Epoch Duration (s)'), ('throughput', 'Throughput (Samples/s)'), ('time_per_1k', 'Cost per 1k Samples (s)')]

    models = df['model'].unique(); subsets = sorted(df['subset_size'].unique())
    colors = plt.cm.plasma(np.linspace(0.1, 0.9, len(subsets)))
    x = np.arange(len(models)); w = 0.8 / len(subsets)

    for idx, (col, title) in enumerate(configs):
        ax = axes[idx]

        for i, s in enumerate(subsets):
            sub_df = df[df['subset_size'] == s]
            vals = [sub_df[sub_df['model']==m][col].values[0] if not sub_df[sub_df['model']==m].empty else 0 for m in models]
            ax.bar(x + (i - len(subsets)/2)*w + w/2, vals, w, color=colors[i], edgecolor='white')

        ax.set_title(title, fontweight='bold', pad=15)
        ax.set_xticks(x); ax.set_xticklabels(models, rotation=90); ax.grid(axis='y', linestyle='--', alpha=0.3)

    plt.subplots_adjust(wspace=0.35, bottom=0.2)
    plt.savefig(os.path.join(SCRIPT_DIR, "temporal_efficiency.png"), dpi=300); 
    plt.show()

def plot_3d_intersecting_planes(df):
    """Section 3: High-Visibility 3D Planes (Time-Loss-ROUGE)."""

    fig = plt.figure(figsize=(14, 10))
    ax = fig.add_subplot(111, projection='3d')
    
    models = df['model'].unique()
    # Distinct palettes for surfaces
    cmaps = ['Blues', 'Oranges', 'Greens', 'Reds']
    
    for i, model in enumerate(models):
        d = df[df['model'] == model]
        if len(d) < 3: continue # Need at least 3 points for a plane
        
        # Data extraction
        x = d['Mean Epoch (s)'].values
        y = d['loss'].values
        z = d['rougeL'].values
        
        # Grid creation for surface interpolation
        xi = np.linspace(x.min(), x.max(), 20)
        yi = np.linspace(y.min(), y.max(), 20)
        XI, YI = np.meshgrid(xi, yi)
        ZI = griddata((x, y), z, (XI, YI), method='linear')
        
        # Plot Surface
        surf = ax.plot_surface(XI, YI, ZI, cmap=cmaps[i % len(cmaps)], alpha=0.6, edgecolor='none', label=model)
        # Add original scatter points for grounding
        ax.scatter(x, y, z, s=60, edgecolors='black', alpha=1.0)

    ax.set_title("3D Efficiency Planes: Time vs. Loss vs. ROUGE-L", pad=30, fontweight='bold')
    ax.set_xlabel("Epoch Time (s)", labelpad=15)
    ax.set_ylabel("Validation Loss", labelpad=15)
    ax.set_zlabel("ROUGE-L Score", labelpad=15)
    
    # Custom legend for surfaces (Matplotlib fix)
    from matplotlib.lines import Line2D
    custom_lines = [Line2D([0], [0], color=plt.get_cmap(cmaps[i])(0.5), lw=4) for i in range(len(models))]
    ax.legend(custom_lines, models, loc='upper left', bbox_to_anchor=(0.1, 0.9))

    ax.view_init(elev=25, azim=-45) # Optimal angle for intersection visibility
    plt.tight_layout()
    plt.savefig(os.path.join(SCRIPT_DIR, "3d_efficiency_planes.png"), dpi=300); 
    plt.show()

# --- [MAIN EXECUTION] ---

if __name__ == "__main__":
    json_path = "Project/logs/result_metrics.json"
    
    if os.path.exists(json_path):
        with open(json_path, 'r') as f: df = pd.DataFrame(json.load(f))
        
        # Feature Engineering
        df['subset_size'] = df['file'].apply(extract_subset_size)
        df['Mean Epoch (s)'] = pd.to_numeric(df['Mean Epoch (s)'], errors='coerce').fillna(0)
        df['throughput'] = df['subset_size'] / df['Mean Epoch (s)']
        df['time_per_1k'] = (df['Mean Epoch (s)'] / df['subset_size']) * 1000
        df = df.sort_values(by='subset_size')

        # Run Sectioned Audit
        plot_linguistic_metrics(df)
        plot_temporal_efficiency(df)
        plot_3d_intersecting_planes(df)
    else:
        print(f"[ERROR] Metrics file missing: {json_path}")

    # Final trace: Bahdanau 70k Champion
    best_log = "Project/logs/Bahdanau/20260203_002032_LSTM__S70000_B0.01.log"
    if os.path.exists(best_log):
        LogComparator().plot_best_model_trace(best_log)

================================================================================

--- FILE: train.py | PATH: Project\scripts\train.py ---
-------------------------------------------------------

"""
================================================================================
TRAINING ENGINE
================================================================================
ROLE: Core Optimization Loop and Gradient Descent Orchestrator.

DESIGN PRINCIPLES:
- Numerical Stability: Implements Gradient Clipping and Label Smoothing.
- Hardware Acceleration: Fully integrated with Mixed Precision (FP16/BF16).
- Adaptive Regularization: Hyperparameters scale based on the model phenotype 
  (Transformer vs. LSTM) to combat Overfitting vs. Underfitting.
================================================================================
"""

import torch
import torch.nn as nn
import torch.optim as optim
import logging
import time
from tqdm import tqdm

# Standard logger hook for training progress and diagnostic telemetry.
logger = logging.getLogger(__name__)

def train_model(model, train_loader, valid_loader, config, device, telemetry=None):
    """
    Main Training Loop Orchestrator.
    
    Args:
        model: PyTorch Module (LSTM or Transformer).
        train_loader: Iterator for training data batches.
        valid_loader: Iterator for validation data batches.
        config: Namespace containing epochs, model_type, and batch_size.
        device: Target compute device (cuda:0, cuda:1, or cpu).
        telemetry: Optional logger class for persisting training history.
    """

    # --- [PHASE 0] ARCHITECTURAL AUDIT ---
    # Log total trainable parameters to estimate model capacity and memory footprint.
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"START | Model: {config.model} | Params: {total_params:,} | Device: {device}")

    # Integrity Check: Prevents runtime division errors if the dataset refinery failed.
    steps_per_epoch = len(train_loader)
    if steps_per_epoch == 0:
        raise ValueError("Error: train_loader is empty. Check data refinery output.")

    # --- [PHASE 1] DIFFERENTIATED HYPERPARAMETER INJECTION ---
    # Mental Model: 'The Transformer' needs high pressure regularization 
    # to avoid memorization, while 'The LSTM' needs guidance.
    
    if config.model == "transformer":
        # Strategy for Transformers: Prevent 'Dirac Delta' distributions in Softmax.
        weight_decay = 0.1       # Strong L2 penalty to keep weights from exploding.
        label_smoothing = 0.2    # Soften targets to encourage exploration.
        max_lr = 0.0001          # Transformers are sensitive to high LR during the early phase.
        pct_start = 0.3          # 30% of total steps used for warm-up.
    else:
        # Strategy for LSTM: Standard Sequence-to-Sequence approach.
        weight_decay = 0.01      # Conservative weight decay.
        label_smoothing = 0.1    # Standard smoothing.
        max_lr = 0.0005          # Recurrent nets can often handle faster initial learning.
        pct_start = 0.3

    # CRITERION: CrossEntropy with ignore_index=0 ensures <PAD> tokens do not compute gradients.
    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=label_smoothing)

    # OPTIMIZER: AdamW decouples Weight Decay from the gradient update (crucial for Attention).
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=weight_decay)

    # SCHEDULER: OneCycleLR implements 'Super-Convergence' by cycling LR and Momentum.
    total_steps = config.epochs * steps_per_epoch
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, 
        max_lr=max_lr, 
        steps_per_epoch=steps_per_epoch, 
        epochs=config.epochs, 
        pct_start=pct_start
    ) if total_steps >= 2 else None

    # --- [PHASE 2] CORE TRAINING LOOP ---
    # SCALER: Manages precision scaling for FP16 training to prevent underflow.
    scaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None
    best_valid_loss = float('inf')

    for epoch in range(config.epochs):
        epoch_start_time = time.time()
        model.train() # Enable Dropout and BatchNorm.
        train_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.epochs}")
        
        for src, trg in pbar:
            # Move data to VRAM. non_blocking=True allows overlapping compute/transfer.
            src, trg = src.to(device, non_blocking=True), trg.to(device, non_blocking=True)
            
            # Efficiently reset gradients for the new batch.
            optimizer.zero_grad(set_to_none=True)

            # AUTO-MIXED PRECISION (AMP): Automatically casts tensors to half-precision.
            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
                # Decay Teacher Forcing: Start by guiding the model, end by letting it guess.
                tf_ratio = max(0.2, 0.5 - (epoch * 0.1)) 
                output = model(src, trg, teacher_forcing_ratio=tf_ratio)
                
                # Reshape logic: Flatten [Batch, Seq, Vocab] for standard CrossEntropy format.
                output_dim = output.shape[-1]
                # Slice [:, 1:] to skip <SOS> token, matching target alignment.
                loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))

            # BACKWARD PASS: Scale loss, compute gradients, and step optimizer.
            if scaler:
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                # Gradient Clipping: Prevents the 'Exploding Gradient' problem.
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            if scheduler:
                scheduler.step()

            # Batch Diagnostics: Update progress bar postfix with real-time loss/LR.
            train_loss += loss.item()
            pbar.set_postfix(loss=f"{loss.item():.4f}", lr=f"{optimizer.param_groups[0]['lr']:.6f}")

        # --- [PHASE 3] EPOCH EVALUATION ---
        # Mental Model: Validating generalizability on unseen data.
        epoch_duration = time.time() - epoch_start_time
        avg_train_loss = train_loss / steps_per_epoch
        valid_loss = evaluate_validation(model, valid_loader, criterion, device)
        
        # Persist epoch results to log files for future plotting/analysis.
        if telemetry:
            telemetry.log_epoch(epoch+1, avg_train_loss, valid_loss, optimizer.param_groups[0]['lr'], epoch_duration)
        
        # LOGGING SUMMARY: Report performance and check for 'New Best' checkpoint.
        status_msg = f"Epoch {epoch+1} | Loss: {avg_train_loss:.4f} | Val: {valid_loss:.4f} | Time: {epoch_duration:.2f}s"
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            status_msg += "[NEW BEST]"
        
        logger.info(status_msg)

def evaluate_validation(model, loader, criterion, device):
    """ 
    Calculates average loss on the validation set.
    Teacher Forcing is strictly disabled (ratio=0) for honest assessment.
    """
    model.eval() # Disable Dropout and BatchNorm.
    epoch_loss = 0
    num_batches = len(loader)
    if num_batches == 0: return 0   

    with torch.no_grad(): # Disable gradient graph construction to save memory.
        for src, trg in loader:
            src, trg = src.to(device), trg.to(device)
            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
                # Model predicts autoregressively based on context.
                output = model(src, trg, teacher_forcing_ratio=0)
                output_dim = output.shape[-1]
                # Compare predicted tokens with actual ground truth.
                loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))
                epoch_loss += loss.item()

    return epoch_loss / num_batches

================================================================================

